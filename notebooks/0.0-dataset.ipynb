{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8cbde96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35495905",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from utils.general import set_seed\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from datasets import Dataset, load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "import random\n",
    "\n",
    "seed = 42\n",
    "token_lengths=[10, 20, 30, 40, 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00a7184c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"bookcorpus\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "188f9591",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3458396bc4e949968f937c82a88c5faa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing and calculating lengths:   0%|          | 0/1000000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daa060f56650459d8e3bf7e683eba129",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filtering for token length 10:   0%|          | 0/1000000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c258bb70b3c45588040853eb17a8b9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filtering for token length 20:   0%|          | 0/1000000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3de9c63630c04f33aa256777c0aa071b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filtering for token length 30:   0%|          | 0/1000000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b9f3a748f5d4dda9239cd461bbb1480",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filtering for token length 40:   0%|          | 0/1000000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92a9de8676884bc08bc814af8f2fbd11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filtering for token length 50:   0%|          | 0/1000000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text_dict = {'text': dataset[:1000000]['text']}\n",
    "dataset_text_only = Dataset.from_dict(text_dict)\n",
    "\n",
    "def filter_by_token_length(\n",
    "    tokenizer: AutoTokenizer,\n",
    "    token_lengths: List[int],\n",
    "    dataset: Dataset,\n",
    "    length: int,\n",
    "    seed: int = 42\n",
    ") -> Dict[int, Dataset]:\n",
    "\n",
    "    set_seed(seed)\n",
    "\n",
    "    def tokenize_length(example):\n",
    "        tokens = tokenizer(example[\"text\"], truncation=False, add_special_tokens=True)\n",
    "        return {\"token_length\": len(tokens[\"input_ids\"])}\n",
    "\n",
    "    dataset_with_lengths = dataset.map(tokenize_length, desc=\"Tokenizing and calculating lengths\")\n",
    "\n",
    "    result_datasets = {}\n",
    "\n",
    "    for token_length in token_lengths:\n",
    "        filtered = dataset_with_lengths.filter(\n",
    "            lambda x: x[\"token_length\"] == token_length,\n",
    "            desc=f\"Filtering for token length {token_length}\"\n",
    "        )\n",
    "\n",
    "        # Shuffle and take the desired number of rows\n",
    "        if len(filtered) >= length:\n",
    "            filtered = filtered.shuffle(seed=seed).select(range(length))\n",
    "            result_datasets[token_length] = filtered\n",
    "\n",
    "    return result_datasets\n",
    "\n",
    "model_id = \"roneneldan/TinyStories-1M\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "result_dataset= filter_by_token_length(\n",
    "    tokenizer,\n",
    "    token_lengths=token_lengths,\n",
    "    dataset=dataset_text_only,\n",
    "    length=100,\n",
    "    seed=seed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7bbafe44",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dataset_df = {k: v['text'] for k, v in result_dataset.items()}\n",
    "result_dataset_df = pd.DataFrame(dict([(k, pd.Series(v)) for k, v in result_dataset_df.items()]))\n",
    "result_dataset_df.to_csv('../data/token_length_filtered_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aae852dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>random_10</th>\n",
       "      <th>random_20</th>\n",
       "      <th>random_30</th>\n",
       "      <th>random_40</th>\n",
       "      <th>random_50</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fiunited Luxem tradingdule Making695 Geoff se...</td>\n",
       "      <td>brilliantcat ensuring Sid Lynch tremb CSVku53...</td>\n",
       "      <td>craft externalToEVAOnly Sheridan processor spe...</td>\n",
       "      <td>hallucinations hugmortem PROTAren lever worse...</td>\n",
       "      <td>ackle Dow Maverifullyittyclaimaligned barb tru...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hints Warranty mortarurized Ric subscribegey ...</td>\n",
       "      <td>BASEherence down 162Modeloday functionalityaf...</td>\n",
       "      <td>August Included573 crashesiliation adventBIren...</td>\n",
       "      <td>Source� post Sept pine Judiciary Bureau   Fiv...</td>\n",
       "      <td>Planning NETWORK Document dilutedulp whispere...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jesould rodent waves Denver mounted///lon limi...</td>\n",
       "      <td>firefightercovered Turner entryMult dictancin...</td>\n",
       "      <td>churches prohib Secondaryanyl LMzbek subsid s...</td>\n",
       "      <td>inning hearings treHttp Technologies slowingt...</td>\n",
       "      <td>manner recruiting orally Desire licensing avo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Billion ≤ illustrated EkensicalToddapandem sh...</td>\n",
       "      <td>Huawei Pru KB diarrnamese \\( pools McKenna ex...</td>\n",
       "      <td>Shed uncleagarcookPort apprehend inconsist Lu...</td>\n",
       "      <td>unidentified readADS BusXM Fat worshipped dea...</td>\n",
       "      <td>Investorsorthodox brim Playoffs Action satisf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bloomlehem clans484uations underminingactivat...</td>\n",
       "      <td>decrease curse unstoppable downloading sear L...</td>\n",
       "      <td>oul appropriation latch ankModule arous Schwde...</td>\n",
       "      <td>combustion ambig DeterズGV Supreme980sexual un...</td>\n",
       "      <td>actually undisclosedWait Prelreachformedinches...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           random_10  \\\n",
       "0   fiunited Luxem tradingdule Making695 Geoff se...   \n",
       "1   hints Warranty mortarurized Ric subscribegey ...   \n",
       "2  Jesould rodent waves Denver mounted///lon limi...   \n",
       "3   Billion ≤ illustrated EkensicalToddapandem sh...   \n",
       "4   bloomlehem clans484uations underminingactivat...   \n",
       "\n",
       "                                           random_20  \\\n",
       "0   brilliantcat ensuring Sid Lynch tremb CSVku53...   \n",
       "1   BASEherence down 162Modeloday functionalityaf...   \n",
       "2   firefightercovered Turner entryMult dictancin...   \n",
       "3   Huawei Pru KB diarrnamese \\( pools McKenna ex...   \n",
       "4   decrease curse unstoppable downloading sear L...   \n",
       "\n",
       "                                           random_30  \\\n",
       "0  craft externalToEVAOnly Sheridan processor spe...   \n",
       "1  August Included573 crashesiliation adventBIren...   \n",
       "2   churches prohib Secondaryanyl LMzbek subsid s...   \n",
       "3   Shed uncleagarcookPort apprehend inconsist Lu...   \n",
       "4  oul appropriation latch ankModule arous Schwde...   \n",
       "\n",
       "                                           random_40  \\\n",
       "0   hallucinations hugmortem PROTAren lever worse...   \n",
       "1   Source� post Sept pine Judiciary Bureau   Fiv...   \n",
       "2   inning hearings treHttp Technologies slowingt...   \n",
       "3   unidentified readADS BusXM Fat worshipped dea...   \n",
       "4   combustion ambig DeterズGV Supreme980sexual un...   \n",
       "\n",
       "                                           random_50  \n",
       "0  ackle Dow Maverifullyittyclaimaligned barb tru...  \n",
       "1   Planning NETWORK Document dilutedulp whispere...  \n",
       "2   manner recruiting orally Desire licensing avo...  \n",
       "3   Investorsorthodox brim Playoffs Action satisf...  \n",
       "4  actually undisclosedWait Prelreachformedinches...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def random_sentence_of_size(\n",
    "    tokenizer,\n",
    "    n: int,\n",
    "    seed: int = 42\n",
    ") -> str:\n",
    "\n",
    "    set_seed(seed)\n",
    "\n",
    "    vocab = tokenizer.get_vocab()  # dict: token -> id\n",
    "    id_to_token = {idx: tok for tok, idx in vocab.items()}\n",
    "    special = set(tokenizer.all_special_tokens)\n",
    "\n",
    "    valid_tokens = [\n",
    "        tok for tok in id_to_token.values()\n",
    "        if tok not in special and not tok.startswith(\"##\")\n",
    "    ]\n",
    "\n",
    "    if len(valid_tokens) == 0:\n",
    "        raise ValueError(\"No valid tokens available in tokenizer vocab.\")\n",
    "\n",
    "    # 3) sample n tokens (with replacement so n can be large)\n",
    "    sampled = random.choices(valid_tokens, k=n)\n",
    "\n",
    "    # 4) join/clean up via the tokenizer’s decoder\n",
    "    return tokenizer.convert_tokens_to_string(sampled)\n",
    "\n",
    "def random_sentence_list(\n",
    "    tokenizer,\n",
    "    n: int,\n",
    "    num_sentences: int = 10,\n",
    "    seed: int = 42\n",
    ") -> List[str]:\n",
    "    return [random_sentence_of_size(tokenizer, n, seed+i) for i in range(num_sentences)]\n",
    "\n",
    "list_random_sentence = random_sentence_list(\n",
    "    tokenizer,\n",
    "    n=10,\n",
    "    num_sentences=100,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "dict_random = {\n",
    "    f\"random_{length}\": random_sentence_list(\n",
    "        tokenizer,\n",
    "        n=length,\n",
    "        num_sentences=100,\n",
    "        seed=seed + length\n",
    "    ) for length in token_lengths\n",
    "}\n",
    "result_random_dataset_df = pd.DataFrame(dict([(k, pd.Series(v)) for k, v in dict_random.items()]))\n",
    "result_random_dataset_df.to_csv('../data/random_sentences_dataset.csv', index=False)\n",
    "result_random_dataset_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "itallm2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
