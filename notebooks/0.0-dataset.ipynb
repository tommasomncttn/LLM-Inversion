{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cbde96",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35495905",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from utils.general import set_seed\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from datasets import Dataset, load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "import random\n",
    "\n",
    "seed = 42\n",
    "token_lengths=[10, 20, 30, 40, 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a7184c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = load_dataset(\"bookcorpus\", split=\"train\")\n",
    "dataset = 1_000_000 * ['blooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188f9591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_dict = {'text': dataset[:1_000_000]['text']}\n",
    "text_dict = {'text': dataset[:1_000_000]}\n",
    "dataset_text_only = Dataset.from_dict(text_dict)\n",
    "\n",
    "def filter_by_token_length(\n",
    "    tokenizer: AutoTokenizer,\n",
    "    token_lengths: List[int],\n",
    "    dataset: Dataset,\n",
    "    length: int,\n",
    "    seed: int = 42\n",
    ") -> Dict[int, Dataset]:\n",
    "\n",
    "    set_seed(seed)\n",
    "\n",
    "    def tokenize_length(example):\n",
    "        tokens = tokenizer(example[\"text\"], truncation=False, add_special_tokens=True)\n",
    "        return {\"token_length\": len(tokens[\"input_ids\"])}\n",
    "\n",
    "    dataset_with_lengths = dataset.map(tokenize_length, desc=\"Tokenizing and calculating lengths\")\n",
    "\n",
    "    result_datasets = {}\n",
    "\n",
    "    for token_length in token_lengths:\n",
    "        filtered = dataset_with_lengths.filter(\n",
    "            lambda x: x[\"token_length\"] == token_length,\n",
    "            desc=f\"Filtering for token length {token_length}\"\n",
    "        )\n",
    "\n",
    "        # Shuffle and take the desired number of rows\n",
    "        if len(filtered) >= length:\n",
    "            filtered = filtered.shuffle(seed=seed).select(range(length))\n",
    "            result_datasets[token_length] = filtered\n",
    "\n",
    "    return result_datasets\n",
    "\n",
    "model_id = \"roneneldan/TinyStories-1M\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "result_dataset= filter_by_token_length(\n",
    "    tokenizer,\n",
    "    token_lengths=token_lengths,\n",
    "    dataset=dataset_text_only,\n",
    "    length=100,\n",
    "    seed=seed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbafe44",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dataset_df = {k: v['text'] for k, v in result_dataset.items()}\n",
    "result_dataset_df = pd.DataFrame(dict([(k, pd.Series(v)) for k, v in result_dataset_df.items()]))\n",
    "result_dataset_df.to_csv('../data/token_length_filtered_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae852dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_sentence_of_size(\n",
    "    tokenizer,\n",
    "    n: int,\n",
    "    seed: int = 42\n",
    ") -> str:\n",
    "\n",
    "    set_seed(seed)\n",
    "\n",
    "    vocab = tokenizer.get_vocab()  # dict: token -> id\n",
    "    id_to_token = {idx: tok for tok, idx in vocab.items()}\n",
    "    special = set(tokenizer.all_special_tokens)\n",
    "\n",
    "    valid_tokens = [\n",
    "        tok for tok in id_to_token.values()\n",
    "        if tok not in special and not tok.startswith(\"##\")\n",
    "    ]\n",
    "\n",
    "    if len(valid_tokens) == 0:\n",
    "        raise ValueError(\"No valid tokens available in tokenizer vocab.\")\n",
    "\n",
    "    # 3) sample n tokens (with replacement so n can be large)\n",
    "    sampled = random.choices(valid_tokens, k=n)\n",
    "\n",
    "    # 4) join/clean up via the tokenizerâ€™s decoder\n",
    "    return tokenizer.convert_tokens_to_string(sampled)\n",
    "\n",
    "def random_sentence_list(\n",
    "    tokenizer,\n",
    "    n: int,\n",
    "    num_sentences: int = 10,\n",
    "    seed: int = 42\n",
    ") -> List[str]:\n",
    "    return [random_sentence_of_size(tokenizer, n, seed+i) for i in range(num_sentences)]\n",
    "\n",
    "list_random_sentence = random_sentence_list(\n",
    "    tokenizer,\n",
    "    n=10,\n",
    "    num_sentences=100,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "dict_random = {\n",
    "    f\"random_{length}\": random_sentence_list(\n",
    "        tokenizer,\n",
    "        n=length,\n",
    "        num_sentences=100,\n",
    "        seed=seed + length\n",
    "    ) for length in token_lengths\n",
    "}\n",
    "result_random_dataset_df = pd.DataFrame(dict([(k, pd.Series(v)) for k, v in dict_random.items()]))\n",
    "result_random_dataset_df.to_csv('../data/random_sentences_dataset.csv', index=False)\n",
    "result_random_dataset_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "itallm2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
