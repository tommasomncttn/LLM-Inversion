{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "123aa9f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/arena-env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# import from huggingface roneneldan/TinyStories-1M\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba510893",
   "metadata": {},
   "source": [
    "# Trying with TinyStories "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a285c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roneneldan/TinyStories-1M\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"roneneldan/TinyStories-1M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ee257c2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 64])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_trial = \"Paris is the capital of \"\n",
    "inputs = tokenizer(prompt_trial, return_tensors=\"pt\")\n",
    "input_ids = inputs.input_ids\n",
    "model.transformer.wte(input_ids).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06deb0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings_gpt(prompt):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\") # dictionary with input_ids and attention_mask\n",
    "    input_ids = inputs.input_ids # tensor of shape (1, 10)\n",
    "    attention_mask = inputs.attention_mask # tensor of shape (1, 10)\n",
    "    # Get the embeddings\n",
    "    with torch.no_grad():\n",
    "        token_emb = model.transformer.wte(input_ids)\n",
    "        position_ids = torch.arange(input_ids.size(1), device=input_ids.device).unsqueeze(0)\n",
    "        pos_emb = model.transformer.wpe(position_ids)\n",
    "        embeddings = token_emb + pos_emb\n",
    "\n",
    "    # return the embeddings batch x sequence x embedding_dim\n",
    "    return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "719037f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embeddings_to_texts_baseline(embeddings, model, tokenizer, skip_special_tokens=True):\n",
    "    \"\"\"\n",
    "    Map input embeddings (batch, seq_len, emb_dim) → list of decoded strings.\n",
    "    \"\"\"\n",
    "    # 1) Project embeddings to vocab logits\n",
    "    logits = model.lm_head(embeddings)           # (batch, seq_len, vocab_size)\n",
    "    # 2) Greedy decode: pick highest logit per position\n",
    "    token_ids = torch.argmax(logits, dim=-1)     # (batch, seq_len)\n",
    "    # 3) Transform each sequence of IDs into text\n",
    "    texts = tokenizer.batch_decode(token_ids, skip_special_tokens=skip_special_tokens)\n",
    "    return logits, texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f441559f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 64])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trying to get embeddings from a prompt_trial \n",
    "embeddings = get_embeddings_gpt(prompt_trial)\n",
    "# print(embeddings.shape) # (1, 10, 768)\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "993a037c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 6, 50257]), ['umm is the capital of '])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits, text = embeddings_to_texts_baseline(embeddings, model, tokenizer)\n",
    "logits.shape, text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659c0e64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Once upon a time, in a land far away, there lived a']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"Once upon a time, in a land far away, there lived a\"\n",
    "# get the embeddings for the prompt\n",
    "\n",
    "_, text = embeddings_to_texts_baseline(get_embeddings_gpt(prompt), model, tokenizer)\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1142651d",
   "metadata": {},
   "source": [
    "# Trying With Mistral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fba7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "login(token=os.getenv(\"HUGGINGFACE_TOKEN\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "981d5e90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.06s/it]\n"
     ]
    }
   ],
   "source": [
    "model_id = \"mistralai/Mistral-7B-v0.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",       # auto-slice layers across GPUs/CPU\n",
    "    load_in_8bit=True,       # or load_in_4bit=True\n",
    "    torch_dtype=\"auto\"       # keep LayerNorm etc. in fp16/32\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89600d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings_mistral(prompt):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\") # dictionary with input_ids and attention_mask\n",
    "    input_ids = inputs.input_ids.to(model.device) # tensor of shape (1, 10)\n",
    "    attention_mask = inputs.attention_mask.to(model.device) # tensor of shape (1, 10)\n",
    "    # Get the embeddings\n",
    "    with torch.no_grad():\n",
    "        embeddings = model.model.embed_tokens(input_ids)\n",
    "\n",
    "    # return the embeddings batch x sequence x embedding_dim\n",
    "    return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fd749bfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 15, 4096])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_trial = \"Once upon a time, in a land far away, there lived a\"\n",
    "embeddings = get_embeddings_mistral(prompt_trial)\n",
    "# print(embeddings.shape) # (1, 10, 768)\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "11c13998",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ocker Groupisenerial候puislcerialuvudstock awaypuisafteronnaerial']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits, text = embeddings_to_texts_baseline(embeddings, model, tokenizer)\n",
    "text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "166c478e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.embed_tokens.weight.shape == model.lm_head.weight.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d49bff1",
   "metadata": {},
   "source": [
    "# Trying with GPT-2 Small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b440909f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load with gpt2 \n",
    "SMALL = False\n",
    "model_id = [\"openai-community/gpt2-xl\", \"openai-community/gpt2\"][SMALL]\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b24fe1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 14, 1600])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_trial = \"Once upon a time, in a land far away, there lived a\"\n",
    "embeddings = get_embeddings_gpt(prompt_trial)\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef849733",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Once upon a time, in a land far away, there lived a']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits, text = embeddings_to_texts_baseline(embeddings, model, tokenizer)\n",
    "text \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e2bc85c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([50257, 1600]), torch.Size([50257, 1600]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.lm_head.weight.shape, model.transformer.wte.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "772fb5a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.lm_head.weight.shape == model.transformer.wte.weight.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5e8d39",
   "metadata": {},
   "source": [
    "# Trying With Gemma2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10f4925a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Downloading shards: 100%|██████████| 2/2 [01:47<00:00, 53.61s/it] \n",
      "`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n",
      "Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n",
      "`config.hidden_activation` if you want to override this behaviour.\n",
      "See https://github.com/huggingface/transformers/pull/29402 for more details.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.54s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GemmaForCausalLM(\n",
       "  (model): GemmaModel(\n",
       "    (embed_tokens): Embedding(256000, 2048, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-17): 18 x GemmaDecoderLayer(\n",
       "        (self_attn): GemmaSdpaAttention(\n",
       "          (q_proj): Linear8bitLt(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear8bitLt(in_features=2048, out_features=256, bias=False)\n",
       "          (v_proj): Linear8bitLt(in_features=2048, out_features=256, bias=False)\n",
       "          (o_proj): Linear8bitLt(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): GemmaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): GemmaMLP(\n",
       "          (gate_proj): Linear8bitLt(in_features=2048, out_features=16384, bias=False)\n",
       "          (up_proj): Linear8bitLt(in_features=2048, out_features=16384, bias=False)\n",
       "          (down_proj): Linear8bitLt(in_features=16384, out_features=2048, bias=False)\n",
       "          (act_fn): PytorchGELUTanh()\n",
       "        )\n",
       "        (input_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
       "        (post_attention_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): GemmaRMSNorm((2048,), eps=1e-06)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=256000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_id = \"google/gemma-2b\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",       # auto-slice layers across GPUs/CPU\n",
    "    load_in_8bit=True,       # or load_in_4bit=True\n",
    "    torch_dtype=\"auto\"       # keep LayerNorm etc. in fp16/32\n",
    ")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a0a33fc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 15, 2048]), torch.Size([1, 15, 2048]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_trial = \"Once upon a time, in a land far away, there lived a\"\n",
    "embeddings = get_embeddings_mistral(prompt_trial)\n",
    "normalized_embeddings = model.model.norm(embeddings)\n",
    "# print(embeddings.shape) # (1, 10, 768)\n",
    "embeddings.shape, normalized_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "589bfbec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' increa increa increa increa increa increa increa increa increa increa increa increa increa increa']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits, text = embeddings_to_texts_baseline(embeddings, model, tokenizer)\n",
    "text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "69f5caef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' increa increa increa increa increa increa increa increa increa increa increa increa increa increa']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits_normalized, text_normalized = embeddings_to_texts_baseline(normalized_embeddings, model, tokenizer)\n",
    "text_normalized \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbe059d",
   "metadata": {},
   "source": [
    "# Trying With GPT-J "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874debbe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arena-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
