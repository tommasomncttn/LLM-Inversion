{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "123aa9f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/itallm2/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# import from huggingface roneneldan/TinyStories-1M\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from nnsight import LanguageModel\n",
    "import torch as t\n",
    "# load garbage collection and empty cache \n",
    "import gc\n",
    "from torch.cuda import empty_cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba510893",
   "metadata": {},
   "source": [
    "# UNEMBEDDING BASELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8ba736b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean():\n",
    "    gc.collect()\n",
    "    empty_cache()\n",
    "\n",
    "model_id = \"roneneldan/TinyStories-1M\"\n",
    "model_id = \"EleutherAI/gpt-j-6b\"\n",
    "\n",
    "try:\n",
    "    del llm\n",
    "    clean()\n",
    "    llm = LanguageModel(model_id, device_map=\"cuda\", load_in_8bit=True)\n",
    "    tokenizer = llm.tokenizer\n",
    "except:\n",
    "    llm = LanguageModel(model_id, device_map=\"cuda\", load_in_8bit=True)\n",
    "    tokenizer = llm.tokenizer\n",
    "\n",
    "prompt_trial = \"1:10,2:20,3:\"\n",
    "prompt_trial = \"my name is \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "833266d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def embeddings_to_texts_baseline(embeddings, model, tokenizer, skip_special_tokens=True):\n",
    "    \"\"\"\n",
    "    Map input embeddings (batch, seq_len, emb_dim) → list of decoded strings.\n",
    "    \"\"\"\n",
    "    # 1) Project embeddings to vocab logits\n",
    "    logits = model.lm_head(embeddings.to(\"cuda\"))           # (batch, seq_len, vocab_size)\n",
    "    # 2) Greedy decode: pick highest logit per position\n",
    "    token_ids = torch.argmax(logits, dim=-1)     # (batch, seq_len)\n",
    "    # 3) Transform each sequence of IDs into text\n",
    "    texts = tokenizer.batch_decode(token_ids, skip_special_tokens=skip_special_tokens)\n",
    "    return logits, texts\n",
    "\n",
    "def get_next_token_prediction(embeddings, model, tokenizer, skip_special_tokens=True):\n",
    "    # 1) Project embeddings to vocab logits\n",
    "    logits = model.lm_head(embeddings)           # (batch, seq_len, vocab_size)\n",
    "    # 2) Greedy decode: pick highest logit per position\n",
    "    token_ids = torch.argmax(logits, dim=-1)[:,-1]     # (batch, seq_len)\n",
    "    # 3) Transform each sequence of IDs into text\n",
    "    texts = tokenizer.batch_decode(token_ids, skip_special_tokens=skip_special_tokens)\n",
    "    return texts\n",
    "\n",
    "@t.inference_mode()\n",
    "def get_residual_output(prompt, layer_idx, llm, normalize = False):\n",
    "\n",
    "    assert hasattr(llm, 'transformer'), \"The model does not have a transformer attribute.\"\n",
    "    assert hasattr(llm.transformer, 'h'), \"The transformer does not have a 'h' attribute for layers.\"\n",
    "\n",
    "\n",
    "    with llm.trace(prompt):\n",
    "        residual_output = llm.transformer.h[layer_idx].output[0].save()  # Save the output of the layer for inspection\n",
    "\n",
    "    if normalize: # FIXME: check if this is the correct way to normalize\n",
    "        residual_output = llm.transformer.ln_f(residual_output)\n",
    "    \n",
    "    if llm.device.type == \"cuda\":\n",
    "        residual_output = residual_output.detach().to(\"cpu\")\n",
    "        clean()\n",
    "    \n",
    "    return residual_output\n",
    "\n",
    "@t.inference_mode()\n",
    "def get_embeddings(prompt,llm):\n",
    "\n",
    "    assert hasattr(llm, 'transformer'), \"The model does not have a transformer attribute.\"\n",
    "    assert hasattr(llm.transformer, 'drop'), \"The transformer does not have a 'drop' attribute for input embeddings.\"\n",
    "    \n",
    "    with llm.trace(prompt):\n",
    "        input_embeddings = llm.transformer.drop.input.save()\n",
    "\n",
    "    if llm.device.type == \"cuda\":\n",
    "        input_embeddings = input_embeddings.detach().to(\"cpu\")\n",
    "        clean()\n",
    "    return input_embeddings\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50818d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "@t.inference_mode()\n",
    "def get_one_token_h(prompt, layer_idx, model, tokenizer):\n",
    "    llm = LanguageModel(model, tokenizer=tokenizer)\n",
    "\n",
    "    # check that the prompt is a single token\n",
    "    assert len(tokenizer(prompt)[\"input_ids\"]) == 1, \"The prompt must be a single token.\"\n",
    "    # get the hidden representation of the prompt at layer_idx\n",
    "    with llm.trace(prompt):\n",
    "        hidden = llm.transformer.h[layer_idx].input.save()\n",
    "    if llm.device.type == \"cuda\":\n",
    "        hidden = hidden.detach().to(\"cpu\")\n",
    "        clean()\n",
    "    return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c51e30e",
   "metadata": {},
   "source": [
    "### TRYING THE DECODING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3afd5ee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt_trial='my name is ' \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at EleutherAI/gpt-j-6b were not used when initializing GPTJForCausalLM: ['transformer.h.0.attn.bias', 'transformer.h.0.attn.masked_bias', 'transformer.h.1.attn.bias', 'transformer.h.1.attn.masked_bias', 'transformer.h.10.attn.bias', 'transformer.h.10.attn.masked_bias', 'transformer.h.11.attn.bias', 'transformer.h.11.attn.masked_bias', 'transformer.h.12.attn.bias', 'transformer.h.12.attn.masked_bias', 'transformer.h.13.attn.bias', 'transformer.h.13.attn.masked_bias', 'transformer.h.14.attn.bias', 'transformer.h.14.attn.masked_bias', 'transformer.h.15.attn.bias', 'transformer.h.15.attn.masked_bias', 'transformer.h.16.attn.bias', 'transformer.h.16.attn.masked_bias', 'transformer.h.17.attn.bias', 'transformer.h.17.attn.masked_bias', 'transformer.h.18.attn.bias', 'transformer.h.18.attn.masked_bias', 'transformer.h.19.attn.bias', 'transformer.h.19.attn.masked_bias', 'transformer.h.2.attn.bias', 'transformer.h.2.attn.masked_bias', 'transformer.h.20.attn.bias', 'transformer.h.20.attn.masked_bias', 'transformer.h.21.attn.bias', 'transformer.h.21.attn.masked_bias', 'transformer.h.22.attn.bias', 'transformer.h.22.attn.masked_bias', 'transformer.h.23.attn.bias', 'transformer.h.23.attn.masked_bias', 'transformer.h.24.attn.bias', 'transformer.h.24.attn.masked_bias', 'transformer.h.25.attn.bias', 'transformer.h.25.attn.masked_bias', 'transformer.h.26.attn.bias', 'transformer.h.26.attn.masked_bias', 'transformer.h.27.attn.bias', 'transformer.h.27.attn.masked_bias', 'transformer.h.3.attn.bias', 'transformer.h.3.attn.masked_bias', 'transformer.h.4.attn.bias', 'transformer.h.4.attn.masked_bias', 'transformer.h.5.attn.bias', 'transformer.h.5.attn.masked_bias', 'transformer.h.6.attn.bias', 'transformer.h.6.attn.masked_bias', 'transformer.h.7.attn.bias', 'transformer.h.7.attn.masked_bias', 'transformer.h.8.attn.bias', 'transformer.h.8.attn.masked_bias', 'transformer.h.9.attn.bias', 'transformer.h.9.attn.masked_bias']\n",
      "- This IS expected if you are initializing GPTJForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPTJForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reversed Embeddings for the prompt: texts= ['----'] \n",
      "\n",
      "\n",
      "Iterating through each layer's output for the prompt: my name is \n",
      "\n",
      "Layer 0 output texts: ['riad=\" alsoanda']\n",
      "Layer 1 output texts: ['selferver alsodc']\n",
      "Layer 2 output texts: [\".='ntvern\"]\n",
      "Layer 3 output texts: ['\\n=\" supposedids']\n",
      "Layer 4 output texts: ['\\n=\" gonnaids']\n",
      "Layer 5 output texts: ['\\n=\" gonnaiced']\n",
      "Layer 6 output texts: ['\\n=\" \"iced']\n",
      "Layer 7 output texts: ['\\n=\" gonna____']\n",
      "Layer 8 output texts: ['\\n=\" gonna____']\n",
      "Layer 9 output texts: ['\\n=\" gonna____']\n",
      "Layer 10 output texts: ['\\n=\" gonna____']\n",
      "Layer 11 output texts: ['\\n=\" gonna____']\n",
      "Layer 12 output texts: ['\\n=\" Bob____']\n",
      "Layer 13 output texts: ['\\n=\" Jeff____']\n",
      "Layer 14 output texts: ['\\n=\"nt____']\n",
      "Layer 15 output texts: ['\\n=\"nt********']\n",
      "Layer 16 output texts: ['\\n is Jeff********']\n",
      "Layer 17 output texts: ['\\n is Michael********']\n",
      "Layer 18 output texts: ['\\n is Steve********']\n",
      "Layer 19 output texts: ['\\n is Steve********']\n",
      "Layer 20 output texts: ['\\n is john********']\n",
      "Layer 21 output texts: ['\\n is john********']\n",
      "Layer 22 output texts: ['\\n is john********']\n",
      "Layer 23 output texts: ['\\n is john********']\n",
      "Layer 24 output texts: ['\\n is johnian']\n",
      "Layer 25 output texts: ['\\n is jian']\n",
      "Layer 26 output texts: ['. is jian']\n",
      "Layer 27 output texts: ['- is jian']\n"
     ]
    }
   ],
   "source": [
    "# make a for loop where for each layer you try the embeddings_to_texts_baseline for each hidden state output\n",
    "full_text = True\n",
    "print(f\"{prompt_trial=} \\n\\n\")\n",
    "if full_text:   \n",
    "    reversed_embeddings = get_embeddings(prompt_trial, llm)\n",
    "    logits, texts = embeddings_to_texts_baseline(reversed_embeddings, llm, tokenizer)\n",
    "    print(f\"Reversed Embeddings for the prompt: {texts= } \\n\\n\")\n",
    "else:\n",
    "    next_token_prediction = get_next_token_prediction(get_embeddings(prompt_trial, llm), llm, tokenizer)\n",
    "    print(f\"Next token prediction for the prompt: {next_token_prediction= } \\n\\n\")\n",
    "\n",
    "\n",
    "print(f\"Iterating through each layer's output for the prompt: {prompt_trial}\\n\")\n",
    "for layer_idx in range(len(llm.transformer.h)):\n",
    "    residual_output = get_residual_output(prompt_trial, layer_idx, llm, True)\n",
    "    if full_text:\n",
    "        logits, texts = embeddings_to_texts_baseline(residual_output, llm, tokenizer)\n",
    "        print(f\"Layer {layer_idx} output texts: {texts}\")\n",
    "    else:\n",
    "        next_token_prediction = get_next_token_prediction(residual_output, llm, tokenizer)\n",
    "        print(f\"Layer {layer_idx} next token prediction: {next_token_prediction}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096d9fc7",
   "metadata": {},
   "source": [
    "# GRADIENT BASED ALGORITHM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ee9e219",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def compute_last_token_embedding_grad(\n",
    "    y: torch.LongTensor,\n",
    "    llm: torch.nn.Module,\n",
    "    layer_idx: int,\n",
    "    h: torch.Tensor\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Given:\n",
    "      • y: 1D LongTensor of token IDs with shape (t,)\n",
    "      • llm: a HuggingFace‐style language model (e.g. GPT2, BERT), already loaded\n",
    "      • layer_idx: integer layer at which to “prune” and extract the hidden state\n",
    "      • h: target hidden vector of shape (hidden_size,) or (1, hidden_size)\n",
    "\n",
    "    Returns:\n",
    "      • A torch.Tensor of shape (hidden_size,) which is\n",
    "        ∂/∂e_last ( ‖z_last_layer - h‖₂² ), where z_last_layer is the model’s hidden state\n",
    "        for the last token at the requested layer.  Only the embedding of the last token\n",
    "        receives a nonzero gradient.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) Make sure the model is in eval() mode (not strictly needed for gradients,\n",
    "    #    but sets layers like dropout to eval).  Then freeze all parameters.\n",
    "    llm.eval()\n",
    "    for p in llm.parameters():\n",
    "        p.requires_grad_(False)\n",
    "\n",
    "    # 2) Prepare input IDs: ensure shape is (1, t)\n",
    "    if not isinstance(y, torch.Tensor):\n",
    "        y = torch.tensor(y, dtype=torch.long)\n",
    "    if y.dim() == 1:\n",
    "        input_ids = y.unsqueeze(0)  # now shape is (1, t)\n",
    "    else:\n",
    "        input_ids = y  # assume the user already shaped it as (1, t)\n",
    "\n",
    "    device = next(llm.parameters()).device\n",
    "    input_ids = input_ids.to(device)\n",
    "    h = h.to(device)\n",
    "    # If h has shape (hidden_size,), unsqueeze to (1, hidden_size)\n",
    "    if h.dim() == 1:\n",
    "        h = h.unsqueeze(0)\n",
    "\n",
    "    # 3) Run the embedding layer under no_grad to get embeddings, then detach.\n",
    "    #    We want to treat embeddings as a leaf variable that can require_grad for the last token.\n",
    "    with torch.no_grad():\n",
    "        # Assuming llm has a method get_input_embeddings() that returns the embedding layer\n",
    "        embed_layer = llm.get_input_embeddings()\n",
    "        full_embeddings = embed_layer(input_ids)  \n",
    "        # full_embeddings: (1, t, hidden_size)\n",
    "\n",
    "    # 4) Detach and set requires_grad=True only on full_embeddings ↦ we'll mask out everything\n",
    "    #    except the last token’s embedding when we take gradients.\n",
    "    embeddings = full_embeddings.detach().requires_grad_(True)\n",
    "    # shape: (1, t, hidden_size)\n",
    "\n",
    "    # 5) Forward‐pass through the model “up to” layer_idx.\n",
    "    #    We rely on the model’s ability to accept inputs_embeds and to return hidden_states.\n",
    "    #    Many HF models put hidden_states[0] = embeddings,\n",
    "    #                                                  hidden_states[1] = after layer 0, etc.\n",
    "    outputs = llm(\n",
    "        inputs_embeds=embeddings,\n",
    "        output_hidden_states=True,\n",
    "        return_dict=True\n",
    "    )\n",
    "    # outputs.hidden_states is a tuple of length (num_layers+1).  Indexing might differ by model:\n",
    "    #    - hidden_states[0]: the embedding output\n",
    "    #    - hidden_states[1]: after the first Transformer block\n",
    "    #    - …\n",
    "    #    - hidden_states[L]: after the L-th block, etc.\n",
    "\n",
    "    all_hidden_states = outputs.hidden_states\n",
    "    # Sanity‐check: layer_idx must be < len(all_hidden_states)\n",
    "    if layer_idx >= len(all_hidden_states):\n",
    "        raise ValueError(\n",
    "            f\"layer_idx={layer_idx} is too large; model only returns \"\n",
    "            f\"{len(all_hidden_states)} hidden states.\"\n",
    "        )\n",
    "\n",
    "    # 6) Extract the last‐token hidden vector at the requested layer.\n",
    "    #    hidden_states[layer_idx] has shape (1, t, hidden_size).\n",
    "    z_at_layer = all_hidden_states[layer_idx]  # shape (1, t, hidden_size)\n",
    "    z_last_token = z_at_layer[:, -1, :]        # shape (1, hidden_size)\n",
    "\n",
    "    # 7) Compute the (squared) L2 loss: sum((z_last_token - h)^2) \n",
    "    #    (If you want the non‐squared L2 distance, use torch.norm(z - h, p=2), but squared is simpler.)\n",
    "    #    Here we choose squared L2 because its gradient is 2*(z - h), which is fine.\n",
    "    diff = z_last_token - h             # shape (1, hidden_size)\n",
    "    loss = torch.sum(diff * diff)       # scalar\n",
    "\n",
    "    # 8) Backward pass: only embeddings require grad; all model parameters are frozen.\n",
    "    llm.zero_grad()       # clear any stored gradients (just in case)\n",
    "    loss.backward()\n",
    "\n",
    "    # 9) Now `embeddings.grad` has shape (1, t, hidden_size).  We only care about the last token.\n",
    "    grad_embeddings = embeddings.grad        # (1, t, hidden_size)\n",
    "    grad_last = grad_embeddings[:, -1, :]    # (1, hidden_size)\n",
    "    # Squeeze to make it (hidden_size,)\n",
    "    grad_last = grad_last.squeeze(0)\n",
    "\n",
    "    return grad_last\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dff6eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['output_hidden_states']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['output_hidden_states']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grad shape: torch.Size([2, 64])\n"
     ]
    }
   ],
   "source": [
    "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# import torch\n",
    "\n",
    "# # 1) Load a pretrained LLM (e.g. GPT-2)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"roneneldan/TinyStories-1M\")\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "# model     = AutoModelForCausalLM.from_pretrained(\"roneneldan/TinyStories-1M\", output_hidden_states=True)\n",
    "\n",
    "\n",
    "# # 2) Suppose y = [\"Hello\", \"world\"]  → token IDs:\n",
    "# tokens = tokenizer([\"Hello\", \"world\"], return_tensors=\"pt\", add_special_tokens=False)\n",
    "# y_ids  = tokens[\"input_ids\"].squeeze(0)   # e.g. tensor([15496, 995])\n",
    "\n",
    "# # 3) Suppose we want the target vector h to be some random “desired” hidden representation \n",
    "# #    at layer 4 (for illustration).\n",
    "# hidden_size = model.config.hidden_size          # 768 for GPT-2 small\n",
    "# h_target    = torch.randn(1, hidden_size)       # pretend this is our “objective”\n",
    "\n",
    "# # 4) Call the function:\n",
    "# grad_last_embedding = compute_last_token_embedding_grad(\n",
    "#     y=y_ids,\n",
    "#     llm=model,\n",
    "#     layer_idx=4,\n",
    "#     h=h_target\n",
    "# )\n",
    "\n",
    "# print(\"Grad shape:\", grad_last_embedding.shape)  # → torch.Size([768])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32764b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that takes as input a tokenizer and an embedding matrix and returns a random token and its embedding\n",
    "def get_random_token_and_embedding(tokenizer, embedding_matrix):\n",
    "    # Get the vocabulary size\n",
    "    vocab_size = embedding_matrix.shape[0]\n",
    "    \n",
    "    # Generate a random token ID\n",
    "    random_token_id = torch.randint(0, vocab_size, (1,)).item()\n",
    "    \n",
    "    # Get the corresponding embedding\n",
    "    embedding = embedding_matrix[random_token_id]\n",
    "    \n",
    "    # Decode the token ID to get the token string\n",
    "    token_string = tokenizer.decode(random_token_id)\n",
    "    \n",
    "    return token_string, embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8772337f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['output_hidden_states']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['output_hidden_states']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "# take as input llm, tokenizer, embedding matrix, an hidden representation, layer_idx, n_iterations, step_size, \n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roneneldan/TinyStories-1M\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"roneneldan/TinyStories-1M\", output_hidden_states=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "prompt = \"my\"\n",
    "h = get_one_token_h(prompt, 4, model,tokenizer)\n",
    "embedding_matrix = model.get_input_embeddings().weight  # (vocab_size, hidden_size)\n",
    "gamma = 1  # step size for gradient descent\n",
    "layer_idx = 4  # layer at which we want to compute the gradient\n",
    "n_iterations = 10000  # number of iterations for gradient descent\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e3e8c22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 1562/10000 [00:17<01:34, 89.58it/s, Current guess:  9]                       \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m e_i = embedding_matrix[tokenizer.encode(y_i)[\u001b[32m0\u001b[39m]]  \u001b[38;5;66;03m# (hidden_size,)\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# compute the gradient of the oracle with respect to e_i\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m grad_oracle = \u001b[43mcompute_last_token_embedding_grad\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_i\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlong\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mllm\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlayer_idx\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlayer_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mh\u001b[49m\u001b[43m=\u001b[49m\u001b[43mh\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (hidden_size,)\u001b[39;00m\n\u001b[32m     17\u001b[39m bar.set_postfix_str(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCurrent guess: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my_i\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Gradient norm: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgrad_oracle.norm().item()\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# update the guess using gradient descent\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 94\u001b[39m, in \u001b[36mcompute_last_token_embedding_grad\u001b[39m\u001b[34m(y, llm, layer_idx, h)\u001b[39m\n\u001b[32m     92\u001b[39m \u001b[38;5;66;03m# 8) Backward pass: only embeddings require grad; all model parameters are frozen.\u001b[39;00m\n\u001b[32m     93\u001b[39m llm.zero_grad()       \u001b[38;5;66;03m# clear any stored gradients (just in case)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     96\u001b[39m \u001b[38;5;66;03m# 9) Now `embeddings.grad` has shape (1, t, hidden_size).  We only care about the last token.\u001b[39;00m\n\u001b[32m     97\u001b[39m grad_embeddings = embeddings.grad        \u001b[38;5;66;03m# (1, t, hidden_size)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/itallm2/lib/python3.11/site-packages/torch/_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/itallm2/lib/python3.11/site-packages/torch/autograd/__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/itallm2/lib/python3.11/site-packages/torch/autograd/graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# make a guess for (e_0,y_0)\n",
    "y_i, e_0= get_random_token_and_embedding(tokenizer, embedding_matrix)\n",
    "\n",
    "# loop through the n_iterations\n",
    "for iteration in (bar:=tqdm(range(n_iterations))):\n",
    "    bar.set_postfix_str(f\"Current guess: {y_i}\")\n",
    "    # compute the embedding of the current guess\n",
    "    e_i = embedding_matrix[tokenizer.encode(y_i)[0]]  # (hidden_size,)\n",
    "    \n",
    "    # compute the gradient of the oracle with respect to e_i\n",
    "    grad_oracle = compute_last_token_embedding_grad(\n",
    "        y=torch.tensor(tokenizer.encode(y_i), dtype=torch.long),\n",
    "        llm=model,\n",
    "        layer_idx=layer_idx,\n",
    "        h=h\n",
    "    )  # (hidden_size,)\n",
    "    bar.set_postfix_str(f\"Current guess: {y_i}, Gradient norm: {grad_oracle.norm().item():.4f}\")\n",
    "\n",
    "    # update the guess using gradient descent\n",
    "    x_i_plus_1 = e_i - gamma * grad_oracle  # (hidden_size,)\n",
    "    \n",
    "    # project e_i_plus_1 into the embedding space\n",
    "    # e_i_plus_1 = torch.nn.functional.normalize(x_i_plus_1, p=2, dim=-1)  # normalize to unit norm\n",
    "    \n",
    "    # find the closest token in the embedding space to e_i_plus_1\n",
    "    distances = torch.norm(embedding_matrix - x_i_plus_1, dim=1)  # (vocab_size,)\n",
    "    closest_token_id = torch.argmin(distances).item()\n",
    "    \n",
    "    # decode the closest token ID to get the new guess\n",
    "    y_i = tokenizer.decode(closest_token_id)\n",
    "\n",
    "# compute the LMO oracle as l2 norm of ||h-LLM(y_i)||_2 -> need oracle(y, llm, layer_idx, h)\n",
    "\n",
    "# differentiate the oracle \n",
    "\n",
    "# compute x_i+1 e_i+1 = e_i - step_size * grad(oracle)\n",
    "\n",
    "# project x_i+1 into e_i+1 -> need projection(x_i+1:tensor, E: tensor) -> e_i+1:tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710630c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "itallm2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
