{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123aa9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import from huggingface roneneldan/TinyStories-1M\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from nnsight import LanguageModel\n",
    "import torch as t\n",
    "# load garbage collection and empty cache \n",
    "import gc\n",
    "from torch.cuda import empty_cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba510893",
   "metadata": {},
   "source": [
    "# UNEMBEDDING BASELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ba736b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean():\n",
    "    gc.collect()\n",
    "    empty_cache()\n",
    "\n",
    "model_id = \"roneneldan/TinyStories-1M\"\n",
    "# model_id = \"EleutherAI/gpt-j-6b\"\n",
    "\n",
    "try:\n",
    "    del llm\n",
    "    clean()\n",
    "    llm = LanguageModel(model_id, device_map=\"cuda\", load_in_8bit=True)\n",
    "    tokenizer = llm.tokenizer\n",
    "except:\n",
    "    llm = LanguageModel(model_id, device_map=\"cuda\", load_in_8bit=True)\n",
    "    tokenizer = llm.tokenizer\n",
    "\n",
    "prompt_trial = \"1:10,2:20,3:\"\n",
    "prompt_trial = \"my name is \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833266d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def embeddings_to_texts_baseline(embeddings, model, tokenizer, skip_special_tokens=True):\n",
    "    \"\"\"\n",
    "    Map input embeddings (batch, seq_len, emb_dim) → list of decoded strings.\n",
    "    \"\"\"\n",
    "    # 1) Project embeddings to vocab logits\n",
    "    logits = model.lm_head(embeddings.to(\"cuda\"))           # (batch, seq_len, vocab_size)\n",
    "    # 2) Greedy decode: pick highest logit per position\n",
    "    token_ids = torch.argmax(logits, dim=-1)     # (batch, seq_len)\n",
    "    # 3) Transform each sequence of IDs into text\n",
    "    texts = tokenizer.batch_decode(token_ids, skip_special_tokens=skip_special_tokens)\n",
    "    return logits, texts\n",
    "\n",
    "def get_next_token_prediction(embeddings, model, tokenizer, skip_special_tokens=True):\n",
    "    # 1) Project embeddings to vocab logits\n",
    "    logits = model.lm_head(embeddings)           # (batch, seq_len, vocab_size)\n",
    "    # 2) Greedy decode: pick highest logit per position\n",
    "    token_ids = torch.argmax(logits, dim=-1)[:,-1]     # (batch, seq_len)\n",
    "    # 3) Transform each sequence of IDs into text\n",
    "    texts = tokenizer.batch_decode(token_ids, skip_special_tokens=skip_special_tokens)\n",
    "    return texts\n",
    "\n",
    "@t.inference_mode()\n",
    "def get_residual_output(prompt, layer_idx, llm, normalize = False):\n",
    "\n",
    "    assert hasattr(llm, 'transformer'), \"The model does not have a transformer attribute.\"\n",
    "    assert hasattr(llm.transformer, 'h'), \"The transformer does not have a 'h' attribute for layers.\"\n",
    "\n",
    "\n",
    "    with llm.trace(prompt):\n",
    "        residual_output = llm.transformer.h[layer_idx].output[0].save()  # Save the output of the layer for inspection\n",
    "\n",
    "    if normalize: # FIXME: check if this is the correct way to normalize\n",
    "        residual_output = llm.transformer.ln_f(residual_output)\n",
    "    \n",
    "    if llm.device.type == \"cuda\":\n",
    "        residual_output = residual_output.detach().to(\"cpu\")\n",
    "        clean()\n",
    "    \n",
    "    return residual_output\n",
    "\n",
    "@t.inference_mode()\n",
    "def get_embeddings(prompt,llm):\n",
    "\n",
    "    assert hasattr(llm, 'transformer'), \"The model does not have a transformer attribute.\"\n",
    "    assert hasattr(llm.transformer, 'drop'), \"The transformer does not have a 'drop' attribute for input embeddings.\"\n",
    "    \n",
    "    with llm.trace(prompt):\n",
    "        input_embeddings = llm.transformer.drop.input.save()\n",
    "\n",
    "    if llm.device.type == \"cuda\":\n",
    "        input_embeddings = input_embeddings.detach().to(\"cpu\")\n",
    "        clean()\n",
    "    return input_embeddings\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50818d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "@t.inference_mode()\n",
    "def get_one_token_h(prompt, layer_idx, model, tokenizer):\n",
    "    llm = LanguageModel(model, tokenizer=tokenizer)\n",
    "\n",
    "    # check that the prompt is a single token\n",
    "    assert len(tokenizer(prompt)[\"input_ids\"]) == 1, \"The prompt must be a single token.\"\n",
    "    # get the hidden representation of the prompt at layer_idx\n",
    "    with llm.trace(prompt):\n",
    "        hidden = llm.transformer.h[layer_idx].output.save()\n",
    "    if llm.device.type == \"cuda\":\n",
    "        hidden = hidden.detach().to(\"cpu\")\n",
    "        clean()\n",
    "    return hidden\n",
    "\n",
    "def get_hidden_representation(prompt, layer_idx, model, tokenizer):\n",
    "    llm = LanguageModel(model, tokenizer=tokenizer)\n",
    "    with llm.trace(prompt):\n",
    "        hidden = llm.transformer.h[layer_idx].output.save()\n",
    "    if llm.device.type == \"cuda\":\n",
    "        hidden = hidden.detach().to(\"cpu\")\n",
    "        clean()\n",
    "    return hidden[0].squeeze()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c51e30e",
   "metadata": {},
   "source": [
    "### TRYING THE DECODING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afd5ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a for loop where for each layer you try the embeddings_to_texts_baseline for each hidden state output\n",
    "full_text = True\n",
    "print(f\"{prompt_trial=} \\n\\n\")\n",
    "if full_text:   \n",
    "    reversed_embeddings = get_embeddings(prompt_trial, llm)\n",
    "    logits, texts = embeddings_to_texts_baseline(reversed_embeddings, llm, tokenizer)\n",
    "    print(f\"Reversed Embeddings for the prompt: {texts= } \\n\\n\")\n",
    "else:\n",
    "    next_token_prediction = get_next_token_prediction(get_embeddings(prompt_trial, llm), llm, tokenizer)\n",
    "    print(f\"Next token prediction for the prompt: {next_token_prediction= } \\n\\n\")\n",
    "\n",
    "\n",
    "print(f\"Iterating through each layer's output for the prompt: {prompt_trial}\\n\")\n",
    "for layer_idx in range(len(llm.transformer.h)):\n",
    "    residual_output = get_residual_output(prompt_trial, layer_idx, llm, True)\n",
    "    if full_text:\n",
    "        logits, texts = embeddings_to_texts_baseline(residual_output, llm, tokenizer)\n",
    "        print(f\"Layer {layer_idx} output texts: {texts}\")\n",
    "    else:\n",
    "        next_token_prediction = get_next_token_prediction(residual_output, llm, tokenizer)\n",
    "        print(f\"Layer {layer_idx} next token prediction: {next_token_prediction}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096d9fc7",
   "metadata": {},
   "source": [
    "# GRADIENT BASED ALGORITHM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee9e219",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def compute_last_token_embedding_grad(\n",
    "    y: torch.LongTensor,\n",
    "    llm: torch.nn.Module,\n",
    "    layer_idx: int,\n",
    "    h: torch.Tensor\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Given:\n",
    "      • y: 1D LongTensor of token IDs with shape (t,)\n",
    "      • llm: a HuggingFace‐style language model (e.g. GPT2, BERT), already loaded\n",
    "      • layer_idx: integer layer at which to “prune” and extract the hidden state\n",
    "      • h: target hidden vector of shape (hidden_size,) or (1, hidden_size)\n",
    "\n",
    "    Returns:\n",
    "      • A torch.Tensor of shape (hidden_size,) which is\n",
    "        ∂/∂e_last ( ‖z_last_layer - h‖₂² ), where z_last_layer is the model’s hidden state\n",
    "        for the last token at the requested layer.  Only the embedding of the last token\n",
    "        receives a nonzero gradient.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) Make sure the model is in eval() mode (not strictly needed for gradients,\n",
    "    #    but sets layers like dropout to eval).  Then freeze all parameters.\n",
    "    llm.eval()\n",
    "    for p in llm.parameters():\n",
    "        p.requires_grad_(False)\n",
    "\n",
    "    # 2) Prepare input IDs: ensure shape is (1, t)\n",
    "    if not isinstance(y, torch.Tensor):\n",
    "        y = torch.tensor(y, dtype=torch.long)\n",
    "    if y.dim() == 1:\n",
    "        input_ids = y.unsqueeze(0)  # now shape is (1, t)\n",
    "    else:\n",
    "        input_ids = y  # assume the user already shaped it as (1, t)\n",
    "\n",
    "    device = next(llm.parameters()).device\n",
    "    input_ids = input_ids.to(device)\n",
    "    h = h.to(device)\n",
    "    # If h has shape (hidden_size,), unsqueeze to (1, hidden_size)\n",
    "    if h.dim() == 1:\n",
    "        h = h.unsqueeze(0)\n",
    "\n",
    "    # 3) Run the embedding layer under no_grad to get embeddings, then detach.\n",
    "    #    We want to treat embeddings as a leaf variable that can require_grad for the last token.\n",
    "    with torch.no_grad():\n",
    "        # Assuming llm has a method get_input_embeddings() that returns the embedding layer\n",
    "        embed_layer = llm.get_input_embeddings()\n",
    "        full_embeddings = embed_layer(input_ids)  \n",
    "        # full_embeddings: (1, t, hidden_size)\n",
    "\n",
    "    # 4) Detach and set requires_grad=True only on full_embeddings ↦ we'll mask out everything\n",
    "    #    except the last token’s embedding when we take gradients.\n",
    "    embeddings = full_embeddings.detach().requires_grad_(True)\n",
    "    # shape: (1, t, hidden_size)\n",
    "\n",
    "    # 5) Forward‐pass through the model “up to” layer_idx.\n",
    "    #    We rely on the model’s ability to accept inputs_embeds and to return hidden_states.\n",
    "    #    Many HF models put hidden_states[0] = embeddings,\n",
    "    #                                                  hidden_states[1] = after layer 0, etc.\n",
    "    outputs = llm(\n",
    "        inputs_embeds=embeddings,\n",
    "        output_hidden_states=True,\n",
    "        return_dict=True\n",
    "    )\n",
    "    # outputs.hidden_states is a tuple of length (num_layers+1).  Indexing might differ by model:\n",
    "    #    - hidden_states[0]: the embedding output\n",
    "    #    - hidden_states[1]: after the first Transformer block\n",
    "    #    - …\n",
    "    #    - hidden_states[L]: after the L-th block, etc.\n",
    "\n",
    "    all_hidden_states = outputs.hidden_states\n",
    "    # Sanity‐check: layer_idx must be < len(all_hidden_states)\n",
    "    if layer_idx >= len(all_hidden_states):\n",
    "        raise ValueError(\n",
    "            f\"layer_idx={layer_idx} is too large; model only returns \"\n",
    "            f\"{len(all_hidden_states)} hidden states.\"\n",
    "        )\n",
    "\n",
    "    # 6) Extract the last‐token hidden vector at the requested layer.\n",
    "    #    hidden_states[layer_idx] has shape (1, t, hidden_size).\n",
    "    z_at_layer = all_hidden_states[layer_idx]  # shape (1, t, hidden_size)\n",
    "    z_last_token = z_at_layer[:, -1, :]        # shape (1, hidden_size)\n",
    "\n",
    "    # 7) Compute the (squared) L2 loss: sum((z_last_token - h)^2) \n",
    "    #    (If you want the non‐squared L2 distance, use torch.norm(z - h, p=2), but squared is simpler.)\n",
    "    #    Here we choose squared L2 because its gradient is 2*(z - h), which is fine.\n",
    "    diff = z_last_token - h             # shape (1, hidden_size)\n",
    "    loss = torch.sum(diff * diff)       # scalar\n",
    "\n",
    "    # 8) Backward pass: only embeddings require grad; all model parameters are frozen.\n",
    "    llm.zero_grad()       # clear any stored gradients (just in case)\n",
    "    loss.backward()\n",
    "\n",
    "    # 9) Now `embeddings.grad` has shape (1, t, hidden_size).  We only care about the last token.\n",
    "    grad_embeddings = embeddings.grad        # (1, t, hidden_size)\n",
    "    grad_last = grad_embeddings[:, -1, :]    # (1, hidden_size)\n",
    "    # Squeeze to make it (hidden_size,)\n",
    "    grad_last = grad_last.squeeze(0)\n",
    "\n",
    "    return grad_last\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dff6eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# import torch\n",
    "\n",
    "# # 1) Load a pretrained LLM (e.g. GPT-2)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"roneneldan/TinyStories-1M\")\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "# model     = AutoModelForCausalLM.from_pretrained(\"roneneldan/TinyStories-1M\", output_hidden_states=True)\n",
    "\n",
    "\n",
    "# # 2) Suppose y = [\"Hello\", \"world\"]  → token IDs:\n",
    "# tokens = tokenizer([\"Hello\", \"world\"], return_tensors=\"pt\", add_special_tokens=False)\n",
    "# y_ids  = tokens[\"input_ids\"].squeeze(0)   # e.g. tensor([15496, 995])\n",
    "\n",
    "# # 3) Suppose we want the target vector h to be some random “desired” hidden representation \n",
    "# #    at layer 4 (for illustration).\n",
    "# hidden_size = model.config.hidden_size          # 768 for GPT-2 small\n",
    "# h_target    = torch.randn(1, hidden_size)       # pretend this is our “objective”\n",
    "\n",
    "# # 4) Call the function:\n",
    "# grad_last_embedding = compute_last_token_embedding_grad(\n",
    "#     y=y_ids,\n",
    "#     llm=model,\n",
    "#     layer_idx=4,\n",
    "#     h=h_target\n",
    "# )\n",
    "\n",
    "# print(\"Grad shape:\", grad_last_embedding.shape)  # → torch.Size([768])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32764b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that takes as input a tokenizer and an embedding matrix and returns a random token and its embedding\n",
    "def get_random_token_and_embedding(tokenizer, embedding_matrix):\n",
    "    # Get the vocabulary size\n",
    "    vocab_size = embedding_matrix.shape[0]\n",
    "    \n",
    "    # Generate a random token ID\n",
    "    random_token_id = torch.randint(0, vocab_size, (1,)).item()\n",
    "    \n",
    "    # Get the corresponding embedding\n",
    "    embedding = embedding_matrix[random_token_id]\n",
    "    \n",
    "    # Decode the token ID to get the token string\n",
    "    token_string = tokenizer.decode(random_token_id)\n",
    "    \n",
    "    return token_string, embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8772337f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "# take as input llm, tokenizer, embedding matrix, an hidden representation, layer_idx, n_iterations, step_size, \n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roneneldan/TinyStories-1M\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"roneneldan/TinyStories-1M\", output_hidden_states=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c772133",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"George\"\n",
    "h = get_one_token_h(prompt, 4, model,tokenizer)\n",
    "embedding_matrix = model.get_input_embeddings().weight  # (vocab_size, hidden_size)\n",
    "gamma = 1  # step size for gradient descent\n",
    "layer_idx = 4  # layer at which we want to compute the gradient\n",
    "n_iterations = 100  # number of iterations for gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efef9e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"George\"\n",
    "h = get_one_token_h(prompt, 4, model,tokenizer)\n",
    "embedding_matrix = model.get_input_embeddings().weight  # (vocab_size, hidden_size)\n",
    "gamma = 1  # step size for gradient descent\n",
    "layer_idx = 4  # layer at which we want to compute the gradient\n",
    "n_iterations = 100  # number of iterations for gradient descent\n",
    "\n",
    "# make a guess for (e_0,y_0)\n",
    "y_i, x_i_plus_1= get_random_token_and_embedding(tokenizer, embedding_matrix)\n",
    "\n",
    "# loop through the n_iterations\n",
    "for iteration in (bar:=tqdm(range(n_iterations))):\n",
    "    bar.set_postfix_str(f\"Current guess: {y_i}\")\n",
    "    # compute the embedding of the current guess\n",
    "    # P_hat_i = embedding_matrix[tokenizer.encode(y_i)[0]]  # (hidden_size,)\n",
    "    \n",
    "    # compute the gradient of the oracle with respect to e_i -> inner for\n",
    "    grad_oracle = compute_last_token_embedding_grad(\n",
    "        y=torch.tensor(tokenizer.encode(y_i), dtype=torch.long), # turn into x_i_plus_1\n",
    "        llm=model,\n",
    "        layer_idx=layer_idx,\n",
    "        h=h\n",
    "    )  # (hidden_size,)\n",
    "    bar.set_postfix_str(f\"Current guess: {y_i}, Gradient norm: {grad_oracle.norm().item():.4f}\")\n",
    "\n",
    "    # update the guess using gradient descent\n",
    "    x_i_plus_1 = x_i_plus_1 - gamma * grad_oracle  # (hidden_size,)\n",
    "    \n",
    "    # find the closest token in the embedding space to e_i_plus_1\n",
    "    distances = torch.norm(embedding_matrix - x_i_plus_1, dim=1)  # (vocab_size,)\n",
    "    closest_token_id = torch.argmin(distances).item()\n",
    "    y_i = tokenizer.decode(closest_token_id)\n",
    "\n",
    "    # make a summary via prints\n",
    "    print(f\"Iteration {iteration + 1}/{n_iterations}:\")\n",
    "    print(f\"  Current guess: {y_i}\")\n",
    "    print(f\"  Gradient norm: {grad_oracle.norm().item():.4f}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768caba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"George\"\n",
    "n = 100\n",
    "# tokenizer it but return text\n",
    "h = get_hidden_representation(prompt, layer_idx, model, tokenizer)\n",
    "size = 1 if len(h.shape)==1 else h.shape[0]\n",
    "\n",
    "# discovered_tokens = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d275d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.tokenize(\"my name is George\")\n",
    "# tokenizer.encode(\"name\")\n",
    "# discovered_tokens = [\"my\", \" name\", \" is\"]\n",
    "# # merge discovered tokens and y_i in a list and a string\n",
    "# lista = discovered_tokens + [y_i]\n",
    "# stringa = \"\".join(lista)\n",
    "# y_i, discovered_tokens, lista, stringa, tokenizer.tokenize(stringa)\n",
    "# y_extend = \"\".join((discovered_tokens + [y_i]))\n",
    "# print(f\"Extended guess: {y_extend}\")\n",
    "# encoded_y = torch.tensor(tokenizer.encode(y_extend), dtype=torch.long) \n",
    "# grad_oracle = compute_last_token_embedding_grad(\n",
    "#         y=encoded_y, # turn into x_i_plus_1\n",
    "#         llm=model,\n",
    "#         layer_idx=layer_idx,\n",
    "#         h=h\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3e8c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# make a guess for (e_0,y_0)\n",
    "\n",
    "discovered_tokens = []\n",
    "for j in range(size):\n",
    "    y_i, x_i_plus_1= get_random_token_and_embedding(tokenizer, embedding_matrix)\n",
    "\n",
    "\n",
    "    # loop through the n_iterations\n",
    "    for iteration in (bar:=tqdm(range(n_iterations))):\n",
    "        bar.set_postfix_str(f\"Current guess: {y_i}\")\n",
    "        # compute the embedding of the current guess\n",
    "        # P_hat_i = embedding_matrix[tokenizer.encode(y_i)[0]]  # (hidden_size,)\n",
    "        \n",
    "        y_extend = \"\".join((discovered_tokens + [y_i]))\n",
    "        print(f\"Extended guess: {y_extend}\")\n",
    "        encoded_y = torch.tensor(tokenizer.encode(y_extend), dtype=torch.long) \n",
    "        grad_oracle = compute_last_token_embedding_grad(\n",
    "                y=encoded_y, # turn into x_i_plus_1\n",
    "                llm=model,\n",
    "                layer_idx=layer_idx,\n",
    "                h=h\n",
    "            )\n",
    "        bar.set_postfix_str(f\"Current guess: {y_i}, Gradient norm: {grad_oracle.norm().item():.4f}\")\n",
    "\n",
    "        # update the guess using gradient descent\n",
    "        x_i_plus_1 = x_i_plus_1 - gamma * grad_oracle  # (hidden_size,)\n",
    "        \n",
    "        # find the closest token in the embedding space to e_i_plus_1\n",
    "        distances = torch.norm(embedding_matrix - x_i_plus_1, dim=1)  # (vocab_size,)\n",
    "        closest_token_id = torch.argmin(distances).item()\n",
    "        y_i = tokenizer.decode(closest_token_id)\n",
    "        # if l2 norm between j of h and embedding_matrix is small break\n",
    "        if torch.norm(h[j] - embedding_matrix[closest_token_id]).item() < 1e-3:\n",
    "            print(f\"Found token {y_i} with small L2 norm: {torch.norm(h[j] - embedding_matrix[closest_token_id]).item():.4f}\")\n",
    "            break\n",
    "        # make a summary via prints\n",
    "        # print(f\"Iteration {iteration + 1}/{n_iterations}:\")\n",
    "        # print(f\"  Current guess: {y_i}\")\n",
    "        # print(f\"  Gradient norm: {grad_oracle.norm().item():.4f}\")\n",
    "\n",
    "\n",
    "    # add the discovered token to the list\n",
    "    discovered_tokens.append(y_i)\n",
    "    # print the discovered tokens\n",
    "    print(f\"Discovered tokens so far: {discovered_tokens}\")\n",
    "\n",
    "# print the final discovered tokens\n",
    "print(f\"Final discovered tokens: {discovered_tokens}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710630c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
