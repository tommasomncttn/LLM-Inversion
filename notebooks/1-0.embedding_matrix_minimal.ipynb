{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cbde96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# no need to restart kernel after code changes (useful to separate code to modules)\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35495905",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123aa9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "from utils.general import compute_last_token_embedding_grad_emb, get_whole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97a57e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int = 8):\n",
    "    \"\"\"Set seed for reproducibility.\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)  # if using multi-GPU\n",
    "\n",
    "    # Ensure deterministic behavior in cuDNN (may impact performance)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2200cf7a",
   "metadata": {},
   "source": [
    "## Torch Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175d2c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "\n",
    "model_id = 'roneneldan/TinyStories-1M'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c739dc55",
   "metadata": {},
   "source": [
    "### Gradient based on projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9468faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_token(\n",
    "    token_idx,\n",
    "    embedding_matrix,\n",
    "    discovered_embeddings, discovered_ids,\n",
    "    llm, layer_idx, h_target,\n",
    "    optimizer_cls, lr\n",
    "    \n",
    "):\n",
    "    copy_embedding_matrix = embedding_matrix.clone().detach().requires_grad_(False)\n",
    "\n",
    "    token_id = torch.randint(0, embedding_matrix.size(0), (1,)).item()\n",
    "    \n",
    "    embedding = copy_embedding_matrix[token_id].clone().requires_grad_(True)\n",
    "    temp_embedding = copy_embedding_matrix[token_id].clone().detach()\n",
    "\n",
    "    optimizer = optimizer_cls([embedding], lr=lr)\n",
    "\n",
    "    bar = tqdm(\n",
    "        range(embedding_matrix.size(0)), \n",
    "        desc=f'Token [{token_idx + 1:2d}/{h_target.size(0):2d}]'\n",
    "    )\n",
    "\n",
    "    for _ in bar:\n",
    "        input_embeddings = torch.stack(\n",
    "            discovered_embeddings + [temp_embedding]\n",
    "        ).unsqueeze(0) \n",
    "\n",
    "        grad_oracle, loss = compute_last_token_embedding_grad_emb(\n",
    "            embeddings=input_embeddings, \n",
    "            llm=llm,\n",
    "            layer_idx=layer_idx,\n",
    "            h_target=h_target[token_idx],\n",
    "        )\n",
    "\n",
    "        grad_norm = grad_oracle.norm().item()\n",
    "        string_so_far = tokenizer.decode(discovered_ids + [token_id], skip_special_tokens=True)\n",
    "        bar.set_postfix_str(f\"Loss: {loss:.2e} - Gradient norm: {grad_norm:.2e} - String: {string_so_far}\")\n",
    "\n",
    "        if loss < 1e-5 or grad_norm < 1e-12:\n",
    "            break\n",
    "\n",
    "        embedding.grad = grad_oracle\n",
    "        optimizer.step()\n",
    "\n",
    "        copy_embedding_matrix[token_id] = float('inf')\n",
    "        distances = torch.norm(copy_embedding_matrix - embedding, dim=1)\n",
    "        token_id = int(torch.argmin(distances))\n",
    "        temp_embedding = copy_embedding_matrix[token_id].clone()\n",
    "\n",
    "    return token_id, copy_embedding_matrix[token_id]\n",
    "\n",
    "\n",
    "def find_prompt(\n",
    "    llm, layer_idx, h_target,\n",
    "    optimizer_cls, lr,\n",
    "):\n",
    "    embedding_matrix = model.get_input_embeddings().weight\n",
    "\n",
    "    if h_target.dim() == 1:\n",
    "        h_target = h_target.unsqueeze(0)\n",
    "\n",
    "    discovered_embeddings = []\n",
    "    discovered_ids        = []\n",
    "\n",
    "    start_time = time()\n",
    "    for i in range(h_target.size(0)):\n",
    "        next_token_id, next_token_embedding = find_token(\n",
    "            i, embedding_matrix, \n",
    "            discovered_embeddings, discovered_ids, \n",
    "            llm, layer_idx, h_target,\n",
    "            optimizer_cls, lr\n",
    "        )\n",
    "\n",
    "        discovered_embeddings.append(next_token_embedding)\n",
    "        discovered_ids.append(next_token_id)\n",
    "    \n",
    "    end_time = time()\n",
    "\n",
    "    final_string = tokenizer.decode(discovered_ids, skip_special_tokens=True)\n",
    "\n",
    "    return end_time - start_time, final_string\n",
    "\n",
    "# from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "def inversion_attack(\n",
    "    prompt, llm, layer_idx,\n",
    "    optimizer_cls, lr,\n",
    "    seed=8\n",
    "):\n",
    "    \n",
    "    set_seed(seed)\n",
    "    h_target = get_whole(prompt, model, tokenizer, layer_idx)\n",
    "\n",
    "    invertion_time, predicted_prompt = find_prompt(\n",
    "        llm, layer_idx, h_target, \n",
    "        optimizer_cls, lr\n",
    "    )\n",
    "\n",
    "    print(f'Orignial prompt : {prompt}')\n",
    "    print(f'Predicted prompt: {predicted_prompt}')\n",
    "    print(f'Invertion time  : {invertion_time:.2f} seconds')\n",
    "\n",
    "inversion_attack(\n",
    "    # prompt='my name is george and my secret is that i have a house in greece with the key: b92n0999olaellinika',\n",
    "    prompt='12autoZeinai ena~~ !poli, a1212kiro pr33-=ompt tao op\"\\oio ;::/>elpizo na d1212isko1212leyt5646ei na ma77ntepsei to montelo',\n",
    "    # llm=model, layer_idx=4, \n",
    "    # optimizer_cls=torch.optim.SGD, lr=1e-0\n",
    "    llm=model, layer_idx=8, \n",
    "    optimizer_cls=torch.optim.AdamW, lr=1e-1\n",
    ")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e87717",
   "metadata": {},
   "source": [
    "### Batched with smart init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4c6c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def pdist_to_matrix(dists, n):\n",
    "    \"\"\"\n",
    "    Converts pdist output (condensed upper-triangle vector) into a full (n, n) distance matrix.\n",
    "    \n",
    "    Args:\n",
    "        dists (torch.Tensor): Output from torch.pdist (1D tensor)\n",
    "        n (int): Number of rows (original matrix had shape (n, d))\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: Full (n, n) symmetric distance matrix\n",
    "    \"\"\"\n",
    "    mat = torch.zeros((n, n), dtype=dists.dtype, device=dists.device)\n",
    "    idx = torch.triu_indices(n, n, offset=1)\n",
    "    mat[idx[0], idx[1]] = dists\n",
    "    mat[idx[1], idx[0]] = dists  # Make it symmetric\n",
    "    return mat\n",
    "\n",
    "x = torch.tensor([[0.0, 0.0],\n",
    "                  [1.0, 0.0],\n",
    "                  [0.0, 1.0]])\n",
    "\n",
    "d = torch.pdist(x, p=2)\n",
    "n = x.size(0)\n",
    "dist_matrix = pdist_to_matrix(d, n)\n",
    "print(dist_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cbe9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdist_index(i, j, n):\n",
    "    \"\"\"Given indices i < j and matrix size n, return index into pdist vector.\"\"\"\n",
    "    i, j = min(i, j), max(i, j)\n",
    "    assert i < j, 'Lol my g'\n",
    "    return i * (2 * n - i - 1) // 2 + (j - i - 1)\n",
    "\n",
    "\n",
    "# Dists essentially needs to be computed once in the very beginning\n",
    "# We could also try to do some dimensionality reduction of matrix \n",
    "def kmeans_pp_init(matrix):\n",
    "    dists = torch.pdist(matrix)\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def find_token(\n",
    "    token_idx,\n",
    "    embedding_matrix,\n",
    "    discovered_embeddings, discovered_ids,\n",
    "    llm, layer_idx, h_target,\n",
    "    optimizer_cls, lr,\n",
    "    B = 10\n",
    "    \n",
    "):\n",
    "    copy_embedding_matrix = embedding_matrix.clone().detach().requires_grad_(False)\n",
    "\n",
    "    token_id = torch.randint(0, embedding_matrix.size(0), (1,)).item()\n",
    "\n",
    "    # I need to sample B tokens and create a B x dim matrix\n",
    "\n",
    "    \n",
    "    embedding = copy_embedding_matrix[token_id].clone().requires_grad_(True)\n",
    "    temp_embedding = copy_embedding_matrix[token_id].clone().detach()\n",
    "\n",
    "    optimizer = optimizer_cls([embedding], lr=lr)\n",
    "\n",
    "    bar = tqdm(\n",
    "        range(embedding_matrix.size(0)), \n",
    "        desc=f'Token [{token_idx + 1:2d}/{h_target.size(0):2d}]'\n",
    "    )\n",
    "\n",
    "    for _ in bar:\n",
    "        input_embeddings = torch.stack(\n",
    "            discovered_embeddings + [temp_embedding]\n",
    "        ).unsqueeze(0) \n",
    "\n",
    "        grad_oracle, loss = compute_last_token_embedding_grad_emb(\n",
    "            embeddings=input_embeddings, \n",
    "            llm=llm,\n",
    "            layer_idx=layer_idx,\n",
    "            h_target=h_target[token_idx],\n",
    "        )\n",
    "\n",
    "        grad_norm = grad_oracle.norm().item()\n",
    "        string_so_far = tokenizer.decode(discovered_ids + [token_id], skip_special_tokens=True)\n",
    "        bar.set_postfix_str(f\"Loss: {loss:.2e} - Gradient norm: {grad_norm:.2e} - String: {string_so_far}\")\n",
    "\n",
    "        if loss < 1e-5 or grad_norm < 1e-12:\n",
    "            break\n",
    "\n",
    "        embedding.grad = grad_oracle\n",
    "        optimizer.step()\n",
    "\n",
    "        copy_embedding_matrix[token_id] = float('inf')\n",
    "        distances = torch.norm(copy_embedding_matrix - embedding, dim=1)\n",
    "        token_id = int(torch.argmin(distances))\n",
    "        temp_embedding = copy_embedding_matrix[token_id].clone()\n",
    "\n",
    "    return token_id, copy_embedding_matrix[token_id]\n",
    "\n",
    "\n",
    "def find_prompt(\n",
    "    llm, layer_idx, h_target,\n",
    "    optimizer_cls, lr,\n",
    "):\n",
    "    embedding_matrix = model.get_input_embeddings().weight\n",
    "\n",
    "    if h_target.dim() == 1:\n",
    "        h_target = h_target.unsqueeze(0)\n",
    "\n",
    "    discovered_embeddings = []\n",
    "    discovered_ids        = []\n",
    "\n",
    "    start_time = time()\n",
    "    for i in range(h_target.size(0)):\n",
    "        next_token_id, next_token_embedding = find_token(\n",
    "            i, embedding_matrix, \n",
    "            discovered_embeddings, discovered_ids, \n",
    "            llm, layer_idx, h_target,\n",
    "            optimizer_cls, lr\n",
    "        )\n",
    "\n",
    "        discovered_embeddings.append(next_token_embedding)\n",
    "        discovered_ids.append(next_token_id)\n",
    "    \n",
    "    end_time = time()\n",
    "\n",
    "    final_string = tokenizer.decode(discovered_ids, skip_special_tokens=True)\n",
    "\n",
    "    return end_time - start_time, final_string\n",
    "\n",
    "# from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "def inversion_attack(\n",
    "    prompt, llm, layer_idx,\n",
    "    optimizer_cls, lr,\n",
    "    seed=8\n",
    "):\n",
    "    \n",
    "    set_seed(seed)\n",
    "    h_target = get_whole(prompt, model, tokenizer, layer_idx)\n",
    "\n",
    "    invertion_time, predicted_prompt = find_prompt(\n",
    "        llm, layer_idx, h_target, \n",
    "        optimizer_cls, lr\n",
    "    )\n",
    "\n",
    "    print(f'Orignial prompt : {prompt}')\n",
    "    print(f'Predicted prompt: {predicted_prompt}')\n",
    "    print(f'Invertion time  : {invertion_time:.2f} seconds')\n",
    "\n",
    "inversion_attack(\n",
    "    # prompt='my name is george and my secret is that i have a house in greece with the key: b92n0999olaellinika',\n",
    "    prompt='12autoZeinai ena~~ !poli, a1212kiro pr33-=ompt tao op\"\\oio ;::/>elpizo na d1212isko1212leyt5646ei na ma77ntepsei to montelo',\n",
    "    # llm=model, layer_idx=4, \n",
    "    # optimizer_cls=torch.optim.SGD, lr=1e-0\n",
    "    llm=model, layer_idx=8, \n",
    "    optimizer_cls=torch.optim.AdamW, lr=1e-1\n",
    ")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7320dc",
   "metadata": {},
   "source": [
    "### Gradient Descent, No projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd413bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_closest_token_id(embedding_matrix, embedding):\n",
    "    distances = torch.norm(embedding_matrix - embedding, dim=1)\n",
    "    return int(torch.argmin(distances))\n",
    "\n",
    "def find_token_no_proj(\n",
    "    token_idx,\n",
    "    embedding_matrix,\n",
    "    discovered_embeddings, discovered_ids,\n",
    "    llm, layer_idx, h_target,\n",
    "    optimizer_cls, lr\n",
    "    \n",
    "):\n",
    "    copy_embedding_matrix = embedding_matrix.clone().detach().requires_grad_(False)\n",
    "\n",
    "    token_id = torch.randint(0, embedding_matrix.size(0), (1,)).item()\n",
    "    \n",
    "    embedding = torch.zeros_like(copy_embedding_matrix[token_id], requires_grad=True)\n",
    "    embedding.data = copy_embedding_matrix.data[token_id].clone()\n",
    "\n",
    "    optimizer = optimizer_cls([embedding], lr=lr)\n",
    "\n",
    "    bar = tqdm(\n",
    "        range(embedding_matrix.size(0)), \n",
    "        desc=f'Token [{token_idx + 1:2d}/{h_target.size(0):2d}]'\n",
    "    )\n",
    "\n",
    "    for _ in bar:\n",
    "        input_embeddings = torch.stack(\n",
    "            discovered_embeddings + [embedding]\n",
    "        ).unsqueeze(0) \n",
    "\n",
    "        grad_oracle, loss = compute_last_token_embedding_grad_emb(\n",
    "            embeddings=input_embeddings, \n",
    "            llm=llm,\n",
    "            layer_idx=layer_idx,\n",
    "            h_target=h_target[token_idx],\n",
    "        )\n",
    "\n",
    "        grad_norm = grad_oracle.norm().item()\n",
    "        string_so_far = tokenizer.decode(\n",
    "            discovered_ids + [get_closest_token_id(copy_embedding_matrix, embedding)], \n",
    "            skip_special_tokens=True\n",
    "        )\n",
    "        bar.set_postfix_str(f\"Loss: {loss:.2e} - Gradient norm: {grad_norm:.2e} - String: {string_so_far}\")\n",
    "\n",
    "        if loss < 1e-6 or grad_norm < 1e-12:\n",
    "            break\n",
    "\n",
    "        if loss < 1e-4:\n",
    "            token_id = get_closest_token_id(copy_embedding_matrix, embedding)\n",
    "            embedding.data = copy_embedding_matrix.data[token_id].clone()\n",
    "            copy_embedding_matrix.data[token_id] = float('inf')\n",
    "            continue\n",
    "\n",
    "        embedding.grad = grad_oracle\n",
    "        optimizer.step()\n",
    "\n",
    "    token_id = get_closest_token_id(copy_embedding_matrix, embedding)\n",
    "    return token_id, copy_embedding_matrix[token_id]\n",
    "\n",
    "\n",
    "def find_prompt_no_proj(\n",
    "    llm, layer_idx, h_target,\n",
    "    optimizer_cls, lr,\n",
    "):\n",
    "    embedding_matrix = model.get_input_embeddings().weight\n",
    "\n",
    "    if h_target.dim() == 1:\n",
    "        h_target = h_target.unsqueeze(0)\n",
    "\n",
    "    discovered_embeddings = []\n",
    "    discovered_ids        = []\n",
    "\n",
    "    start_time = time()\n",
    "    for i in range(h_target.size(0)):\n",
    "        next_token_id, next_token_embedding = find_token_no_proj(\n",
    "            i, embedding_matrix, \n",
    "            discovered_embeddings, discovered_ids, \n",
    "            llm, layer_idx, h_target,\n",
    "            optimizer_cls, lr\n",
    "        )\n",
    "\n",
    "        discovered_embeddings.append(next_token_embedding)\n",
    "        discovered_ids.append(next_token_id)\n",
    "    \n",
    "    end_time = time()\n",
    "\n",
    "    final_string = tokenizer.decode(discovered_ids, skip_special_tokens=True)\n",
    "\n",
    "    return end_time - start_time, final_string\n",
    "\n",
    "# from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "def inversion_attack_no_proj(\n",
    "    prompt, llm, layer_idx,\n",
    "    optimizer_cls, lr,\n",
    "    seed=8\n",
    "):\n",
    "    \n",
    "    set_seed(seed)\n",
    "    h_target = get_whole(prompt, model, tokenizer, layer_idx)\n",
    "\n",
    "    invertion_time, predicted_prompt = find_prompt_no_proj(\n",
    "        llm, layer_idx, h_target, \n",
    "        optimizer_cls, lr\n",
    "    )\n",
    "\n",
    "    print(f'Orignial prompt : {prompt}')\n",
    "    print(f'Predicted prompt: {predicted_prompt}')\n",
    "    print(f'Invertion time  : {invertion_time:.2f} seconds')\n",
    "\n",
    "inversion_attack_no_proj(\n",
    "    prompt='my name is george and my secret is that i have a house in greece with the key: b92n0999olaellinika',\n",
    "    llm=model, layer_idx=4, \n",
    "    optimizer_cls=torch.optim.Adam, lr=1e-2\n",
    ")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec417523",
   "metadata": {},
   "source": [
    "### PGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259a7a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_token(\n",
    "    token_idx,\n",
    "    embedding_matrix,\n",
    "    discovered_embeddings, discovered_ids,\n",
    "    llm, layer_idx, h_target,\n",
    "    optimizer_cls, lr\n",
    "    \n",
    "):\n",
    "    copy_embedding_matrix = embedding_matrix.clone().detach().requires_grad_(False)\n",
    "\n",
    "    token_id = torch.randint(0, embedding_matrix.size(0), (1,)).item()\n",
    "    embedding = copy_embedding_matrix[token_id].clone().requires_grad_(True)\n",
    "\n",
    "    optimizer = optimizer_cls([embedding], lr=lr)\n",
    "\n",
    "    bar = tqdm(\n",
    "        range(embedding_matrix.size(0)), \n",
    "        desc=f'Token [{token_idx + 1:2d}/{h_target.size(0):2d}]'\n",
    "    )\n",
    "\n",
    "    for _ in bar:\n",
    "        input_embeddings = torch.stack(\n",
    "            discovered_embeddings + [embedding]\n",
    "        ).unsqueeze(0) \n",
    "\n",
    "        grad_oracle, loss = compute_last_token_embedding_grad_emb(\n",
    "            embeddings=input_embeddings, \n",
    "            llm=llm,\n",
    "            layer_idx=layer_idx,\n",
    "            h_target=h_target[token_idx],\n",
    "        )\n",
    "\n",
    "        grad_norm = grad_oracle.norm().item()\n",
    "        string_so_far = tokenizer.decode(discovered_ids + [token_id], skip_special_tokens=True)\n",
    "        bar.set_postfix_str(f\"Loss: {loss:.2e} - Gradient norm: {grad_norm:.2e} - String: {string_so_far}\")\n",
    "\n",
    "        if loss < 1e-5 or grad_norm < 1e-12:\n",
    "            break\n",
    "\n",
    "        embedding.grad = grad_oracle\n",
    "        optimizer.step()\n",
    "\n",
    "        copy_embedding_matrix.data[token_id] = float('inf')\n",
    "        token_id = get_closest_token_id(copy_embedding_matrix, embedding)\n",
    "        embedding.data = copy_embedding_matrix.data[token_id].clone()\n",
    "\n",
    "    return token_id, copy_embedding_matrix[token_id]\n",
    "\n",
    "\n",
    "def find_prompt(\n",
    "    llm, layer_idx, h_target,\n",
    "    optimizer_cls, lr,\n",
    "):\n",
    "    embedding_matrix = model.get_input_embeddings().weight\n",
    "\n",
    "    if h_target.dim() == 1:\n",
    "        h_target = h_target.unsqueeze(0)\n",
    "\n",
    "    discovered_embeddings = []\n",
    "    discovered_ids        = []\n",
    "\n",
    "    start_time = time()\n",
    "    for i in range(h_target.size(0)):\n",
    "        next_token_id, next_token_embedding = find_token(\n",
    "            i, embedding_matrix, \n",
    "            discovered_embeddings, discovered_ids, \n",
    "            llm, layer_idx, h_target,\n",
    "            optimizer_cls, lr\n",
    "        )\n",
    "\n",
    "        discovered_embeddings.append(next_token_embedding)\n",
    "        discovered_ids.append(next_token_id)\n",
    "    \n",
    "    end_time = time()\n",
    "\n",
    "    final_string = tokenizer.decode(discovered_ids, skip_special_tokens=True)\n",
    "\n",
    "    return end_time - start_time, final_string\n",
    "\n",
    "# from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "def inversion_attack(\n",
    "    prompt, llm, layer_idx,\n",
    "    optimizer_cls, lr,\n",
    "    seed=8\n",
    "):\n",
    "    \n",
    "    set_seed(seed)\n",
    "    h_target = get_whole(prompt, model, tokenizer, layer_idx)\n",
    "\n",
    "    invertion_time, predicted_prompt = find_prompt(\n",
    "        llm, layer_idx, h_target, \n",
    "        optimizer_cls, lr\n",
    "    )\n",
    "\n",
    "    print(f'Orignial prompt : {prompt}')\n",
    "    print(f'Predicted prompt: {predicted_prompt}')\n",
    "    print(f'Invertion time  : {invertion_time:.2f} seconds')\n",
    "\n",
    "inversion_attack(\n",
    "    # prompt='my name is george and my secret is that i have a house in greece with the key: b92n0999olaellinika',\n",
    "    prompt='12autoZeinai ena~~ !poli, a1212kiro pr33-=ompt tao op\"\\oio ;::/>elpizo na d1212isko1212leyt5646ei na ma77ntepsei to montelo',\n",
    "    llm=model, layer_idx=2, \n",
    "    # optimizer_cls=torch.optim.SGD, lr=1e+1,\n",
    "    optimizer_cls=torch.optim.Adam, lr=1e-0,\n",
    ")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfc7e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def pairwise_distance_matrix(x):\n",
    "    \"\"\"\n",
    "    Computes full pairwise Euclidean distance matrix using torch.pdist.\n",
    "    \n",
    "    Args:\n",
    "        x (torch.Tensor): Input tensor of shape (n, d)\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: Full (n, n) matrix of pairwise distances\n",
    "    \"\"\"\n",
    "    dists = torch.pdist(x, p=2)  # Condensed vector form\n",
    "    print(dists.shape)\n",
    "    print(dists)\n",
    "    n = x.size(0)\n",
    "    \n",
    "    # Create a full (n x n) distance matrix and fill upper triangle\n",
    "    dist_matrix = torch.zeros((n, n), device=x.device, dtype=x.dtype)\n",
    "    idx = torch.triu_indices(n, n, offset=1)\n",
    "    dist_matrix[idx[0], idx[1]] = dists\n",
    "    \n",
    "    # Mirror to the lower triangle\n",
    "    # dist_matrix = dist_matrix + dist_matrix.T\n",
    "    return dist_matrix\n",
    "\n",
    "x = torch.tensor([[0.0, 0.0],\n",
    "                  [1.0, 0.0],\n",
    "                  [1.0, 1.0],\n",
    "                  [0.0, 1.0]])\n",
    "\n",
    "points = x.size(0)\n",
    "\n",
    "i = 1\n",
    "j = 2\n",
    "idx = i * points + j \n",
    "\n",
    "dist_matrix = pairwise_distance_matrix(x)\n",
    "print(dist_matrix)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "itallm2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
