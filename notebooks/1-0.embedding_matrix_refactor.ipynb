{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "123aa9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import from huggingface roneneldan/TinyStories-1M\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from nnsight import LanguageModel\n",
    "import torch as t\n",
    "# load garbage collection and empty cache \n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba510893",
   "metadata": {},
   "source": [
    "# UNEMBEDDING BASELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8ba736b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean():\n",
    "    gc.collect()\n",
    "    # empty_cache()\n",
    "\n",
    "model_id = \"roneneldan/TinyStories-1M\"\n",
    "# model_id = \"EleutherAI/gpt-j-6b\"\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "load_in_8bit = False\n",
    "\n",
    "try:\n",
    "    del llm\n",
    "    clean()\n",
    "    llm = LanguageModel(model_id, device_map=device, load_in_8bit=load_in_8bit)\n",
    "    tokenizer = llm.tokenizer\n",
    "except:\n",
    "    llm = LanguageModel(model_id, device_map=device, load_in_8bit=load_in_8bit)\n",
    "    tokenizer = llm.tokenizer\n",
    "\n",
    "prompt_trial = \"1:10,2:20,3:\"\n",
    "prompt_trial = \"my name is \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "833266d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def embeddings_to_texts_baseline(embeddings, model, tokenizer, skip_special_tokens=True):\n",
    "    \"\"\"\n",
    "    Map input embeddings (batch, seq_len, emb_dim) → list of decoded strings.\n",
    "    \"\"\"\n",
    "    # 1) Project embeddings to vocab logits\n",
    "    logits = model.lm_head(embeddings.to(device))           # (batch, seq_len, vocab_size)\n",
    "    # 2) Greedy decode: pick highest logit per position\n",
    "    token_ids = torch.argmax(logits, dim=-1)     # (batch, seq_len)\n",
    "    # 3) Transform each sequence of IDs into text\n",
    "    texts = tokenizer.batch_decode(token_ids, skip_special_tokens=skip_special_tokens)\n",
    "    return logits, texts\n",
    "\n",
    "def get_next_token_prediction(embeddings, model, tokenizer, skip_special_tokens=True):\n",
    "    # 1) Project embeddings to vocab logits\n",
    "    logits = model.lm_head(embeddings)           # (batch, seq_len, vocab_size)\n",
    "    # 2) Greedy decode: pick highest logit per position\n",
    "    token_ids = torch.argmax(logits, dim=-1)[:,-1]     # (batch, seq_len)\n",
    "    # 3) Transform each sequence of IDs into text\n",
    "    texts = tokenizer.batch_decode(token_ids, skip_special_tokens=skip_special_tokens)\n",
    "    return texts\n",
    "\n",
    "@t.inference_mode()\n",
    "def get_residual_output(prompt, layer_idx, llm, normalize = False):\n",
    "\n",
    "    assert hasattr(llm, 'transformer'), \"The model does not have a transformer attribute.\"\n",
    "    assert hasattr(llm.transformer, 'h'), \"The transformer does not have a 'h' attribute for layers.\"\n",
    "\n",
    "\n",
    "    with llm.trace(prompt):\n",
    "        residual_output = llm.transformer.h[layer_idx].output[0].save()  # Save the output of the layer for inspection\n",
    "\n",
    "    if normalize: # FIXME: check if this is the correct way to normalize\n",
    "        residual_output = llm.transformer.ln_f(residual_output)\n",
    "    \n",
    "    if llm.device.type == \"cuda\":\n",
    "        residual_output = residual_output.detach().cpu()\n",
    "        clean()\n",
    "    \n",
    "    return residual_output\n",
    "\n",
    "@t.inference_mode()\n",
    "def get_embeddings(prompt,llm):\n",
    "\n",
    "    assert hasattr(llm, 'transformer'), \"The model does not have a transformer attribute.\"\n",
    "    assert hasattr(llm.transformer, 'drop'), \"The transformer does not have a 'drop' attribute for input embeddings.\"\n",
    "    \n",
    "    with llm.trace(prompt):\n",
    "        input_embeddings = llm.transformer.drop.input.save()\n",
    "\n",
    "    if llm.device.type == \"cuda\":\n",
    "        input_embeddings = input_embeddings.detach().to(\"cpu\")\n",
    "        clean()\n",
    "    return input_embeddings\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50818d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "@t.inference_mode()\n",
    "def get_one_token_h(prompt, layer_idx, model, tokenizer):\n",
    "    llm = LanguageModel(model, tokenizer=tokenizer) if not isinstance(model, LanguageModel) else model\n",
    "\n",
    "    # check that the prompt is a single token\n",
    "    assert len(tokenizer(prompt)[\"input_ids\"]) == 1, \"The prompt must be a single token.\"\n",
    "    # get the hidden representation of the prompt at layer_idx\n",
    "    with llm.trace(prompt):\n",
    "        hidden = llm.transformer.h[layer_idx].output.save()\n",
    "    if llm.device.type == \"cuda\":\n",
    "        hidden = hidden.detach().cpu()\n",
    "        clean()\n",
    "    return hidden\n",
    "\n",
    "@t.inference_mode()\n",
    "def get_hidden_representation(prompt, layer_idx, model, tokenizer):\n",
    "    llm = LanguageModel(model, tokenizer=tokenizer) if not isinstance(model, LanguageModel) else model\n",
    "    with llm.trace(prompt):\n",
    "        hidden = llm.transformer.h[layer_idx].output.save()\n",
    "    if llm.device.type == \"cuda\":\n",
    "        hidden = hidden.detach().cpu()\n",
    "        clean()\n",
    "    return hidden[0].squeeze()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c51e30e",
   "metadata": {},
   "source": [
    "### TRYING THE DECODING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3afd5ee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt_trial='my name is ' \n",
      "\n",
      "\n",
      "Next token prediction for the prompt: next_token_prediction= [' '] \n",
      "\n",
      "\n",
      "Iterating through each layer's output for the prompt: my name is \n",
      "\n",
      "Layer 0 next token prediction: [' ']\n",
      "Layer 1 next token prediction: [' ']\n",
      "Layer 2 next token prediction: [' upon']\n",
      "Layer 3 next token prediction: ['.']\n",
      "Layer 4 next token prediction: [' own']\n",
      "Layer 5 next token prediction: [' I']\n",
      "Layer 6 next token prediction: ['.']\n",
      "Layer 7 next token prediction: ['\\n']\n"
     ]
    }
   ],
   "source": [
    "# make a for loop where for each layer you try the embeddings_to_texts_baseline for each hidden state output\n",
    "full_text = False\n",
    "print(f\"{prompt_trial=} \\n\\n\")\n",
    "if full_text:   \n",
    "    reversed_embeddings = get_embeddings(prompt_trial, llm)\n",
    "    logits, texts = embeddings_to_texts_baseline(reversed_embeddings, llm, tokenizer)\n",
    "    print(f\"Reversed Embeddings for the prompt: {texts= } \\n\\n\")\n",
    "else:\n",
    "    next_token_prediction = get_next_token_prediction(get_embeddings(prompt_trial, llm), llm, tokenizer)\n",
    "    print(f\"Next token prediction for the prompt: {next_token_prediction= } \\n\\n\")\n",
    "\n",
    "\n",
    "print(f\"Iterating through each layer's output for the prompt: {prompt_trial}\\n\")\n",
    "for layer_idx in range(len(llm.transformer.h)):\n",
    "    residual_output = get_residual_output(prompt_trial, layer_idx, llm, True)\n",
    "    if full_text:\n",
    "        logits, texts = embeddings_to_texts_baseline(residual_output, llm, tokenizer)\n",
    "        print(f\"Layer {layer_idx} output texts: {texts}\")\n",
    "    else:\n",
    "        next_token_prediction = get_next_token_prediction(residual_output, llm, tokenizer)\n",
    "        print(f\"Layer {layer_idx} next token prediction: {next_token_prediction}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "734142f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def get_whole1(\n",
    "    prompt: str,\n",
    "    llm: torch.nn.Module,\n",
    "    tokenizer,\n",
    "    layer_idx: int\n",
    ") -> torch.Tensor:\n",
    "    # 1) Tokenize (and move to same device as the model).\n",
    "    device = next(llm.parameters()).device\n",
    "    encoded = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input_ids = encoded[\"input_ids\"].to(device)      # shape (1, seq_len)\n",
    "    attention_mask = encoded.get(\"attention_mask\", None)\n",
    "    if attention_mask is not None:\n",
    "        attention_mask = attention_mask.to(device)\n",
    "\n",
    "    # 2) Forward pass under no_grad, requesting hidden_states\n",
    "    with torch.no_grad():\n",
    "        outputs = llm(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            output_hidden_states=True,\n",
    "        )\n",
    "        hidden_states = outputs.hidden_states\n",
    "        # hidden_states is a tuple: (layer0, layer1, ..., layerN),\n",
    "        # where layer0 = embeddings output, layer1 = first transformer block, etc.\n",
    "\n",
    "        # 3) Index into [0, -1, :] to get the last token for this single‐batch example\n",
    "        #    hidden_states[layer_idx] has shape (batch_size=1, seq_len, hidden_size).\n",
    "        return hidden_states[layer_idx][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71f436e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 64])\n",
      "torch.Size([2, 64])\n",
      "tensor(0.)\n",
      "\n",
      "torch.Size([5, 64])\n",
      "torch.Size([2, 64])\n",
      "tensor(0.)\n",
      "\n",
      "torch.Size([5, 64])\n",
      "torch.Size([2, 64])\n",
      "tensor(0.)\n",
      "\n",
      "torch.Size([5, 64])\n",
      "torch.Size([2, 64])\n",
      "tensor(0.)\n",
      "\n",
      "torch.Size([5, 64])\n",
      "torch.Size([2, 64])\n",
      "tensor(0.)\n",
      "\n",
      "torch.Size([5, 64])\n",
      "torch.Size([2, 64])\n",
      "tensor(0.)\n",
      "\n",
      "torch.Size([5, 64])\n",
      "torch.Size([2, 64])\n",
      "tensor(0.)\n",
      "\n",
      "torch.Size([5, 64])\n",
      "torch.Size([2, 64])\n",
      "tensor(0.)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# make a for loop where for each layer you try the embeddings_to_texts_baseline for each hidden state output\n",
    "prompt = 'my name is george'\n",
    "prefix = 'my name'\n",
    "\n",
    "for layer_idx in range(len(llm.transformer.h)):\n",
    "    # residual_output1 = get_residual_output(prompt, layer_idx, llm, True)\n",
    "    # residual_output2 = get_residual_output(prefix, layer_idx, llm, True)\n",
    "\n",
    "    # residual_output1 = get_hidden_representation(prompt, layer_idx, llm, tokenizer)\n",
    "    # residual_output2 = get_hidden_representation(prefix, layer_idx, llm, tokenizer)\n",
    "\n",
    "    residual_output1 = get_whole1(prompt, llm, tokenizer, layer_idx)\n",
    "    residual_output2 = get_whole1(prefix, llm, tokenizer, layer_idx)\n",
    "\n",
    "    print(residual_output1.shape)\n",
    "    print(residual_output2.shape)\n",
    "    print(torch.mean((residual_output1[:2,:] - residual_output2) ** 2))\n",
    "    print()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096d9fc7",
   "metadata": {},
   "source": [
    "# GRADIENT BASED ALGORITHM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9ee9e219",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def compute_last_token_embedding_grad(\n",
    "    y: torch.LongTensor,\n",
    "    llm: torch.nn.Module,\n",
    "    layer_idx: int,\n",
    "    h: torch.Tensor\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Given:\n",
    "      • y: 1D LongTensor of token IDs with shape (t,)\n",
    "      • llm: a HuggingFace‐style language model (e.g. GPT2, BERT), already loaded\n",
    "      • layer_idx: integer layer at which to “prune” and extract the hidden state\n",
    "      • h: target hidden vector of shape (hidden_size,) or (1, hidden_size)\n",
    "\n",
    "    Returns:\n",
    "      • A torch.Tensor of shape (hidden_size,) which is\n",
    "        ∂/∂e_last ( ‖z_last_layer - h‖₂² ), where z_last_layer is the model’s hidden state\n",
    "        for the last token at the requested layer.  Only the embedding of the last token\n",
    "        receives a nonzero gradient.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) Make sure the model is in eval() mode (not strictly needed for gradients,\n",
    "    #    but sets layers like dropout to eval).  Then freeze all parameters.\n",
    "    llm.eval()\n",
    "    for p in llm.parameters():\n",
    "        p.requires_grad_(False)\n",
    "\n",
    "    # 2) Prepare input IDs: ensure shape is (1, t)\n",
    "    if not isinstance(y, torch.Tensor):\n",
    "        y = torch.tensor(y, dtype=torch.long)\n",
    "    if y.dim() == 1:\n",
    "        input_ids = y.unsqueeze(0)  # now shape is (1, t)\n",
    "    else:\n",
    "        input_ids = y  # assume the user already shaped it as (1, t)\n",
    "\n",
    "    device = next(llm.parameters()).device\n",
    "    input_ids = input_ids.to(device)\n",
    "    h = h.to(device)\n",
    "    # If h has shape (hidden_size,), unsqueeze to (1, hidden_size)\n",
    "    if h.dim() == 1:\n",
    "        h = h.unsqueeze(0)\n",
    "\n",
    "    # 3) Run the embedding layer under no_grad to get embeddings, then detach.\n",
    "    #    We want to treat embeddings as a leaf variable that can require_grad for the last token.\n",
    "    with torch.no_grad():\n",
    "        # Assuming llm has a method get_input_embeddings() that returns the embedding layer\n",
    "        embed_layer = llm.get_input_embeddings()\n",
    "        full_embeddings = embed_layer(input_ids)  \n",
    "        # full_embeddings: (1, t, hidden_size)\n",
    "\n",
    "    # 4) Detach and set requires_grad=True only on full_embeddings ↦ we'll mask out everything\n",
    "    #    except the last token’s embedding when we take gradients.\n",
    "    embeddings = full_embeddings.detach().requires_grad_(True)\n",
    "    # shape: (1, t, hidden_size)\n",
    "\n",
    "    # 5) Forward‐pass through the model “up to” layer_idx.\n",
    "    #    We rely on the model’s ability to accept inputs_embeds and to return hidden_states.\n",
    "    #    Many HF models put hidden_states[0] = embeddings,\n",
    "    #                                                  hidden_states[1] = after layer 0, etc.\n",
    "    outputs = llm(\n",
    "        inputs_embeds=embeddings,\n",
    "        output_hidden_states=True,\n",
    "        return_dict=True\n",
    "    )\n",
    "    # outputs.hidden_states is a tuple of length (num_layers+1).  Indexing might differ by model:\n",
    "    #    - hidden_states[0]: the embedding output\n",
    "    #    - hidden_states[1]: after the first Transformer block\n",
    "    #    - …\n",
    "    #    - hidden_states[L]: after the L-th block, etc.\n",
    "\n",
    "    all_hidden_states = outputs.hidden_states\n",
    "    # Sanity‐check: layer_idx must be < len(all_hidden_states)\n",
    "    if layer_idx >= len(all_hidden_states):\n",
    "        raise ValueError(\n",
    "            f\"layer_idx={layer_idx} is too large; model only returns \"\n",
    "            f\"{len(all_hidden_states)} hidden states.\"\n",
    "        )\n",
    "\n",
    "    # 6) Extract the last‐token hidden vector at the requested layer.\n",
    "    #    hidden_states[layer_idx] has shape (1, t, hidden_size).\n",
    "    z_at_layer = all_hidden_states[layer_idx]  # shape (1, t, hidden_size)\n",
    "    z_last_token = z_at_layer[:, -1, :]        # shape (1, hidden_size)\n",
    "\n",
    "    # 7) Compute the (squared) L2 loss: sum((z_last_token - h)^2) \n",
    "    #    (If you want the non‐squared L2 distance, use torch.norm(z - h, p=2), but squared is simpler.)\n",
    "    #    Here we choose squared L2 because its gradient is 2*(z - h), which is fine.\n",
    "    diff = z_last_token - h             # shape (1, hidden_size)\n",
    "    loss = torch.sum(diff * diff)       # scalar\n",
    "\n",
    "    # 8) Backward pass: only embeddings require grad; all model parameters are frozen.\n",
    "    llm.zero_grad()       # clear any stored gradients (just in case)\n",
    "    loss.backward()\n",
    "\n",
    "    # 9) Now `embeddings.grad` has shape (1, t, hidden_size).  We only care about the last token.\n",
    "    grad_embeddings = embeddings.grad        # (1, t, hidden_size)\n",
    "    grad_last = grad_embeddings[:, -1, :]    # (1, hidden_size)\n",
    "    # Squeeze to make it (hidden_size,)\n",
    "    grad_last = grad_last.squeeze(0)\n",
    "\n",
    "    return grad_last, loss.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32764b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that takes as input a tokenizer and an embedding matrix and returns a random token and its embedding\n",
    "def get_random_token_and_embedding(tokenizer, embedding_matrix):\n",
    "    # Get the vocabulary size\n",
    "    vocab_size = embedding_matrix.shape[0]\n",
    "    \n",
    "    # Generate a random token ID\n",
    "    random_token_id = torch.randint(0, vocab_size, (1,)).item()\n",
    "    \n",
    "    # Get the corresponding embedding\n",
    "    embedding = embedding_matrix[random_token_id]\n",
    "    \n",
    "    # Decode the token ID to get the token string\n",
    "    token_string = tokenizer.decode(random_token_id)\n",
    "    \n",
    "    return token_string, embedding\n",
    "\n",
    "# function that takes as input a tokenizer and an embedding matrix and returns a random token and its embedding\n",
    "def get_random_token_id_and_embedding(embedding_matrix):\n",
    "    # Get the vocabulary size\n",
    "    vocab_size = embedding_matrix.shape[0]\n",
    "    \n",
    "    # Generate a random token ID\n",
    "    random_token_id = torch.randint(0, vocab_size, (1,)).item()\n",
    "    \n",
    "    # Get the corresponding embedding\n",
    "    embedding = embedding_matrix[random_token_id]\n",
    "\n",
    "    \n",
    "    return random_token_id, embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8772337f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/giorgosnikolaou/Desktop/Research/LLM-Inversion/.venv/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:820: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_hidden_states` is. When `return_dict_in_generate` is not `True`, `output_hidden_states` is ignored.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "# take as input llm, tokenizer, embedding matrix, an hidden representation, layer_idx, n_iterations, step_size, \n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roneneldan/TinyStories-1M\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"roneneldan/TinyStories-1M\", output_hidden_states=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7c772133",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"George\"\n",
    "h = get_one_token_h(prompt, 4, model,tokenizer)\n",
    "embedding_matrix = model.get_input_embeddings().weight  # (vocab_size, hidden_size)\n",
    "gamma = 1  # step size for gradient descent\n",
    "layer_idx = 4  # layer at which we want to compute the gradient\n",
    "n_iterations = 100  # number of iterations for gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efef9e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"George\"\n",
    "h = get_one_token_h(prompt, 4, model,tokenizer)\n",
    "embedding_matrix = model.get_input_embeddings().weight  # (vocab_size, hidden_size)\n",
    "gamma = 1  # step size for gradient descent\n",
    "layer_idx = 4  # layer at which we want to compute the gradient\n",
    "n_iterations = 100  # number of iterations for gradient descent\n",
    "\n",
    "# make a guess for (e_0,y_0)\n",
    "y_i, x_i_plus_1= get_random_token_and_embedding(tokenizer, embedding_matrix)\n",
    "\n",
    "# loop through the n_iterations\n",
    "for iteration in (bar:=tqdm(range(n_iterations))):\n",
    "    bar.set_postfix_str(f\"Current guess: {y_i}\")\n",
    "    # compute the embedding of the current guess\n",
    "    # P_hat_i = embedding_matrix[tokenizer.encode(y_i)[0]]  # (hidden_size,)\n",
    "    \n",
    "    # compute the gradient of the oracle with respect to e_i -> inner for\n",
    "    grad_oracle = compute_last_token_embedding_grad(\n",
    "        y=torch.tensor(tokenizer.encode(y_i), dtype=torch.long), # turn into x_i_plus_1\n",
    "        llm=model,\n",
    "        layer_idx=layer_idx,\n",
    "        h=h\n",
    "    )  # (hidden_size,)\n",
    "    bar.set_postfix_str(f\"Current guess: {y_i}, Gradient norm: {grad_oracle.norm().item():.4f}\")\n",
    "\n",
    "    # update the guess using gradient descent\n",
    "    x_i_plus_1 = x_i_plus_1 - gamma * grad_oracle  # (hidden_size,)\n",
    "    \n",
    "    # find the closest token in the embedding space to e_i_plus_1\n",
    "    distances = torch.norm(embedding_matrix - x_i_plus_1, dim=1)  # (vocab_size,)\n",
    "    closest_token_id = torch.argmin(distances).item()\n",
    "    y_i = tokenizer.decode(closest_token_id)\n",
    "\n",
    "    # make a summary via prints\n",
    "    print(f\"Iteration {iteration + 1}/{n_iterations}:\")\n",
    "    print(f\"  Current guess: {y_i}\")\n",
    "    print(f\"  Gradient norm: {grad_oracle.norm().item():.4f}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "768caba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"George\"\n",
    "n = 100\n",
    "# tokenizer it but return text\n",
    "h = get_hidden_representation(prompt, layer_idx, model, tokenizer)\n",
    "size = 1 if len(h.shape)==1 else h.shape[0]\n",
    "\n",
    "# discovered_tokens = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "4d275d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.tokenize(\"my name is George\")\n",
    "# tokenizer.encode(\"name\")\n",
    "# discovered_tokens = [\"my\", \" name\", \" is\"]\n",
    "# # merge discovered tokens and y_i in a list and a string\n",
    "# lista = discovered_tokens + [y_i]\n",
    "# stringa = \"\".join(lista)\n",
    "# y_i, discovered_tokens, lista, stringa, tokenizer.tokenize(stringa)\n",
    "# y_extend = \"\".join((discovered_tokens + [y_i]))\n",
    "# print(f\"Extended guess: {y_extend}\")\n",
    "# encoded_y = torch.tensor(tokenizer.encode(y_extend), dtype=torch.long) \n",
    "# grad_oracle = compute_last_token_embedding_grad(\n",
    "#         y=encoded_y, # turn into x_i_plus_1\n",
    "#         llm=model,\n",
    "#         layer_idx=layer_idx,\n",
    "#         h=h\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73a682d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "from typing import Optional\n",
    "\n",
    "def get_whole(\n",
    "    prompt: str,\n",
    "    llm: torch.nn.Module,\n",
    "    tokenizer,\n",
    "    layer_idx: int,\n",
    "    input_ids: Optional[torch.Tensor] = None,\n",
    "    grad: bool = False\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Tokenize `prompt`, do a forward pass with output_hidden_states=True,\n",
    "    and return the hidden vector of the *last* token at layer `layer_idx`.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): the input string, e.g. \"Harry\".\n",
    "        llm (nn.Module): a HuggingFace‐style model (with embeddings + hidden_states).\n",
    "        tokenizer: the corresponding tokenizer for `llm`.\n",
    "        layer_idx (int): which hidden‐layer index to extract (0=embeddings, 1=first block, etc.)\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape (hidden_size,) = the last‐token hidden state at `layer_idx`,\n",
    "        computed under torch.no_grad().\n",
    "    \"\"\"\n",
    "    # 1) Tokenize (and move to same device as the model).\n",
    "    device = next(llm.parameters()).device\n",
    "    if input_ids is None:\n",
    "        encoded = tokenizer(prompt, return_tensors=\"pt\")\n",
    "        input_ids = encoded[\"input_ids\"].to(device)      # shape (1, seq_len)\n",
    "    \n",
    "    if not grad:\n",
    "        # Forward under no_grad and detach before returning\n",
    "        with torch.no_grad():\n",
    "            outputs = llm(\n",
    "                input_ids=input_ids,\n",
    "                output_hidden_states=True\n",
    "            )\n",
    "            hidden_states = outputs.hidden_states\n",
    "            h = hidden_states[layer_idx][0]  # shape = (seq_len, hidden_size)\n",
    "\n",
    "        return h.detach()\n",
    "    else:\n",
    "        # Forward normally, so gradients can flow\n",
    "        outputs = llm(\n",
    "            input_ids=input_ids,\n",
    "            output_hidden_states=True\n",
    "        )\n",
    "        hidden_states = outputs.hidden_states\n",
    "        h = hidden_states[layer_idx][0]  # shape = (seq_len, hidden_size)\n",
    "        return h\n",
    "\n",
    "\n",
    "def compute_last_token_embedding_grad(\n",
    "    y: torch.LongTensor,\n",
    "    llm: torch.nn.Module,\n",
    "    layer_idx: int,\n",
    "    h_target: torch.Tensor\n",
    "):\n",
    "    device = next(llm.parameters()).device\n",
    "    y = y.to(device)\n",
    "    h_target = h_target.to(device)\n",
    "\n",
    "    emb_layer = llm.get_input_embeddings()\n",
    "    if not emb_layer.weight.requires_grad:\n",
    "        emb_layer.weight.requires_grad_(True)\n",
    "\n",
    "    llm.zero_grad()\n",
    "    emb_layer.zero_grad()\n",
    "\n",
    "    with torch.set_grad_enabled(True):\n",
    "        h_last = get_whole('', llm, tokenizer, layer_idx, y.unsqueeze(0), grad=True)[-1]\n",
    "        diff = h_last - h_target\n",
    "        loss = torch.dot(diff, diff)\n",
    "        loss.backward()\n",
    "\n",
    "    last_token_id = y[-1].item()\n",
    "    grad_last_embedding = emb_layer.weight.grad[last_token_id].detach().clone() # TODO: Is this gradient with respect to the input or not?\n",
    "\n",
    "    llm.zero_grad()\n",
    "    emb_layer.zero_grad()\n",
    "\n",
    "    return grad_last_embedding, loss.item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c380f235",
   "metadata": {},
   "source": [
    "## Vanilla Algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e3e8c22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Token [ 0/29]:   0%|          | 17/5000 [00:00<00:56, 88.34it/s, Loss: 1.94e-09 - Gradient norm: 5.92e-03 - String: my]     \n",
      "Token [ 1/29]:   0%|          | 6/5000 [00:00<00:59, 84.02it/s, Loss: 2.97e-09 - Gradient norm: 1.94e-02 - String: my name]\n",
      "Token [ 2/29]:  27%|██▋       | 1353/5000 [00:13<00:36, 101.17it/s, Loss: 8.78e+02 - Gradient norm: 4.16e+03 - String: my name and]      \n",
      "\n",
      "Token [ 2/29]:  27%|██▋       | 1353/5000 [00:13<00:36, 101.17it/s, Loss: 1.03e+03 - Gradient norm: 3.54e+03 - String: my name he]\n",
      "Token [ 2/29]:  27%|██▋       | 1358/5000 [00:13<00:35, 101.49it/s, Loss: 6.09e-09 - Gradient norm: 1.91e-02 - String: my name is]\n",
      "Token [ 3/29]:   0%|          | 8/5000 [00:00<00:54, 92.37it/s, Loss: 3.36e-09 - Gradient norm: 8.22e-03 - String: my name is ge]\n",
      "Token [ 4/29]:   0%|          | 2/5000 [00:00<01:12, 68.84it/s, Loss: 2.26e-09 - Gradient norm: 5.65e-03 - String: my name is george]\n",
      "Token [ 5/29]:  10%|█         | 505/5000 [00:04<00:44, 101.14it/s, Loss: 1.44e-08 - Gradient norm: 2.44e-02 - String: my name is george and]   \n",
      "Token [ 6/29]:  20%|█▉        | 978/5000 [00:10<00:41, 97.18it/s, Loss: 0.00e+00 - Gradient norm: 0.00e+00 - String: my name is george and i]         \n",
      "Token [ 7/29]:   0%|          | 4/5000 [00:00<01:03, 78.48it/s, Loss: 0.00e+00 - Gradient norm: 0.00e+00 - String: my name is george and i am]\n",
      "Token [ 8/29]:   0%|          | 12/5000 [00:00<00:56, 88.36it/s, Loss: 0.00e+00 - Gradient norm: 0.00e+00 - String: my name is george and i am living]   \n",
      "Token [ 9/29]: 100%|██████████| 5000/5000 [00:58<00:00, 85.31it/s, Loss: 4.67e+02 - Gradient norm: 3.03e+03 - String: my name is george and i am living more]        \n",
      "Token [10/29]:  39%|███▉      | 1955/5000 [00:23<00:36, 83.32it/s, Loss: 8.97e+02 - Gradient norm: 1.83e+03 - String: my name is george and i am livingx.]           \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEarly stopping\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m grad_oracle, loss \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_last_token_embedding_grad\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# turn into x_i_plus_1\u001b[39;49;00m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mh_target\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mh\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# TODO: Dont use tokens, rather use previous embeddings + x_i_plus_1\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# TODO: non-zero loss even when having the correct y_i_id\u001b[39;00m\n\u001b[1;32m     37\u001b[0m string_so_far \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(input_ids\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mtolist(), skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "embedding_matrix = model.get_input_embeddings().weight  # (vocab_size, hidden_size)\n",
    "\n",
    "prompt = \"my name is george and i am living here in greece, i am 20 years old and my secret is that i am in love\"\n",
    "layer_idx = 8  # layer at which we want to compute the gradient\n",
    "\n",
    "# h = get_hidden_representation(prompt, layer_idx, model, tokenizer)\n",
    "h = get_whole(prompt, model, tokenizer, layer_idx)\n",
    "if h.dim() == 1:\n",
    "    h = h.unsqueeze(0)\n",
    "\n",
    "gamma = 1e-0  # step size for gradient descent\n",
    "n_iterations = 5000  # number of iterations for gradient descent\n",
    "\n",
    "discovered_ids = []\n",
    "for j in range(h.size(0)):\n",
    "    # an idea here is to initialize with feeding an LLM with the prompt so far\n",
    "    # and getting the next token, more expensive, likely to work much better\n",
    "    y_i_id, x_i_plus_1 = get_random_token_id_and_embedding(embedding_matrix)\n",
    "\n",
    "    for iteration in (bar := tqdm(range(n_iterations), desc=f'Token [{j:2d}/{h.size(0):2d}]')):\n",
    "        input_ids = torch.tensor(discovered_ids + [y_i_id], dtype=torch.long)\n",
    "\n",
    "        h_pred = get_whole('', model, tokenizer, layer_idx, input_ids.unsqueeze(0), grad=False)[-1]\n",
    "\n",
    "        if torch.sum((h_pred - h[:h_pred.size(0),:]) ** 2) <= 1e-10:\n",
    "            print('Early stopping')\n",
    "            break\n",
    "\n",
    "        grad_oracle, loss = compute_last_token_embedding_grad(\n",
    "            y=input_ids, # turn into x_i_plus_1\n",
    "            llm=model,\n",
    "            layer_idx=layer_idx,\n",
    "            h_target=h[j]\n",
    "        ) # TODO: Dont use tokens, rather use previous embeddings + x_i_plus_1\n",
    "        # TODO: non-zero loss even when having the correct y_i_id\n",
    "        \n",
    "        string_so_far = tokenizer.decode(input_ids.cpu().tolist(), skip_special_tokens=True)\n",
    "\n",
    "        bar.set_postfix_str(f\"Loss: {loss:.2e} - Gradient norm: {grad_oracle.norm().item():.2e} - String: {string_so_far}\")\n",
    "        # print('', flush=True)\n",
    "        \n",
    "        if string_so_far == prompt:\n",
    "            break\n",
    "\n",
    "        if loss < 1e-6:\n",
    "            # print(f\"Found token {y_i} with small L2 norm: {torch.norm(h[j] - embedding_matrix[closest_token_id]).item():.4f}\")\n",
    "            break\n",
    "\n",
    "        # update the guess using gradient descent\n",
    "        x_i_plus_1 = x_i_plus_1 - gamma * grad_oracle  # (hidden_size,)\n",
    "        \n",
    "        # find the closest token in the embedding space to e_i_plus_1\n",
    "        distances = torch.norm(embedding_matrix - x_i_plus_1, dim=1)  # (vocab_size,)\n",
    "        y_i_id = int(torch.argmin(distances))\n",
    "\n",
    "\n",
    "    \n",
    "    discovered_ids.append(y_i_id)\n",
    "\n",
    "final_string = tokenizer.decode(input_ids.cpu().tolist(), skip_special_tokens=True)\n",
    "print(f\"Final discovered tokens: {final_string}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "100bb853",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token [ 0/12]:   0%|          | 11/5000 [00:00<00:35, 139.53it/s, Loss: 0.0000 | ‖grad‖: 0.0000 | 'Hel']\n",
      "Token [ 1/12]:   1%|▏         | 72/5000 [00:00<00:32, 151.39it/s, Loss: 0.0000 | ‖grad‖: 0.0000 | 'Helen']         \n",
      "Token [ 2/12]:   0%|          | 10/5000 [00:00<00:38, 129.42it/s, Loss: 0.0000 | ‖grad‖: 0.0000 | 'Helenaki']\n",
      "Token [ 3/12]:   1%|          | 39/5000 [00:00<00:33, 147.71it/s, Loss: 0.0000 | ‖grad‖: 0.0000 | 'Helenaki is']       \n",
      "Token [ 4/12]:   0%|          | 14/5000 [00:00<00:36, 136.61it/s, Loss: 0.0000 | ‖grad‖: 0.0000 | 'Helenaki is he']\n",
      "Token [ 5/12]:   0%|          | 5/5000 [00:00<00:40, 124.05it/s, Loss: 0.0000 | ‖grad‖: 0.0000 | 'Helenaki is heree']\n",
      "Token [ 6/12]:   0%|          | 12/5000 [00:00<00:36, 135.82it/s, Loss: 0.0000 | ‖grad‖: 0.0000 | 'Helenaki is hereeee']\n",
      "Token [ 7/12]:   0%|          | 20/5000 [00:00<00:42, 116.88it/s, Loss: 0.0000 | ‖grad‖: 0.0000 | 'Helenaki is hereeee and'] \n",
      "Token [ 8/12]:   1%|▏         | 74/5000 [00:00<00:35, 137.09it/s, Loss: 0.0000 | ‖grad‖: 0.0000 | 'Helenaki is hereeee and is']     \n",
      "Token [ 9/12]:   0%|          | 20/5000 [00:00<00:39, 126.34it/s, Loss: 0.0000 | ‖grad‖: 0.0000 | 'Helenaki is hereeee and is a']      \n",
      "Token [10/12]:   6%|▌         | 309/5000 [00:02<00:35, 131.59it/s, Loss: 0.0000 | ‖grad‖: 0.0000 | 'Helenaki is hereeee and is a cut']     \n",
      "Token [11/12]:   3%|▎         | 159/5000 [00:01<00:37, 129.36it/s, Loss: 0.0000 | ‖grad‖: 0.0000 | 'Helenaki is hereeee and is a cutie']         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final discovered tokens: Helenaki is hereeee and is a cutie\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "embedding_matrix = model.get_input_embeddings().weight  # shape (vocab_size, hidden_size)\n",
    "\n",
    "prompt = \"Helenaki is hereeee and is a cutie\"\n",
    "layer_idx = 4  # layer at which we want to compute the gradient\n",
    "\n",
    "# 1) Get the “target” hidden states for each token in `prompt`\n",
    "h = get_whole(prompt, model, tokenizer, layer_idx, grad=False)\n",
    "if h.dim() == 1:\n",
    "    h = h.unsqueeze(0)  # now shape (seq_len, hidden_size)\n",
    "\n",
    "# 2) Adam hyperparameters\n",
    "gamma = 1e-1        # base learning rate (you can tune this)\n",
    "beta1 = 0.9\n",
    "beta2 = 0.999\n",
    "eps = 1e-8\n",
    "n_iterations = 5000\n",
    "\n",
    "discovered_ids = []\n",
    "\n",
    "for j in range(h.size(0)):\n",
    "    # ——— Initialize a random token and its “fake” embedding guess ———\n",
    "    y_i_id, x_i_plus_1 = get_random_token_id_and_embedding(embedding_matrix)\n",
    "    # x_i_plus_1 has shape (hidden_size,), on same device as embedding_matrix\n",
    "\n",
    "    # 3) Initialize Adam buffers m, v (shape = hidden_size) to zero\n",
    "    m = torch.zeros_like(x_i_plus_1)\n",
    "    v = torch.zeros_like(x_i_plus_1)\n",
    "\n",
    "    for iteration in (bar := tqdm(range(n_iterations), desc=f\"Token [{j:2d}/{h.size(0):2d}]\")):\n",
    "        # Build the current hypothesis of token‐IDs:\n",
    "        input_ids = torch.tensor(discovered_ids + [y_i_id], dtype=torch.long, device=embedding_matrix.device)\n",
    "\n",
    "        # 4) Compute gradient ∇_x L₂( h_last, h_target ) at layer_idx\n",
    "        grad_oracle, loss = compute_last_token_embedding_grad(\n",
    "            y=input_ids,          # tensor of shape (seq_len,)\n",
    "            llm=model,\n",
    "            layer_idx=layer_idx,\n",
    "            h_target=h[j]\n",
    "        )\n",
    "\n",
    "        # 5) Logging / progress\n",
    "        string_so_far = tokenizer.decode(input_ids.cpu().tolist(), skip_special_tokens=True)\n",
    "        bar.set_postfix_str(\n",
    "            f\"Loss: {loss:.4f} | ‖grad‖: {grad_oracle.norm().item():.4f} | '{string_so_far}'\"\n",
    "        )\n",
    "\n",
    "        if string_so_far == prompt:\n",
    "            break\n",
    "        if grad_oracle.norm(p=2) < 1e-4:\n",
    "            # gradient is essentially zero → we’re “close enough”\n",
    "            break\n",
    "\n",
    "        # ——— Adam update on x_i_plus_1 ———\n",
    "        t = iteration + 1\n",
    "        # 6a) Update biased first moment estimate\n",
    "        m = beta1 * m + (1 - beta1) * grad_oracle\n",
    "        # 6b) Update biased second moment estimate (elementwise square)\n",
    "        v = beta2 * v + (1 - beta2) * (grad_oracle * grad_oracle)\n",
    "        # 6c) Compute bias‐corrected m_hat, v_hat\n",
    "        m_hat = m / (1 - beta1**t)\n",
    "        v_hat = v / (1 - beta2**t)\n",
    "        # 6d) Take Adam step\n",
    "        x_i_plus_1 = x_i_plus_1 - gamma * (m_hat / (v_hat.sqrt() + eps))\n",
    "\n",
    "        # 7) Project x_i_plus_1 back to the nearest token in embedding space\n",
    "        #    (Choose the token whose embedding row is closest in L₂ distance)\n",
    "        distances = torch.norm(embedding_matrix - x_i_plus_1.unsqueeze(0), dim=1)  # (vocab_size,)\n",
    "        y_i_id = int(torch.argmin(distances))\n",
    "\n",
    "    # End of iteration loop for this token j\n",
    "    discovered_ids.append(y_i_id)\n",
    "\n",
    "# After all tokens are “discovered”:\n",
    "final_string = tokenizer.decode(torch.tensor(discovered_ids, dtype=torch.long), skip_special_tokens=True)\n",
    "print(f\"Final discovered tokens: {final_string}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "6c515682",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 64])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "h = get_whole(prompt, model, tokenizer, layer_idx, grad=False)\n",
    "\n",
    "h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710630c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"George\"\n",
    "h = get_one_token_h(prompt, 4, model,tokenizer)[0]\n",
    "embedding_matrix = model.get_input_embeddings().weight  # (vocab_size, hidden_size)\n",
    "gamma = 1  # step size for gradient descent\n",
    "layer_idx = 4  # layer at which we want to compute the gradient\n",
    "n_iterations = 100  # number of iterations for gradient descent\n",
    "\n",
    "# make a guess for (e_0,y_0)\n",
    "y_i, x_i_plus_1= get_random_token_and_embedding(tokenizer, embedding_matrix)\n",
    "\n",
    "# loop through the n_iterations\n",
    "for iteration in (bar:=tqdm(range(n_iterations))):\n",
    "    bar.set_postfix_str(f\"Current guess: {y_i}\")\n",
    "    # compute the embedding of the current guess\n",
    "    # P_hat_i = embedding_matrix[tokenizer.encode(y_i)[0]]  # (hidden_size,)\n",
    "    \n",
    "    # compute the gradient of the oracle with respect to e_i -> inner for\n",
    "    grad_oracle, _ = compute_last_token_embedding_grad(\n",
    "        y=torch.tensor(tokenizer.encode(y_i), dtype=torch.long), # turn into x_i_plus_1\n",
    "        llm=model,\n",
    "        layer_idx=layer_idx,\n",
    "        h=h\n",
    "    )  # (hidden_size,)\n",
    "    # bar.set_postfix_str(f\"Current guess: {y_i}, Gradient norm: {grad_oracle.norm().item():.4f}\")\n",
    "\n",
    "    # update the guess using gradient descent\n",
    "    x_i_plus_1 = x_i_plus_1 - gamma * grad_oracle  # (hidden_size,)\n",
    "    \n",
    "    # find the closest token in the embedding space to e_i_plus_1\n",
    "    distances = torch.norm(embedding_matrix - x_i_plus_1, dim=1)  # (vocab_size,)\n",
    "    closest_token_id = torch.argmin(distances).item()\n",
    "    y_i = tokenizer.decode(closest_token_id)\n",
    "\n",
    "    # make a summary via prints\n",
    "    print(f\"Current guess: {y_i} - Target: {prompt} - Gradient norm: {grad_oracle.norm().item():.4f}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9964f9",
   "metadata": {},
   "source": [
    "## Boosted Algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304ebebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = model.get_input_embeddings().weight  # (vocab_size, hidden_size)\n",
    "\n",
    "prompt = \"my name is george and i am living here in greece, i am 20 years old and my secret is that i am in love\"\n",
    "layer_idx = 8  # layer at which we want to compute the gradient\n",
    "\n",
    "# h = get_hidden_representation(prompt, layer_idx, model, tokenizer)\n",
    "h = get_whole(prompt, model, tokenizer, layer_idx)\n",
    "if h.dim() == 1:\n",
    "    h = h.unsqueeze(0)\n",
    "\n",
    "gamma = 1e-0  # step size for gradient descent\n",
    "n_iterations = 5000  # number of iterations for gradient descent\n",
    "\n",
    "discovered_ids = []\n",
    "for j in range(h.size(0)):\n",
    "    # an idea here is to initialize with feeding an LLM with the prompt so far\n",
    "    # and getting the next token, more expensive, likely to work much better\n",
    "    y_i_id, x_i_plus_1 = get_random_token_id_and_embedding(embedding_matrix)\n",
    "\n",
    "    for iteration in (bar := tqdm(range(n_iterations), desc=f'Token [{j:2d}/{h.size(0):2d}]')):\n",
    "        input_ids = torch.tensor(discovered_ids + [y_i_id], dtype=torch.long)\n",
    "\n",
    "        h_pred = get_whole('', model, tokenizer, layer_idx, input_ids.unsqueeze(0), grad=False)[-1]\n",
    "\n",
    "        if torch.sum((h_pred - h[:h_pred.size(0),:]) ** 2) <= 1e-10:\n",
    "            print('Early stopping')\n",
    "            break\n",
    "\n",
    "        grad_oracle, loss = compute_last_token_embedding_grad(\n",
    "            y=input_ids, # turn into x_i_plus_1\n",
    "            llm=model,\n",
    "            layer_idx=layer_idx,\n",
    "            h_target=h[j]\n",
    "        ) # TODO: Dont use tokens, rather use previous embeddings + x_i_plus_1\n",
    "        # TODO: non-zero loss even when having the correct y_i_id\n",
    "        \n",
    "        string_so_far = tokenizer.decode(input_ids.cpu().tolist(), skip_special_tokens=True)\n",
    "\n",
    "        bar.set_postfix_str(f\"Loss: {loss:.2e} - Gradient norm: {grad_oracle.norm().item():.2e} - String: {string_so_far}\")\n",
    "        \n",
    "        if string_so_far == prompt:\n",
    "            break\n",
    "\n",
    "        if loss < 1e-8:\n",
    "            # print(f\"Found token {y_i} with small L2 norm: {torch.norm(h[j] - embedding_matrix[closest_token_id]).item():.4f}\")\n",
    "            break\n",
    "\n",
    "        # update the guess using gradient descent\n",
    "        x_i_plus_1 = x_i_plus_1 - gamma * grad_oracle  # (hidden_size,)\n",
    "        \n",
    "        # find the closest token in the embedding space to e_i_plus_1\n",
    "        distances = torch.norm(embedding_matrix - x_i_plus_1, dim=1)  # (vocab_size,)\n",
    "        y_i_id = int(torch.argmin(distances))\n",
    "\n",
    "\n",
    "    \n",
    "    discovered_ids.append(y_i_id)\n",
    "\n",
    "final_string = tokenizer.decode(input_ids.cpu().tolist(), skip_special_tokens=True)\n",
    "print(f\"Final discovered tokens: {final_string}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ae8f03",
   "metadata": {},
   "source": [
    "## Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "c3851d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token [ 0/ 1]:   0%|          | 4/5000 [00:00<01:04, 78.00it/s, Loss: 0.00e+00 - Gradient norm: 0.00e+00 - String: George]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final discovered tokens: George\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = model.get_input_embeddings().weight  # (vocab_size, hidden_size)\n",
    "\n",
    "prompt = \"my name is george and i am living here in greece, i am 20 years old and my secret is that i am in love\"\n",
    "layer_idx = 8  # layer at which we want to compute the gradient\n",
    "\n",
    "\n",
    "tokenized = tokenizer(prompt)\n",
    "\n",
    "# h = get_hidden_representation(prompt, layer_idx, model, tokenizer)\n",
    "h = get_whole(prompt, model, tokenizer, layer_idx)\n",
    "if h.dim() == 1:\n",
    "    h = h.unsqueeze(0)\n",
    "\n",
    "gamma = 1e-0  # step size for gradient descent\n",
    "n_iterations = 5000  # number of iterations for gradient descent\n",
    "\n",
    "discovered_ids = []\n",
    "for j in range(h.size(0)):\n",
    "    # an idea here is to initialize with feeding an LLM with the prompt so far\n",
    "    # and getting the next token, more expensive, likely to work much better\n",
    "    y_i_id, x_i_plus_1 = get_random_token_id_and_embedding(embedding_matrix)\n",
    "\n",
    "    for iteration in (bar := tqdm(range(n_iterations), desc=f'Token [{j:2d}/{h.size(0):2d}]')):\n",
    "        input_ids = torch.tensor(discovered_ids + [y_i_id], dtype=torch.long)\n",
    "\n",
    "        h_pred = get_whole('', model, tokenizer, layer_idx, input_ids.unsqueeze(0), grad=False)[-1]\n",
    "\n",
    "        # if torch.sum((h_pred - h[:h_pred.size(0),:]) ** 2) <= 1e-10:\n",
    "        #     print('Early stopping')\n",
    "        #     break\n",
    "\n",
    "        grad_oracle, loss = compute_last_token_embedding_grad(\n",
    "            y=input_ids, # turn into x_i_plus_1\n",
    "            llm=model,\n",
    "            layer_idx=layer_idx,\n",
    "            h_target=h[j]\n",
    "        ) # TODO: Dont use tokens, rather use previous embeddings + x_i_plus_1\n",
    "        # TODO: non-zero loss even when having the correct y_i_id\n",
    "        \n",
    "        string_so_far = tokenizer.decode(input_ids.cpu().tolist(), skip_special_tokens=True)\n",
    "\n",
    "        bar.set_postfix_str(f\"Loss: {loss:.2e} - Gradient norm: {grad_oracle.norm().item():.2e} - String: {string_so_far}\")\n",
    "        \n",
    "        # if string_so_far == prompt:\n",
    "        #     break\n",
    "\n",
    "        if loss < 1e-8:\n",
    "            # print(f\"Found token {y_i} with small L2 norm: {torch.norm(h[j] - embedding_matrix[closest_token_id]).item():.4f}\")\n",
    "            break\n",
    "\n",
    "        # update the guess using gradient descent\n",
    "        x_i_plus_1 = x_i_plus_1 - gamma * grad_oracle  # (hidden_size,)\n",
    "        \n",
    "        # find the closest token in the embedding space to e_i_plus_1\n",
    "        distances = torch.norm(embedding_matrix - x_i_plus_1, dim=1)  # (vocab_size,)\n",
    "        y_i_id = int(torch.argmin(distances))\n",
    "\n",
    "\n",
    "    \n",
    "    discovered_ids.append(y_i_id)\n",
    "\n",
    "final_string = tokenizer.decode(input_ids.cpu().tolist(), skip_special_tokens=True)\n",
    "print(f\"Final discovered tokens: {final_string}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "itallm2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
