{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cbde96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# no need to restart kernel after code changes (useful to separate code to modules)\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123aa9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import from huggingface roneneldan/TinyStories-1M\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from nnsight import LanguageModel\n",
    "import torch as t\n",
    "# load garbage collection and empty cache \n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba510893",
   "metadata": {},
   "source": [
    "# UNEMBEDDING BASELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ba736b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean():\n",
    "    gc.collect()\n",
    "    # empty_cache()\n",
    "\n",
    "model_id = \"roneneldan/TinyStories-1M\"\n",
    "# model_id = \"EleutherAI/gpt-j-6b\"\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "load_in_8bit = False\n",
    "\n",
    "try:\n",
    "    del llm\n",
    "    clean()\n",
    "    llm = LanguageModel(model_id, device_map=device, load_in_8bit=load_in_8bit)\n",
    "    tokenizer = llm.tokenizer\n",
    "except:\n",
    "    llm = LanguageModel(model_id, device_map=device, load_in_8bit=load_in_8bit)\n",
    "    tokenizer = llm.tokenizer\n",
    "\n",
    "prompt_trial = \"1:10,2:20,3:\"\n",
    "prompt_trial = \"my name is \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833266d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def embeddings_to_texts_baseline(embeddings, model, tokenizer, skip_special_tokens=True):\n",
    "    \"\"\"\n",
    "    Map input embeddings (batch, seq_len, emb_dim) → list of decoded strings.\n",
    "    \"\"\"\n",
    "    # 1) Project embeddings to vocab logits\n",
    "    logits = model.lm_head(embeddings.to(device))           # (batch, seq_len, vocab_size)\n",
    "    # 2) Greedy decode: pick highest logit per position\n",
    "    token_ids = torch.argmax(logits, dim=-1)     # (batch, seq_len)\n",
    "    # 3) Transform each sequence of IDs into text\n",
    "    texts = tokenizer.batch_decode(token_ids, skip_special_tokens=skip_special_tokens)\n",
    "    return logits, texts\n",
    "\n",
    "def get_next_token_prediction(embeddings, model, tokenizer, skip_special_tokens=True):\n",
    "    # 1) Project embeddings to vocab logits\n",
    "    logits = model.lm_head(embeddings)           # (batch, seq_len, vocab_size)\n",
    "    # 2) Greedy decode: pick highest logit per position\n",
    "    token_ids = torch.argmax(logits, dim=-1)[:,-1]     # (batch, seq_len)\n",
    "    # 3) Transform each sequence of IDs into text\n",
    "    texts = tokenizer.batch_decode(token_ids, skip_special_tokens=skip_special_tokens)\n",
    "    return texts\n",
    "\n",
    "@t.inference_mode()\n",
    "def get_residual_output(prompt, layer_idx, llm, normalize = False):\n",
    "\n",
    "    assert hasattr(llm, 'transformer'), \"The model does not have a transformer attribute.\"\n",
    "    assert hasattr(llm.transformer, 'h'), \"The transformer does not have a 'h' attribute for layers.\"\n",
    "\n",
    "\n",
    "    with llm.trace(prompt):\n",
    "        residual_output = llm.transformer.h[layer_idx].output[0].save()  # Save the output of the layer for inspection\n",
    "\n",
    "    if normalize: # FIXME: check if this is the correct way to normalize\n",
    "        residual_output = llm.transformer.ln_f(residual_output)\n",
    "    \n",
    "    if llm.device.type == \"cuda\":\n",
    "        residual_output = residual_output.detach().cpu()\n",
    "        clean()\n",
    "    \n",
    "    return residual_output\n",
    "\n",
    "@t.inference_mode()\n",
    "def get_embeddings(prompt,llm):\n",
    "\n",
    "    assert hasattr(llm, 'transformer'), \"The model does not have a transformer attribute.\"\n",
    "    assert hasattr(llm.transformer, 'drop'), \"The transformer does not have a 'drop' attribute for input embeddings.\"\n",
    "    \n",
    "    with llm.trace(prompt):\n",
    "        input_embeddings = llm.transformer.drop.input.save()\n",
    "\n",
    "    if llm.device.type == \"cuda\":\n",
    "        input_embeddings = input_embeddings.detach().to(\"cpu\")\n",
    "        clean()\n",
    "    return input_embeddings\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50818d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "@t.inference_mode()\n",
    "def get_one_token_h(prompt, layer_idx, model, tokenizer):\n",
    "    llm = LanguageModel(model, tokenizer=tokenizer) if not isinstance(model, LanguageModel) else model\n",
    "\n",
    "    # check that the prompt is a single token\n",
    "    assert len(tokenizer(prompt)[\"input_ids\"]) == 1, \"The prompt must be a single token.\"\n",
    "    # get the hidden representation of the prompt at layer_idx\n",
    "    with llm.trace(prompt):\n",
    "        hidden = llm.transformer.h[layer_idx].output.save()\n",
    "    if llm.device.type == \"cuda\":\n",
    "        hidden = hidden.detach().cpu()\n",
    "        clean()\n",
    "    return hidden\n",
    "\n",
    "@t.inference_mode()\n",
    "def get_hidden_representation(prompt, layer_idx, model, tokenizer):\n",
    "    llm = LanguageModel(model, tokenizer=tokenizer) if not isinstance(model, LanguageModel) else model\n",
    "    with llm.trace(prompt):\n",
    "        hidden = llm.transformer.h[layer_idx].output.save()\n",
    "    if llm.device.type == \"cuda\":\n",
    "        hidden = hidden.detach().cpu()\n",
    "        clean()\n",
    "    return hidden[0].squeeze()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c51e30e",
   "metadata": {},
   "source": [
    "### TRYING THE DECODING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afd5ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a for loop where for each layer you try the embeddings_to_texts_baseline for each hidden state output\n",
    "full_text = False\n",
    "print(f\"{prompt_trial=} \\n\\n\")\n",
    "if full_text:   \n",
    "    reversed_embeddings = get_embeddings(prompt_trial, llm)\n",
    "    logits, texts = embeddings_to_texts_baseline(reversed_embeddings, llm, tokenizer)\n",
    "    print(f\"Reversed Embeddings for the prompt: {texts= } \\n\\n\")\n",
    "else:\n",
    "    next_token_prediction = get_next_token_prediction(get_embeddings(prompt_trial, llm), llm, tokenizer)\n",
    "    print(f\"Next token prediction for the prompt: {next_token_prediction= } \\n\\n\")\n",
    "\n",
    "\n",
    "print(f\"Iterating through each layer's output for the prompt: {prompt_trial}\\n\")\n",
    "for layer_idx in range(len(llm.transformer.h)):\n",
    "    residual_output = get_residual_output(prompt_trial, layer_idx, llm, True)\n",
    "    if full_text:\n",
    "        logits, texts = embeddings_to_texts_baseline(residual_output, llm, tokenizer)\n",
    "        print(f\"Layer {layer_idx} output texts: {texts}\")\n",
    "    else:\n",
    "        next_token_prediction = get_next_token_prediction(residual_output, llm, tokenizer)\n",
    "        print(f\"Layer {layer_idx} next token prediction: {next_token_prediction}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734142f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def get_whole1(\n",
    "    prompt: str,\n",
    "    llm: torch.nn.Module,\n",
    "    tokenizer,\n",
    "    layer_idx: int\n",
    ") -> torch.Tensor:\n",
    "    # 1) Tokenize (and move to same device as the model).\n",
    "    device = next(llm.parameters()).device\n",
    "    encoded = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input_ids = encoded[\"input_ids\"].to(device)      # shape (1, seq_len)\n",
    "    attention_mask = encoded.get(\"attention_mask\", None)\n",
    "    if attention_mask is not None:\n",
    "        attention_mask = attention_mask.to(device)\n",
    "\n",
    "    # 2) Forward pass under no_grad, requesting hidden_states\n",
    "    with torch.no_grad():\n",
    "        outputs = llm(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            output_hidden_states=True,\n",
    "        )\n",
    "        hidden_states = outputs.hidden_states\n",
    "        # hidden_states is a tuple: (layer0, layer1, ..., layerN),\n",
    "        # where layer0 = embeddings output, layer1 = first transformer block, etc.\n",
    "\n",
    "        # 3) Index into [0, -1, :] to get the last token for this single‐batch example\n",
    "        #    hidden_states[layer_idx] has shape (batch_size=1, seq_len, hidden_size).\n",
    "        return hidden_states[layer_idx][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f436e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a for loop where for each layer you try the embeddings_to_texts_baseline for each hidden state output\n",
    "prompt = 'my name is george'\n",
    "prefix = 'my name'\n",
    "\n",
    "for layer_idx in range(len(llm.transformer.h)):\n",
    "    # residual_output1 = get_residual_output(prompt, layer_idx, llm, True)\n",
    "    # residual_output2 = get_residual_output(prefix, layer_idx, llm, True)\n",
    "\n",
    "    # residual_output1 = get_hidden_representation(prompt, layer_idx, llm, tokenizer)\n",
    "    # residual_output2 = get_hidden_representation(prefix, layer_idx, llm, tokenizer)\n",
    "\n",
    "    residual_output1 = get_whole1(prompt, llm, tokenizer, layer_idx)\n",
    "    residual_output2 = get_whole1(prefix, llm, tokenizer, layer_idx)\n",
    "\n",
    "    print(residual_output1.shape)\n",
    "    print(residual_output2.shape)\n",
    "    print(torch.mean((residual_output1[:2,:] - residual_output2) ** 2))\n",
    "    print()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096d9fc7",
   "metadata": {},
   "source": [
    "# GRADIENT BASED ALGORITHM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65975f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import compute_last_token_embedding_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32764b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that takes as input a tokenizer and an embedding matrix and returns a random token and its embedding\n",
    "def get_random_token_and_embedding(tokenizer, embedding_matrix):\n",
    "    # Get the vocabulary size\n",
    "    vocab_size = embedding_matrix.shape[0]\n",
    "    \n",
    "    # Generate a random token ID\n",
    "    random_token_id = torch.randint(0, vocab_size, (1,)).item()\n",
    "    \n",
    "    # Get the corresponding embedding\n",
    "    embedding = embedding_matrix[random_token_id]\n",
    "    \n",
    "    # Decode the token ID to get the token string\n",
    "    token_string = tokenizer.decode(random_token_id)\n",
    "    \n",
    "    return token_string, embedding\n",
    "\n",
    "# function that takes as input a tokenizer and an embedding matrix and returns a random token and its embedding\n",
    "def get_random_token_id_and_embedding(embedding_matrix):\n",
    "    # Get the vocabulary size\n",
    "    vocab_size = embedding_matrix.shape[0]\n",
    "    \n",
    "    # Generate a random token ID\n",
    "    random_token_id = torch.randint(0, vocab_size, (1,)).item()\n",
    "    \n",
    "    # Get the corresponding embedding\n",
    "    embedding = embedding_matrix[random_token_id]\n",
    "\n",
    "    \n",
    "    return random_token_id, embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8772337f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "# take as input llm, tokenizer, embedding matrix, an hidden representation, layer_idx, n_iterations, step_size, \n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roneneldan/TinyStories-1M\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"roneneldan/TinyStories-1M\", output_hidden_states=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c772133",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"George\"\n",
    "h = get_one_token_h(prompt, 4, model,tokenizer)\n",
    "embedding_matrix = model.get_input_embeddings().weight  # (vocab_size, hidden_size)\n",
    "gamma = 1  # step size for gradient descent\n",
    "layer_idx = 4  # layer at which we want to compute the gradient\n",
    "n_iterations = 100  # number of iterations for gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efef9e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"George\"\n",
    "h = get_one_token_h(prompt, 4, model,tokenizer)\n",
    "embedding_matrix = model.get_input_embeddings().weight  # (vocab_size, hidden_size)\n",
    "gamma = 1  # step size for gradient descent\n",
    "layer_idx = 4  # layer at which we want to compute the gradient\n",
    "n_iterations = 100  # number of iterations for gradient descent\n",
    "\n",
    "# make a guess for (e_0,y_0)\n",
    "y_i, x_i_plus_1= get_random_token_and_embedding(tokenizer, embedding_matrix)\n",
    "\n",
    "# loop through the n_iterations\n",
    "for iteration in (bar:=tqdm(range(n_iterations))):\n",
    "    bar.set_postfix_str(f\"Current guess: {y_i}\")\n",
    "    # compute the embedding of the current guess\n",
    "    # P_hat_i = embedding_matrix[tokenizer.encode(y_i)[0]]  # (hidden_size,)\n",
    "    \n",
    "    # compute the gradient of the oracle with respect to e_i -> inner for\n",
    "    grad_oracle = compute_last_token_embedding_grad(\n",
    "        y=torch.tensor(tokenizer.encode(y_i), dtype=torch.long), # turn into x_i_plus_1\n",
    "        llm=model,\n",
    "        layer_idx=layer_idx,\n",
    "        h=h\n",
    "    )  # (hidden_size,)\n",
    "    bar.set_postfix_str(f\"Current guess: {y_i}, Gradient norm: {grad_oracle.norm().item():.4f}\")\n",
    "\n",
    "    # update the guess using gradient descent\n",
    "    x_i_plus_1 = x_i_plus_1 - gamma * grad_oracle  # (hidden_size,)\n",
    "    \n",
    "    # find the closest token in the embedding space to e_i_plus_1\n",
    "    distances = torch.norm(embedding_matrix - x_i_plus_1, dim=1)  # (vocab_size,)\n",
    "    closest_token_id = torch.argmin(distances).item()\n",
    "    y_i = tokenizer.decode(closest_token_id)\n",
    "\n",
    "    # make a summary via prints\n",
    "    print(f\"Iteration {iteration + 1}/{n_iterations}:\")\n",
    "    print(f\"  Current guess: {y_i}\")\n",
    "    print(f\"  Gradient norm: {grad_oracle.norm().item():.4f}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768caba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"George\"\n",
    "n = 100\n",
    "# tokenizer it but return text\n",
    "h = get_hidden_representation(prompt, layer_idx, model, tokenizer)\n",
    "size = 1 if len(h.shape)==1 else h.shape[0]\n",
    "\n",
    "# discovered_tokens = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d275d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.tokenize(\"my name is George\")\n",
    "# tokenizer.encode(\"name\")\n",
    "# discovered_tokens = [\"my\", \" name\", \" is\"]\n",
    "# # merge discovered tokens and y_i in a list and a string\n",
    "# lista = discovered_tokens + [y_i]\n",
    "# stringa = \"\".join(lista)\n",
    "# y_i, discovered_tokens, lista, stringa, tokenizer.tokenize(stringa)\n",
    "# y_extend = \"\".join((discovered_tokens + [y_i]))\n",
    "# print(f\"Extended guess: {y_extend}\")\n",
    "# encoded_y = torch.tensor(tokenizer.encode(y_extend), dtype=torch.long) \n",
    "# grad_oracle = compute_last_token_embedding_grad(\n",
    "#         y=encoded_y, # turn into x_i_plus_1\n",
    "#         llm=model,\n",
    "#         layer_idx=layer_idx,\n",
    "#         h=h\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a682d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "from typing import Optional\n",
    "\n",
    "def get_whole(\n",
    "    prompt: str,\n",
    "    llm: torch.nn.Module,\n",
    "    tokenizer,\n",
    "    layer_idx: int,\n",
    "    input_ids: Optional[torch.Tensor] = None,\n",
    "    grad: bool = False\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Tokenize `prompt`, do a forward pass with output_hidden_states=True,\n",
    "    and return the hidden vector of the *last* token at layer `layer_idx`.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): the input string, e.g. \"Harry\".\n",
    "        llm (nn.Module): a HuggingFace‐style model (with embeddings + hidden_states).\n",
    "        tokenizer: the corresponding tokenizer for `llm`.\n",
    "        layer_idx (int): which hidden‐layer index to extract (0=embeddings, 1=first block, etc.)\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape (hidden_size,) = the last‐token hidden state at `layer_idx`,\n",
    "        computed under torch.no_grad().\n",
    "    \"\"\"\n",
    "    # 1) Tokenize (and move to same device as the model).\n",
    "    device = next(llm.parameters()).device\n",
    "    if input_ids is None:\n",
    "        encoded = tokenizer(prompt, return_tensors=\"pt\")\n",
    "        input_ids = encoded[\"input_ids\"].to(device)      # shape (1, seq_len)\n",
    "    \n",
    "    if not grad:\n",
    "        # Forward under no_grad and detach before returning\n",
    "        with torch.no_grad():\n",
    "            outputs = llm(\n",
    "                input_ids=input_ids,\n",
    "                output_hidden_states=True\n",
    "            )\n",
    "            hidden_states = outputs.hidden_states\n",
    "            h = hidden_states[layer_idx][0]  # shape = (seq_len, hidden_size)\n",
    "\n",
    "        return h.detach()\n",
    "    else:\n",
    "        # Forward normally, so gradients can flow\n",
    "        outputs = llm(\n",
    "            input_ids=input_ids,\n",
    "            output_hidden_states=True\n",
    "        )\n",
    "        hidden_states = outputs.hidden_states\n",
    "        h = hidden_states[layer_idx][0]  # shape = (seq_len, hidden_size)\n",
    "        return h\n",
    "\n",
    "\n",
    "def compute_last_token_embedding_grad(\n",
    "    y: torch.LongTensor,\n",
    "    llm: torch.nn.Module,\n",
    "    layer_idx: int,\n",
    "    h_target: torch.Tensor\n",
    "):\n",
    "    device = next(llm.parameters()).device\n",
    "    y = y.to(device)\n",
    "    h_target = h_target.to(device)\n",
    "\n",
    "    emb_layer = llm.get_input_embeddings()\n",
    "    if not emb_layer.weight.requires_grad:\n",
    "        emb_layer.weight.requires_grad_(True)\n",
    "\n",
    "    llm.zero_grad()\n",
    "    emb_layer.zero_grad()\n",
    "\n",
    "    with torch.set_grad_enabled(True):\n",
    "        h_last = get_whole('', llm, tokenizer, layer_idx, y.unsqueeze(0), grad=True)[-1]\n",
    "        diff = h_last - h_target\n",
    "        loss = torch.dot(diff, diff)\n",
    "        loss.backward()\n",
    "\n",
    "    last_token_id = y[-1].item()\n",
    "    grad_last_embedding = emb_layer.weight.grad[last_token_id].detach().clone() # TODO: Is this gradient with respect to the input or not?\n",
    "\n",
    "    llm.zero_grad()\n",
    "    emb_layer.zero_grad()\n",
    "\n",
    "    return grad_last_embedding, loss.item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c380f235",
   "metadata": {},
   "source": [
    "## Vanilla Algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3e8c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = model.get_input_embeddings().weight  # (vocab_size, hidden_size)\n",
    "\n",
    "prompt = \"my name is george and i am living here in greece, i am 20 years old and my secret is that i am in love\"\n",
    "layer_idx = 8  # layer at which we want to compute the gradient\n",
    "\n",
    "# h = get_hidden_representation(prompt, layer_idx, model, tokenizer)\n",
    "h = get_whole(prompt, model, tokenizer, layer_idx)\n",
    "if h.dim() == 1:\n",
    "    h = h.unsqueeze(0)\n",
    "\n",
    "gamma = 1e-0  # step size for gradient descent\n",
    "n_iterations = 5000  # number of iterations for gradient descent\n",
    "\n",
    "discovered_ids = []\n",
    "for j in range(h.size(0)):\n",
    "    # an idea here is to initialize with feeding an LLM with the prompt so far\n",
    "    # and getting the next token, more expensive, likely to work much better\n",
    "    y_i_id, x_i_plus_1 = get_random_token_id_and_embedding(embedding_matrix)\n",
    "\n",
    "    for iteration in (bar := tqdm(range(n_iterations), desc=f'Token [{j:2d}/{h.size(0):2d}]')):\n",
    "        input_ids = torch.tensor(discovered_ids + [y_i_id], dtype=torch.long)\n",
    "\n",
    "        h_pred = get_whole('', model, tokenizer, layer_idx, input_ids.unsqueeze(0), grad=False)[-1]\n",
    "\n",
    "        if torch.sum((h_pred - h[:h_pred.size(0),:]) ** 2) <= 1e-10:\n",
    "            print('Early stopping')\n",
    "            break\n",
    "\n",
    "        grad_oracle, loss = compute_last_token_embedding_grad(\n",
    "            y=input_ids, # turn into x_i_plus_1\n",
    "            llm=model,\n",
    "            layer_idx=layer_idx,\n",
    "            h_target=h[j]\n",
    "        ) # TODO: Dont use tokens, rather use previous embeddings + x_i_plus_1\n",
    "        # TODO: non-zero loss even when having the correct y_i_id\n",
    "        \n",
    "        string_so_far = tokenizer.decode(input_ids.cpu().tolist(), skip_special_tokens=True)\n",
    "\n",
    "        bar.set_postfix_str(f\"Loss: {loss:.2e} - Gradient norm: {grad_oracle.norm().item():.2e} - String: {string_so_far}\")\n",
    "        # print('', flush=True)\n",
    "        \n",
    "        if string_so_far == prompt:\n",
    "            break\n",
    "\n",
    "        if loss < 1e-6:\n",
    "            # print(f\"Found token {y_i} with small L2 norm: {torch.norm(h[j] - embedding_matrix[closest_token_id]).item():.4f}\")\n",
    "            break\n",
    "\n",
    "        # update the guess using gradient descent\n",
    "        x_i_plus_1 = x_i_plus_1 - gamma * grad_oracle  # (hidden_size,)\n",
    "        \n",
    "        # find the closest token in the embedding space to e_i_plus_1\n",
    "        distances = torch.norm(embedding_matrix - x_i_plus_1, dim=1)  # (vocab_size,)\n",
    "        y_i_id = int(torch.argmin(distances))\n",
    "\n",
    "\n",
    "    \n",
    "    discovered_ids.append(y_i_id)\n",
    "\n",
    "final_string = tokenizer.decode(input_ids.cpu().tolist(), skip_special_tokens=True)\n",
    "print(f\"Final discovered tokens: {final_string}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100bb853",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "embedding_matrix = model.get_input_embeddings().weight  # shape (vocab_size, hidden_size)\n",
    "\n",
    "prompt = \"Helenaki is hereeee and is a cutie\"\n",
    "layer_idx = 4  # layer at which we want to compute the gradient\n",
    "\n",
    "# 1) Get the “target” hidden states for each token in `prompt`\n",
    "h = get_whole(prompt, model, tokenizer, layer_idx, grad=False)\n",
    "if h.dim() == 1:\n",
    "    h = h.unsqueeze(0)  # now shape (seq_len, hidden_size)\n",
    "\n",
    "# 2) Adam hyperparameters\n",
    "gamma = 1e-1        # base learning rate (you can tune this)\n",
    "beta1 = 0.9\n",
    "beta2 = 0.999\n",
    "eps = 1e-8\n",
    "n_iterations = 5000\n",
    "\n",
    "discovered_ids = []\n",
    "\n",
    "for j in range(h.size(0)):\n",
    "    # ——— Initialize a random token and its “fake” embedding guess ———\n",
    "    y_i_id, x_i_plus_1 = get_random_token_id_and_embedding(embedding_matrix)\n",
    "    # x_i_plus_1 has shape (hidden_size,), on same device as embedding_matrix\n",
    "\n",
    "    # 3) Initialize Adam buffers m, v (shape = hidden_size) to zero\n",
    "    m = torch.zeros_like(x_i_plus_1)\n",
    "    v = torch.zeros_like(x_i_plus_1)\n",
    "\n",
    "    for iteration in (bar := tqdm(range(n_iterations), desc=f\"Token [{j:2d}/{h.size(0):2d}]\")):\n",
    "        # Build the current hypothesis of token‐IDs:\n",
    "        input_ids = torch.tensor(discovered_ids + [y_i_id], dtype=torch.long, device=embedding_matrix.device)\n",
    "\n",
    "        # 4) Compute gradient ∇_x L₂( h_last, h_target ) at layer_idx\n",
    "        grad_oracle, loss = compute_last_token_embedding_grad(\n",
    "            y=input_ids,          # tensor of shape (seq_len,)\n",
    "            llm=model,\n",
    "            layer_idx=layer_idx,\n",
    "            h_target=h[j]\n",
    "        )\n",
    "\n",
    "        # 5) Logging / progress\n",
    "        string_so_far = tokenizer.decode(input_ids.cpu().tolist(), skip_special_tokens=True)\n",
    "        bar.set_postfix_str(\n",
    "            f\"Loss: {loss:.4f} | ‖grad‖: {grad_oracle.norm().item():.4f} | '{string_so_far}'\"\n",
    "        )\n",
    "\n",
    "        if string_so_far == prompt:\n",
    "            break\n",
    "        if grad_oracle.norm(p=2) < 1e-4:\n",
    "            # gradient is essentially zero → we’re “close enough”\n",
    "            break\n",
    "\n",
    "        # ——— Adam update on x_i_plus_1 ———\n",
    "        t = iteration + 1\n",
    "        # 6a) Update biased first moment estimate\n",
    "        m = beta1 * m + (1 - beta1) * grad_oracle\n",
    "        # 6b) Update biased second moment estimate (elementwise square)\n",
    "        v = beta2 * v + (1 - beta2) * (grad_oracle * grad_oracle)\n",
    "        # 6c) Compute bias‐corrected m_hat, v_hat\n",
    "        m_hat = m / (1 - beta1**t)\n",
    "        v_hat = v / (1 - beta2**t)\n",
    "        # 6d) Take Adam step\n",
    "        x_i_plus_1 = x_i_plus_1 - gamma * (m_hat / (v_hat.sqrt() + eps))\n",
    "\n",
    "        # 7) Project x_i_plus_1 back to the nearest token in embedding space\n",
    "        #    (Choose the token whose embedding row is closest in L₂ distance)\n",
    "        distances = torch.norm(embedding_matrix - x_i_plus_1.unsqueeze(0), dim=1)  # (vocab_size,)\n",
    "        y_i_id = int(torch.argmin(distances))\n",
    "\n",
    "    # End of iteration loop for this token j\n",
    "    discovered_ids.append(y_i_id)\n",
    "\n",
    "# After all tokens are “discovered”:\n",
    "final_string = tokenizer.decode(torch.tensor(discovered_ids, dtype=torch.long), skip_special_tokens=True)\n",
    "print(f\"Final discovered tokens: {final_string}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c515682",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "h = get_whole(prompt, model, tokenizer, layer_idx, grad=False)\n",
    "\n",
    "h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710630c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"George\"\n",
    "h = get_one_token_h(prompt, 4, model,tokenizer)[0]\n",
    "embedding_matrix = model.get_input_embeddings().weight  # (vocab_size, hidden_size)\n",
    "gamma = 1  # step size for gradient descent\n",
    "layer_idx = 4  # layer at which we want to compute the gradient\n",
    "n_iterations = 100  # number of iterations for gradient descent\n",
    "\n",
    "# make a guess for (e_0,y_0)\n",
    "y_i, x_i_plus_1= get_random_token_and_embedding(tokenizer, embedding_matrix)\n",
    "\n",
    "# loop through the n_iterations\n",
    "for iteration in (bar:=tqdm(range(n_iterations))):\n",
    "    bar.set_postfix_str(f\"Current guess: {y_i}\")\n",
    "    # compute the embedding of the current guess\n",
    "    # P_hat_i = embedding_matrix[tokenizer.encode(y_i)[0]]  # (hidden_size,)\n",
    "    \n",
    "    # compute the gradient of the oracle with respect to e_i -> inner for\n",
    "    grad_oracle, _ = compute_last_token_embedding_grad(\n",
    "        y=torch.tensor(tokenizer.encode(y_i), dtype=torch.long), # turn into x_i_plus_1\n",
    "        llm=model,\n",
    "        layer_idx=layer_idx,\n",
    "        h=h\n",
    "    )  # (hidden_size,)\n",
    "    # bar.set_postfix_str(f\"Current guess: {y_i}, Gradient norm: {grad_oracle.norm().item():.4f}\")\n",
    "\n",
    "    # update the guess using gradient descent\n",
    "    x_i_plus_1 = x_i_plus_1 - gamma * grad_oracle  # (hidden_size,)\n",
    "    \n",
    "    # find the closest token in the embedding space to e_i_plus_1\n",
    "    distances = torch.norm(embedding_matrix - x_i_plus_1, dim=1)  # (vocab_size,)\n",
    "    closest_token_id = torch.argmin(distances).item()\n",
    "    y_i = tokenizer.decode(closest_token_id)\n",
    "\n",
    "    # make a summary via prints\n",
    "    print(f\"Current guess: {y_i} - Target: {prompt} - Gradient norm: {grad_oracle.norm().item():.4f}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9964f9",
   "metadata": {},
   "source": [
    "## Boosted Algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304ebebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = model.get_input_embeddings().weight  # (vocab_size, hidden_size)\n",
    "\n",
    "prompt = \"my name is george and i am living here in greece, i am 20 years old and my secret is that i am in love\"\n",
    "layer_idx = 8  # layer at which we want to compute the gradient\n",
    "\n",
    "# h = get_hidden_representation(prompt, layer_idx, model, tokenizer)\n",
    "h = get_whole(prompt, model, tokenizer, layer_idx)\n",
    "if h.dim() == 1:\n",
    "    h = h.unsqueeze(0)\n",
    "\n",
    "gamma = 1e-0  # step size for gradient descent\n",
    "n_iterations = 5000  # number of iterations for gradient descent\n",
    "\n",
    "discovered_ids = []\n",
    "for j in range(h.size(0)):\n",
    "    # an idea here is to initialize with feeding an LLM with the prompt so far\n",
    "    # and getting the next token, more expensive, likely to work much better\n",
    "    y_i_id, x_i_plus_1 = get_random_token_id_and_embedding(embedding_matrix)\n",
    "\n",
    "    for iteration in (bar := tqdm(range(n_iterations), desc=f'Token [{j:2d}/{h.size(0):2d}]')):\n",
    "        input_ids = torch.tensor(discovered_ids + [y_i_id], dtype=torch.long)\n",
    "\n",
    "        h_pred = get_whole('', model, tokenizer, layer_idx, input_ids.unsqueeze(0), grad=False)[-1]\n",
    "\n",
    "        if torch.sum((h_pred - h[:h_pred.size(0),:]) ** 2) <= 1e-10:\n",
    "            print('Early stopping')\n",
    "            break\n",
    "\n",
    "        grad_oracle, loss = compute_last_token_embedding_grad(\n",
    "            y=input_ids, # turn into x_i_plus_1\n",
    "            llm=model,\n",
    "            layer_idx=layer_idx,\n",
    "            h_target=h[j]\n",
    "        ) # TODO: Dont use tokens, rather use previous embeddings + x_i_plus_1\n",
    "        # TODO: non-zero loss even when having the correct y_i_id\n",
    "        \n",
    "        string_so_far = tokenizer.decode(input_ids.cpu().tolist(), skip_special_tokens=True)\n",
    "\n",
    "        bar.set_postfix_str(f\"Loss: {loss:.2e} - Gradient norm: {grad_oracle.norm().item():.2e} - String: {string_so_far}\")\n",
    "        \n",
    "        if string_so_far == prompt:\n",
    "            break\n",
    "\n",
    "        if loss < 1e-8:\n",
    "            # print(f\"Found token {y_i} with small L2 norm: {torch.norm(h[j] - embedding_matrix[closest_token_id]).item():.4f}\")\n",
    "            break\n",
    "\n",
    "        # update the guess using gradient descent\n",
    "        x_i_plus_1 = x_i_plus_1 - gamma * grad_oracle  # (hidden_size,)\n",
    "        \n",
    "        # find the closest token in the embedding space to e_i_plus_1\n",
    "        distances = torch.norm(embedding_matrix - x_i_plus_1, dim=1)  # (vocab_size,)\n",
    "        y_i_id = int(torch.argmin(distances))\n",
    "\n",
    "\n",
    "    \n",
    "    discovered_ids.append(y_i_id)\n",
    "\n",
    "final_string = tokenizer.decode(input_ids.cpu().tolist(), skip_special_tokens=True)\n",
    "print(f\"Final discovered tokens: {final_string}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ae8f03",
   "metadata": {},
   "source": [
    "## Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3851d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = model.get_input_embeddings().weight  # (vocab_size, hidden_size)\n",
    "\n",
    "prompt = \"my name is george and i am living here in greece, i am 20 years old and my secret is that i am in love\"\n",
    "layer_idx = 8  # layer at which we want to compute the gradient\n",
    "\n",
    "\n",
    "tokenized = tokenizer(prompt)\n",
    "\n",
    "# h = get_hidden_representation(prompt, layer_idx, model, tokenizer)\n",
    "h = get_whole(prompt, model, tokenizer, layer_idx)\n",
    "if h.dim() == 1:\n",
    "    h = h.unsqueeze(0)\n",
    "\n",
    "gamma = 1e-0  # step size for gradient descent\n",
    "n_iterations = 5000  # number of iterations for gradient descent\n",
    "\n",
    "discovered_ids = []\n",
    "for j in range(h.size(0)):\n",
    "    # an idea here is to initialize with feeding an LLM with the prompt so far\n",
    "    # and getting the next token, more expensive, likely to work much better\n",
    "    y_i_id, x_i_plus_1 = get_random_token_id_and_embedding(embedding_matrix)\n",
    "\n",
    "    for iteration in (bar := tqdm(range(n_iterations), desc=f'Token [{j:2d}/{h.size(0):2d}]')):\n",
    "        input_ids = torch.tensor(discovered_ids + [y_i_id], dtype=torch.long)\n",
    "\n",
    "        h_pred = get_whole('', model, tokenizer, layer_idx, input_ids.unsqueeze(0), grad=False)[-1]\n",
    "\n",
    "        # if torch.sum((h_pred - h[:h_pred.size(0),:]) ** 2) <= 1e-10:\n",
    "        #     print('Early stopping')\n",
    "        #     break\n",
    "\n",
    "        grad_oracle, loss = compute_last_token_embedding_grad(\n",
    "            y=input_ids, # turn into x_i_plus_1\n",
    "            llm=model,\n",
    "            layer_idx=layer_idx,\n",
    "            h_target=h[j]\n",
    "        ) # TODO: Dont use tokens, rather use previous embeddings + x_i_plus_1\n",
    "        # TODO: non-zero loss even when having the correct y_i_id\n",
    "        \n",
    "        string_so_far = tokenizer.decode(input_ids.cpu().tolist(), skip_special_tokens=True)\n",
    "\n",
    "        bar.set_postfix_str(f\"Loss: {loss:.2e} - Gradient norm: {grad_oracle.norm().item():.2e} - String: {string_so_far}\")\n",
    "        \n",
    "        # if string_so_far == prompt:\n",
    "        #     break\n",
    "\n",
    "        if loss < 1e-8:\n",
    "            # print(f\"Found token {y_i} with small L2 norm: {torch.norm(h[j] - embedding_matrix[closest_token_id]).item():.4f}\")\n",
    "            break\n",
    "\n",
    "        # update the guess using gradient descent\n",
    "        x_i_plus_1 = x_i_plus_1 - gamma * grad_oracle  # (hidden_size,)\n",
    "        \n",
    "        # find the closest token in the embedding space to e_i_plus_1\n",
    "        distances = torch.norm(embedding_matrix - x_i_plus_1, dim=1)  # (vocab_size,)\n",
    "        y_i_id = int(torch.argmin(distances))\n",
    "\n",
    "\n",
    "    \n",
    "    discovered_ids.append(y_i_id)\n",
    "\n",
    "final_string = tokenizer.decode(input_ids.cpu().tolist(), skip_special_tokens=True)\n",
    "print(f\"Final discovered tokens: {final_string}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "itallm2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
