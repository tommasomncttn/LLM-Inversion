{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cbde96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# no need to restart kernel after code changes (useful to separate code to modules)\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35495905",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123aa9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "from utils.general import compute_last_token_embedding_grad_emb, get_whole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97a57e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int = 8):\n",
    "    \"\"\"Set seed for reproducibility.\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)  # if using multi-GPU\n",
    "\n",
    "    # Ensure deterministic behavior in cuDNN (may impact performance)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2200cf7a",
   "metadata": {},
   "source": [
    "## Torch Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175d2c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "model_id = \"roneneldan/TinyStories-1M\"\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device = \"cpu\"\n",
    "load_in_8bit = False\n",
    "\n",
    "try:\n",
    "    del model\n",
    "    clean()\n",
    "except NameError:\n",
    "    pass \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id,\n",
    "                                               torch_dtype=torch.float16,\n",
    "                                               device_map=device,\n",
    "                                               load_in_8bit=load_in_8bit,\n",
    "                                               trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c739dc55",
   "metadata": {},
   "source": [
    "### Gradient based on projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9468faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_token(\n",
    "    token_idx,\n",
    "    embedding_matrix,\n",
    "    discovered_embeddings, discovered_ids,\n",
    "    llm, layer_idx, h_target,\n",
    "    optimizer_cls, lr\n",
    "    \n",
    "):\n",
    "    copy_embedding_matrix = embedding_matrix.clone().detach().requires_grad_(False)\n",
    "\n",
    "    token_id = torch.randint(0, embedding_matrix.size(0), (1,)).item()\n",
    "    \n",
    "    embedding = copy_embedding_matrix[token_id].clone().requires_grad_(True)\n",
    "    temp_embedding = copy_embedding_matrix[token_id].clone().detach()\n",
    "\n",
    "    optimizer = optimizer_cls([embedding], lr=lr)\n",
    "\n",
    "    bar = tqdm(\n",
    "        range(embedding_matrix.size(0)), \n",
    "        desc=f'Token [{token_idx + 1:2d}/{h_target.size(0):2d}]'\n",
    "    )\n",
    "\n",
    "    for _ in bar:\n",
    "        input_embeddings = torch.stack(\n",
    "            discovered_embeddings + [temp_embedding]\n",
    "        ).unsqueeze(0) \n",
    "\n",
    "        grad_oracle, loss = compute_last_token_embedding_grad_emb(\n",
    "            embeddings=input_embeddings, \n",
    "            llm=llm,\n",
    "            layer_idx=layer_idx,\n",
    "            h_target=h_target[token_idx],\n",
    "        )\n",
    "\n",
    "        grad_norm = grad_oracle.norm().item()\n",
    "        print(f\"Token ID: {token_id} - Grad norm: {grad_norm:.2e} - Loss: {loss:.2e}\")\n",
    "        string_so_far = tokenizer.decode(discovered_ids + [token_id], skip_special_tokens=True)\n",
    "        bar.set_postfix_str(f\"Loss: {loss:.2e} - Gradient norm: {grad_norm:.2e} - String: {string_so_far}\")\n",
    "\n",
    "        if loss < 1e-5 or grad_norm < 1e-12:\n",
    "            break\n",
    "\n",
    "        embedding.grad = grad_oracle\n",
    "        optimizer.step()\n",
    "\n",
    "        copy_embedding_matrix[token_id] = float('inf')\n",
    "        distances = torch.norm(copy_embedding_matrix - embedding, dim=1)\n",
    "        token_id = int(torch.argmin(distances))\n",
    "        temp_embedding = copy_embedding_matrix[token_id].clone()\n",
    "\n",
    "    return token_id, copy_embedding_matrix[token_id]\n",
    "\n",
    "\n",
    "def find_prompt(\n",
    "    llm, layer_idx, h_target,\n",
    "    optimizer_cls, lr,\n",
    "):\n",
    "    embedding_matrix = model.get_input_embeddings().weight\n",
    "\n",
    "    if h_target.dim() == 1:\n",
    "        h_target = h_target.unsqueeze(0)\n",
    "\n",
    "    discovered_embeddings = []\n",
    "    discovered_ids        = []\n",
    "\n",
    "    start_time = time()\n",
    "    for i in range(h_target.size(0)):\n",
    "        next_token_id, next_token_embedding = find_token(\n",
    "            i, embedding_matrix, \n",
    "            discovered_embeddings, discovered_ids, \n",
    "            llm, layer_idx, h_target,\n",
    "            optimizer_cls, lr\n",
    "        )\n",
    "\n",
    "        discovered_embeddings.append(next_token_embedding)\n",
    "        discovered_ids.append(next_token_id)\n",
    "    \n",
    "    end_time = time()\n",
    "\n",
    "    final_string = tokenizer.decode(discovered_ids, skip_special_tokens=True)\n",
    "\n",
    "    return end_time - start_time, final_string\n",
    "\n",
    "# from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "def inversion_attack(\n",
    "    prompt, llm, layer_idx,\n",
    "    optimizer_cls, lr,\n",
    "    seed=8\n",
    "):\n",
    "    \n",
    "    set_seed(seed)\n",
    "    h_target = get_whole(prompt, model, tokenizer, layer_idx)\n",
    "\n",
    "    invertion_time, predicted_prompt = find_prompt(\n",
    "        llm, layer_idx, h_target, \n",
    "        optimizer_cls, lr\n",
    "    )\n",
    "\n",
    "    print(f'Orignial prompt : {prompt}')\n",
    "    print(f'Predicted prompt: {predicted_prompt}')\n",
    "    print(f'Invertion time  : {invertion_time:.2f} seconds')\n",
    "\n",
    "inversion_attack(\n",
    "     # prompt= meaningful_df['50'][0],\n",
    "     prompt='12autoZeinai ena~~ !poli, a1212kiro pr33-=ompt tao op\"\\oio ;::/>elpizo na d1212isko1212leyt5646ei na ma77ntepsei to montelo',\n",
    "    llm=model, layer_idx=7, \n",
    "    # optimizer_cls=torch.optim.SGD, lr=1e-0\n",
    "    # llm=model, layer_idx=8, \n",
    "    optimizer_cls=torch.optim.AdamW, lr=1e-1\n",
    ")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce943af",
   "metadata": {},
   "source": [
    "# Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a7184c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"bookcorpus\", split=\"train\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7060d77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    sentence = dataset[i][\"text\"]\n",
    "    print(f\"{i= } \")\n",
    "    print(f\"Sentence: {sentence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99ee239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentences = [x['text'] for x in dataset if x['text'].strip()]\n",
    "# print(sentences[:5])\n",
    "seed = 42\n",
    "set_seed(seed)\n",
    "N = 100  # number of sentences to process\n",
    "#select a permuted subset of the dataset\n",
    "sentences = dataset.shuffle(seed=42).select(range(N))['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188f9591",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from datasets import Dataset\n",
    "from typing import List, Dict\n",
    "import numpy as np\n",
    "\n",
    "subdataset = dataset[:1000000]\n",
    "text_list = subdataset[\"text\"]\n",
    "\n",
    "# Wrap it in a dict\n",
    "text_dict = {\"text\": text_list}\n",
    "\n",
    "# Rebuild a Dataset with only that column\n",
    "from datasets import Dataset\n",
    "dataset_text_only = Dataset.from_dict(text_dict)\n",
    "\n",
    "def filter_by_token_length(\n",
    "    tokenizer,\n",
    "    token_lengths: List[int],\n",
    "    dataset: Dataset,\n",
    "    length: int,\n",
    "    seed: int = 42\n",
    ") -> Dict[int, Dataset]:\n",
    "\n",
    "    set_seed(seed)\n",
    "    # Tokenize and get lengths\n",
    "    def tokenize_length(example):\n",
    "        tokens = tokenizer(example[\"text\"], truncation=False, add_special_tokens=True)\n",
    "        return {\"token_length\": len(tokens[\"input_ids\"])}\n",
    "\n",
    "    dataset_with_lengths = dataset.map(tokenize_length, desc=\"Tokenizing and calculating lengths\")\n",
    "\n",
    "    result_datasets = {}\n",
    "\n",
    "    for token_length in token_lengths:\n",
    "        filtered = dataset_with_lengths.filter(\n",
    "            lambda x: x[\"token_length\"] == token_length,\n",
    "            desc=f\"Filtering for token length {token_length}\"\n",
    "        )\n",
    "\n",
    "        # Shuffle and take the desired number of rows\n",
    "        if len(filtered) >= length:\n",
    "            filtered = filtered.shuffle(seed=seed).select(range(length))\n",
    "            result_datasets[token_length] = filtered\n",
    "\n",
    "    return result_datasets\n",
    "\n",
    "lengths_tokens = [10, 20, 30, 40, 50]  # Example token lengths to filter by\n",
    "result_dataset= filter_by_token_length(\n",
    "    tokenizer,\n",
    "    token_lengths= lenghts_tokens,\n",
    "    # make the dataset a dict\n",
    "    dataset=dataset_text_only,\n",
    "    length=100,\n",
    "    seed=seed\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3960c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token_length, dataset in result_dataset.items():\n",
    "    print(f\"Token Length: {token_length}\")\n",
    "    print(f\"Dataset: {dataset['text'][:5]}\")\n",
    "    print(f\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbafe44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "result_dataset_df = {k: v['text'] for k, v in result_dataset.items()}\n",
    "result_dataset_df = pd.DataFrame(dict([(k, pd.Series(v)) for k, v in result_dataset_df.items()]))\n",
    "result_dataset_df.to_csv('../data/token_length_filtered_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fce963",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dataset_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d5b3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def random_sentence_of_size(\n",
    "    tokenizer,\n",
    "    n: int,\n",
    "    seed: int = 42\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Generate a random string that, when tokenized by `tokenizer`, has exactly n tokens.\n",
    "\n",
    "    Args:\n",
    "        tokenizer: a HuggingFace tokenizer.\n",
    "        n:         the exact number of tokens you want.\n",
    "        seed:      random seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        A string which tokenizes to length n.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: if the tokenizer’s vocab doesn’t have enough valid tokens.\n",
    "    \"\"\"\n",
    "    # 1) seed RNG\n",
    "    set_seed(seed)\n",
    "\n",
    "    # 2) build a list of “real” tokens\n",
    "    vocab = tokenizer.get_vocab()  # dict: token -> id\n",
    "    id_to_token = {idx: tok for tok, idx in vocab.items()}\n",
    "    special = set(tokenizer.all_special_tokens)\n",
    "\n",
    "    valid_tokens = [\n",
    "        tok for tok in id_to_token.values()\n",
    "        if tok not in special and not tok.startswith(\"##\")\n",
    "    ]\n",
    "\n",
    "    if len(valid_tokens) == 0:\n",
    "        raise ValueError(\"No valid tokens available in tokenizer vocab.\")\n",
    "\n",
    "    # 3) sample n tokens (with replacement so n can be large)\n",
    "    sampled = random.choices(valid_tokens, k=n)\n",
    "\n",
    "    # 4) join/clean up via the tokenizer’s decoder\n",
    "    return tokenizer.convert_tokens_to_string(sampled)\n",
    "\n",
    "def random_sentence_list(\n",
    "    tokenizer,\n",
    "    n: int,\n",
    "    num_sentences: int = 10,\n",
    "    seed: int = 42\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Generate a list of random strings, each of which tokenizes to exactly n tokens.\n",
    "\n",
    "    Args:\n",
    "        tokenizer:       a HuggingFace tokenizer.\n",
    "        n:               the exact number of tokens you want in each string.\n",
    "        num_sentences:   how many strings to generate.\n",
    "        seed:            random seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        A list of strings, each tokenizing to length n.\n",
    "    \"\"\"\n",
    "    return [random_sentence_of_size(tokenizer, n, seed+i) for i in range(num_sentences)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33a8530",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_random_sentence = random_sentence_list(\n",
    "    tokenizer,\n",
    "    n=10,\n",
    "    num_sentences=100,\n",
    "    seed=seed\n",
    ")\n",
    "\n",
    "# create a dataframe as before and save it the same place where now they all random of size 10, 20, 30, 40, 50\n",
    "# list comprension from lengths_tokens 10:...\n",
    "dict_random = {\n",
    "    f\"random_{length}\": random_sentence_list(\n",
    "        tokenizer,\n",
    "        n=length,\n",
    "        num_sentences=100,\n",
    "        seed=seed+length\n",
    "    ) for length in lengths_tokens\n",
    "}\n",
    "result_random_dataset_df = pd.DataFrame(dict([(k, pd.Series(v)) for k, v in dict_random.items()]))\n",
    "result_random_dataset_df.to_csv('../data/random_sentences_dataset.csv', index=False)\n",
    "result_random_dataset_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae3b67a",
   "metadata": {},
   "source": [
    "# Experiment 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41d6e40",
   "metadata": {},
   "source": [
    "<font color=\"#fc5a4e\">\n",
    "\n",
    "**TASK**: \n",
    "- **E8.1** show how the convergence rate change when you **change the gradient step-size**\n",
    "- **E8.2** show how the convergence rate change when you **change the first order algorithm**\n",
    "- **E8.3** show how the convergence rate change when you **change the lenght of the tokens**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cfeb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a list of stepsizes for algorithms\n",
    "step_sizes = [1e-0, 1e-1, 1e-2, 1e-3, 1e-4, 1e-5]\n",
    "# make a list of optimizers: sgd, adam, adamw, rmsprop, nesterov, LBFGS\n",
    "optimizers = {\n",
    "    \"sgd\":torch.optim.SGD,\n",
    "    \"adam\":torch.optim.Adam,\n",
    "    \"adamw\":torch.optim.AdamW,\n",
    "    \"rmsprop\":torch.optim.RMSprop,\n",
    "    \"nadam\":torch.optim.NAdam,\n",
    "    \"lbfgs\":torch.optim.LBFGS\n",
    "}\n",
    "# make a list of lenght_tokens\n",
    "lengths_tokens = [10, 20, 30, 40, 50]  \n",
    "\n",
    "# make a list of layer indices\n",
    "layer_indices = [1, 2, 3, 4, 5, 6, 7, 8]\n",
    "\n",
    "# random df \n",
    "random_df = pd.read_csv('../data/random_sentences_dataset.csv')\n",
    "# meaningful df\n",
    "meaningful_df = pd.read_csv('../data/token_length_filtered_dataset.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ac56d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_prompt = meaningful_df['10'][0]\n",
    "print(f\"Trial prompt: {meaningful_df['50'][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5a7b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "inversion_attack(\n",
    "    prompt=meaningful_df['50'][2],\n",
    "    llm=model, \n",
    "    layer_idx=7, \n",
    "    optimizer_cls= torch.optim.LBFGS, \n",
    "    lr= step_sizes[-1],\n",
    "    seed=seed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71cf317",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "itallm2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
