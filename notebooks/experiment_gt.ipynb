{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "078025ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from typing import List, Dict, Any\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c23dfa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp1 = \"/root/tommaso/Variety/LLM-Inversion/results/ALL_complete_meaningful_2.csv\"\n",
    "exp2 = \"/root/tommaso/Variety/LLM-Inversion/results/ALL_complete_meaningful.csv\"\n",
    "exp3 = \"/root/tommaso/Variety/LLM-Inversion/results/Baseline_20_meaningful.csv\"\n",
    "exp4 = \"/root/tommaso/Variety/LLM-Inversion/results/Baseline_20_random.csv\"\n",
    "exp5 = \"/root/tommaso/Variety/LLM-Inversion/results/SGD_20_random.csv\"\n",
    "exp6 = \"/root/tommaso/Variety/LLM-Inversion/results/SGD_complete_meaningful.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1532fdab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(exp1)\n",
    "df2 = pd.read_csv(exp2)\n",
    "df3 = pd.read_csv(exp3)\n",
    "df4 = pd.read_csv(exp4)\n",
    "df5 = pd.read_csv(exp5)\n",
    "df6 = pd.read_csv(exp6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c5c0b7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat df1 and df2 only \n",
    "df12 = pd.concat([df1, df2], ignore_index=True)\n",
    "# remove element with optimizer column == \"LBFGS\"\n",
    "df12 = df12[df12[\"optimizer\"] != \"LBFGS\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8774e940",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>index</th>\n",
       "      <th>layer</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>token_length</th>\n",
       "      <th>match</th>\n",
       "      <th>inversion_time</th>\n",
       "      <th>timesteps</th>\n",
       "      <th>times</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>token_length_filtered_dataset.csv</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.01</td>\n",
       "      <td>Adam</td>\n",
       "      <td>20</td>\n",
       "      <td>True</td>\n",
       "      <td>20.923944</td>\n",
       "      <td>88_134_77_68_113_119_115_83_69_58_107_132_270_...</td>\n",
       "      <td>0.91_1.31_0.75_0.67_1.10_1.16_1.13_0.81_0.68_0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>token_length_filtered_dataset.csv</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.01</td>\n",
       "      <td>Adam</td>\n",
       "      <td>20</td>\n",
       "      <td>True</td>\n",
       "      <td>23.161463</td>\n",
       "      <td>71_43_89_215_187_420_104_28_32_106_140_76_116_...</td>\n",
       "      <td>0.66_0.42_0.87_2.08_1.78_4.03_0.96_0.26_0.29_0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>token_length_filtered_dataset.csv</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.01</td>\n",
       "      <td>Adam</td>\n",
       "      <td>20</td>\n",
       "      <td>True</td>\n",
       "      <td>18.770709</td>\n",
       "      <td>71_40_130_126_116_105_122_69_66_152_31_118_113...</td>\n",
       "      <td>0.62_0.37_1.19_1.16_1.07_0.97_1.12_0.63_0.61_1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>token_length_filtered_dataset.csv</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0.01</td>\n",
       "      <td>Adam</td>\n",
       "      <td>20</td>\n",
       "      <td>True</td>\n",
       "      <td>18.173630</td>\n",
       "      <td>30_319_199_19_39_127_62_99_33_40_54_101_108_14...</td>\n",
       "      <td>0.26_2.92_1.83_0.17_0.36_1.17_0.57_0.91_0.30_0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>token_length_filtered_dataset.csv</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.01</td>\n",
       "      <td>Adam</td>\n",
       "      <td>20</td>\n",
       "      <td>True</td>\n",
       "      <td>21.562065</td>\n",
       "      <td>30_38_177_67_35_109_88_171_124_122_158_141_130...</td>\n",
       "      <td>0.27_0.35_1.65_0.62_0.33_1.02_0.82_1.60_1.16_1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             dataset  index  layer  learning_rate optimizer  \\\n",
       "0  token_length_filtered_dataset.csv      0      4           0.01      Adam   \n",
       "1  token_length_filtered_dataset.csv      1      4           0.01      Adam   \n",
       "2  token_length_filtered_dataset.csv      2      4           0.01      Adam   \n",
       "3  token_length_filtered_dataset.csv      3      4           0.01      Adam   \n",
       "4  token_length_filtered_dataset.csv      4      4           0.01      Adam   \n",
       "\n",
       "   token_length  match  inversion_time  \\\n",
       "0            20   True       20.923944   \n",
       "1            20   True       23.161463   \n",
       "2            20   True       18.770709   \n",
       "3            20   True       18.173630   \n",
       "4            20   True       21.562065   \n",
       "\n",
       "                                           timesteps  \\\n",
       "0  88_134_77_68_113_119_115_83_69_58_107_132_270_...   \n",
       "1  71_43_89_215_187_420_104_28_32_106_140_76_116_...   \n",
       "2  71_40_130_126_116_105_122_69_66_152_31_118_113...   \n",
       "3  30_319_199_19_39_127_62_99_33_40_54_101_108_14...   \n",
       "4  30_38_177_67_35_109_88_171_124_122_158_141_130...   \n",
       "\n",
       "                                               times  \n",
       "0  0.91_1.31_0.75_0.67_1.10_1.16_1.13_0.81_0.68_0...  \n",
       "1  0.66_0.42_0.87_2.08_1.78_4.03_0.96_0.26_0.29_0...  \n",
       "2  0.62_0.37_1.19_1.16_1.07_0.97_1.12_0.63_0.61_1...  \n",
       "3  0.26_2.92_1.83_0.17_0.36_1.17_0.57_0.91_0.30_0...  \n",
       "4  0.27_0.35_1.65_0.62_0.33_1.02_0.82_1.60_1.16_1...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ec8126cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_time_steps(df, last_timestep: List[int] = [10], type: str = \"timesteps\"):\n",
    "    assert type in [\"timesteps\", \"times\"], \"give a valid type\"\n",
    "    column = df[type]\n",
    "    scores = column.apply(lambda x: list(float(i) for i in x.split(\"_\")))\n",
    "    # initialize an empty dictionary to store the results\n",
    "    results: Dict[str, Any] = {}\n",
    "    # for loop in the time steps and compute the mean and std score up to that point, save it in a dictionary with keys mean_{type}_n std...\n",
    "    for i in last_timestep:\n",
    "        mean_score = scores.apply(lambda x: np.mean(x[:i+1])).mean()\n",
    "        std_score = scores.apply(lambda x: np.std(x[:i+1])).mean()\n",
    "        # mean_score = scores.apply(lambda x: sum(x[:i + 1]) / (i + 1)).mean()\n",
    "        # std_score = scores.apply(lambda x: (sum((xi - mean_score) ** 2 for xi in x[:i + 1]) / (i + 1)) ** 0.5).mean()\n",
    "        results[f\"mean_{type}_{i}\"] = mean_score\n",
    "        results[f\"std_{type}_{i}\"] = std_score\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b0c4f490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print columns with uniques values \n",
    "def print_unique_columns(df):\n",
    "    for column in df.columns:\n",
    "        unique_values = df[column].unique()\n",
    "        print(f\"Column: {column}, Unique values: {unique_values}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80eb19ed",
   "metadata": {},
   "source": [
    "# Plotting Experiment 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7cf70429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: dataset, Unique values: ['token_length_filtered_dataset.csv']\n",
      "Column: index, Unique values: [0 1 2 3 4 5 6 7 8 9]\n",
      "Column: layer, Unique values: [4]\n",
      "Column: learning_rate, Unique values: [0.01 1.  ]\n",
      "Column: optimizer, Unique values: ['Adam' 'AdamW' 'RMSprop']\n",
      "Column: token_length, Unique values: [20]\n",
      "Column: match, Unique values: [ True]\n",
      "Column: inversion_time, Unique values: [ 20.92394352  23.16146302  18.77070904  18.17362976  21.56206465\n",
      "  18.08984542  19.00598383 133.53544545  19.87070227  15.57244444\n",
      "  20.49819398  23.43648076  19.46755981  18.70466161  22.06325674\n",
      "  18.39428163  18.75789785 125.87159395  19.64991283  15.32918692\n",
      "   9.786731    12.05488276   8.40075135   9.27157784  11.47732067\n",
      "   7.05476165   8.26860595 125.61394978   9.10261989  80.78740621\n",
      "   2.63861823   2.52050972   2.07424045   2.8558774    2.91652989\n",
      "   2.22158504   2.51750231 213.06782103   3.25891304 117.56600642\n",
      "   2.66802812   2.52497172   2.04329276   2.87625432   3.31027627\n",
      "   2.23083639   2.51010609 261.73354769   3.13366866 129.92499161\n",
      "   2.70339942   2.31594467   2.33515334   3.6140573    3.33176374\n",
      "   2.05391288   2.33065438 209.32837176   3.72384381 116.65599394]\n",
      "Column: timesteps, Unique values: ['88_134_77_68_113_119_115_83_69_58_107_132_270_87_197_98_65_115_92_56'\n",
      " '71_43_89_215_187_420_104_28_32_106_140_76_116_231_251_63_81_98_69_47'\n",
      " '71_40_130_126_116_105_122_69_66_152_31_118_113_116_121_47_87_297_63_48'\n",
      " '30_319_199_19_39_127_62_99_33_40_54_101_108_142_230_20_29_147_122_46'\n",
      " '30_38_177_67_35_109_88_171_124_122_158_141_130_27_36_353_130_26_296_44'\n",
      " '58_135_85_91_95_137_77_24_136_110_109_122_93_130_95_73_111_109_90_54'\n",
      " '71_99_140_58_117_49_99_98_104_25_86_145_152_172_65_73_130_112_153_62'\n",
      " '19_111_128_102_62_25_153_38_3_69_48_81_51_115_249_71_70_155_12247_56'\n",
      " '30_112_87_17_36_120_49_30_269_126_41_101_140_274_91_93_126_216_65_52'\n",
      " '35_196_99_151_5_16_201_77_211_29_41_34_10_53_106_89_61_36_120_59'\n",
      " '89_132_77_68_113_119_114_83_69_58_107_131_273_84_201_97_65_115_92_57'\n",
      " '71_43_89_213_187_421_104_28_32_106_139_78_113_227_240_63_81_98_69_47'\n",
      " '71_40_130_125_116_104_123_69_66_151_31_117_115_117_120_47_87_294_63_48'\n",
      " '30_323_195_19_39_124_62_99_33_40_54_101_106_143_228_20_29_148_123_46'\n",
      " '30_38_175_67_35_110_88_170_124_124_158_143_129_27_36_355_133_26_300_44'\n",
      " '58_135_85_91_95_137_76_24_137_106_110_123_94_130_97_73_111_109_89_54'\n",
      " '71_99_140_58_116_49_98_97_107_25_85_147_154_174_65_73_131_112_156_62'\n",
      " '19_120_128_103_62_25_150_38_3_69_48_81_51_115_259_71_71_156_11801_56'\n",
      " '30_113_87_17_36_120_49_30_269_126_41_100_140_268_91_92_126_219_65_52'\n",
      " '35_193_97_150_5_16_198_76_209_29_42_34_10_53_106_90_61_36_119_60'\n",
      " '28_61_25_15_47_59_35_28_26_26_38_56_229_36_125_37_23_62_54_26'\n",
      " '17_13_31_75_102_321_42_12_10_44_76_32_60_122_200_19_26_34_24_16'\n",
      " '17_12_79_63_40_34_61_19_20_69_10_36_44_42_34_20_32_211_26_17'\n",
      " '8_224_124_6_12_46_8_35_9_15_24_57_35_41_167_4_10_64_75_18'\n",
      " '8_13_63_25_11_50_35_126_60_61_124_79_48_7_12_254_51_9_167_11'\n",
      " '18_48_26_30_39_63_25_4_51_48_63_45_39_52_34_25_54_31_26_26'\n",
      " '17_43_85_13_45_10_35_47_39_5_28_48_98_101_18_29_80_40_77_18'\n",
      " '5_54_45_46_19_5_76_10_337_21_19_41_12_44_96_39_18_65_12138_25'\n",
      " '8_46_30_5_11_49_17_8_169_63_12_28_50_139_30_33_60_164_26_15'\n",
      " '11_123_40_89_7908_4_84_24_151_7_16_10_4_13_38_30_18_12_64_14'\n",
      " '5_4_5_17_10_4_10_5_4_9_36_5_20_6_11_7_9_8_11_12'\n",
      " '10_6_6_23_10_9_10_4_2_16_8_13_11_20_15_3_7_8_6_10'\n",
      " '10_4_9_7_10_10_8_2_4_5_9_7_7_4_9_4_11_25_6_11'\n",
      " '4_11_34_11_10_8_12_5_7_10_11_14_7_8_22_8_3_8_21_9'\n",
      " '4_7_18_6_3_3_9_15_7_6_9_15_8_10_3_49_8_21_18_8'\n",
      " '6_12_8_5_6_8_12_5_9_5_8_6_5_6_13_8_21_9_9_13'\n",
      " '10_8_10_7_6_5_7_26_8_6_4_7_7_5_9_4_8_8_21_30'\n",
      " '6_11_9_5_8_5_7_8_3663_5_8_12_7_8_241_5_14_10_12567_12'\n",
      " '4_8_8_6_7_9_10_8_65_14_4_5_8_46_14_7_8_10_5_11'\n",
      " '6_58_9_10_8998_5_12_9_14_4_7_5_48_7_5_5_5_4_6_10'\n",
      " '5_4_5_15_10_4_10_5_4_9_45_5_20_6_14_7_9_8_11_12'\n",
      " '10_6_6_23_10_9_10_4_2_19_8_13_11_18_14_3_7_8_6_10'\n",
      " '10_4_9_7_10_10_8_2_4_5_9_7_7_4_9_4_10_23_6_11'\n",
      " '4_11_36_12_10_8_13_5_7_10_11_14_7_8_21_8_3_8_20_9'\n",
      " '4_7_18_6_3_3_9_15_7_6_9_18_8_10_3_73_8_24_18_8'\n",
      " '10_8_10_7_5_5_7_25_8_6_4_7_7_5_9_4_8_7_22_31'\n",
      " '6_11_9_5_8_5_7_8_4592_5_8_12_7_8_276_5_15_10_15161_12'\n",
      " '4_8_8_6_7_9_10_8_62_14_4_5_8_37_13_7_8_10_5_11'\n",
      " '6_120_9_10_9967_5_12_9_13_4_7_5_44_7_5_5_5_4_6_10'\n",
      " '6_15_5_4_5_3_9_4_4_18_23_4_31_6_18_5_10_7_18_18'\n",
      " '7_6_5_33_5_10_5_8_2_12_8_16_6_13_18_2_9_4_6_9'\n",
      " '7_3_17_13_7_12_10_2_5_3_11_8_5_3_6_5_11_41_7_9'\n",
      " '3_8_46_19_25_7_7_3_6_12_17_22_4_8_24_22_3_10_27_14'\n",
      " '3_11_18_4_3_9_10_18_5_5_8_13_7_14_4_43_12_44_13_16'\n",
      " '9_8_9_4_7_7_9_6_7_4_7_9_5_5_8_5_25_8_6_12'\n",
      " '7_6_15_6_6_5_7_12_6_11_4_5_7_5_8_6_8_7_30_21'\n",
      " '4_25_7_10_12_7_7_9_3624_7_7_18_7_6_19_5_8_12_12517_11'\n",
      " '3_9_8_11_4_11_14_12_52_16_4_6_5_88_11_8_8_11_5_10'\n",
      " '7_63_9_6_8993_5_18_5_21_7_7_6_52_5_5_4_10_16_4_12']\n",
      "Column: times, Unique values: ['0.91_1.31_0.75_0.67_1.10_1.16_1.13_0.81_0.68_0.57_1.06_1.30_2.61_0.83_1.89_0.94_0.63_1.12_0.90_0.55'\n",
      " '0.66_0.42_0.87_2.08_1.78_4.03_0.96_0.26_0.29_0.98_1.29_0.70_1.07_2.13_2.31_0.58_0.75_0.91_0.64_0.44'\n",
      " '0.62_0.37_1.19_1.16_1.07_0.97_1.12_0.63_0.61_1.40_0.29_1.09_1.04_1.07_1.12_0.43_0.81_2.77_0.59_0.45'\n",
      " '0.26_2.92_1.83_0.17_0.36_1.17_0.57_0.91_0.30_0.37_0.50_0.93_1.00_1.31_2.14_0.19_0.27_1.39_1.15_0.43'\n",
      " '0.27_0.35_1.65_0.62_0.33_1.02_0.82_1.60_1.16_1.14_1.48_1.32_1.22_0.25_0.34_3.31_1.23_0.25_2.80_0.42'\n",
      " '0.52_1.26_0.79_0.85_0.89_1.28_0.72_0.22_1.27_1.03_1.02_1.14_0.87_1.22_0.89_0.68_1.05_1.03_0.85_0.51'\n",
      " '0.63_0.92_1.31_0.55_1.11_0.46_0.94_0.93_0.98_0.24_0.81_1.37_1.44_1.63_0.62_0.69_1.24_1.07_1.46_0.59'\n",
      " '0.17_1.05_1.21_0.96_0.59_0.24_1.45_0.36_0.03_0.65_0.45_0.77_0.48_1.09_2.36_0.67_0.67_1.48_118.32_0.54'\n",
      " '0.27_1.07_0.83_0.16_0.34_1.15_0.47_0.29_2.57_1.20_0.39_0.97_1.34_2.62_0.87_0.89_1.22_2.09_0.63_0.50'\n",
      " '0.32_1.86_0.94_1.44_0.05_0.15_1.92_0.74_2.02_0.28_0.39_0.33_0.10_0.51_1.01_0.85_0.59_0.35_1.16_0.57'\n",
      " '0.81_1.26_0.73_0.65_1.08_1.14_1.09_0.79_0.66_0.55_1.02_1.25_2.61_0.80_1.93_0.93_0.63_1.11_0.89_0.55'\n",
      " '0.65_0.41_0.85_2.03_1.79_4.03_1.00_0.27_0.31_1.01_1.33_0.75_1.08_2.18_2.30_0.60_0.78_0.95_0.67_0.45'\n",
      " '0.65_0.38_1.24_1.19_1.11_0.99_1.18_0.66_0.63_1.44_0.30_1.12_1.10_1.12_1.15_0.45_0.84_2.84_0.61_0.46'\n",
      " '0.27_3.06_1.85_0.18_0.37_1.18_0.59_0.94_0.31_0.38_0.51_0.96_1.01_1.36_2.18_0.19_0.28_1.43_1.19_0.44'\n",
      " '0.27_0.36_1.66_0.64_0.33_1.05_0.84_1.62_1.18_1.18_1.51_1.36_1.23_0.26_0.34_3.38_1.28_0.25_2.89_0.42'\n",
      " '0.53_1.28_0.81_0.86_0.90_1.30_0.72_0.23_1.30_1.01_1.05_1.17_0.90_1.24_0.92_0.70_1.07_1.04_0.85_0.52'\n",
      " '0.63_0.92_1.30_0.54_1.08_0.45_0.91_0.90_0.99_0.23_0.79_1.37_1.43_1.62_0.60_0.68_1.23_1.05_1.46_0.58'\n",
      " '0.17_1.11_1.18_0.95_0.57_0.23_1.39_0.35_0.03_0.64_0.45_0.75_0.47_1.07_2.41_0.66_0.67_1.47_110.76_0.54'\n",
      " '0.27_1.06_0.82_0.16_0.34_1.14_0.46_0.28_2.55_1.19_0.39_0.95_1.33_2.54_0.86_0.87_1.21_2.10_0.62_0.50'\n",
      " '0.32_1.82_0.91_1.41_0.05_0.15_1.88_0.72_1.98_0.27_0.40_0.32_0.09_0.50_1.01_0.85_0.58_0.35_1.14_0.57'\n",
      " '0.25_0.57_0.23_0.14_0.44_0.56_0.33_0.26_0.25_0.25_0.36_0.53_2.16_0.34_1.18_0.35_0.22_0.59_0.52_0.25'\n",
      " '0.15_0.12_0.29_0.71_0.96_3.03_0.40_0.11_0.09_0.42_0.72_0.30_0.57_1.15_1.89_0.18_0.25_0.33_0.23_0.15'\n",
      " '0.15_0.11_0.75_0.60_0.38_0.32_0.58_0.18_0.19_0.65_0.09_0.34_0.42_0.40_0.32_0.19_0.31_2.01_0.25_0.16'\n",
      " '0.07_2.10_1.17_0.06_0.11_0.43_0.08_0.33_0.08_0.14_0.23_0.54_0.33_0.39_1.58_0.04_0.10_0.61_0.72_0.17'\n",
      " '0.07_0.12_0.59_0.23_0.10_0.47_0.33_1.19_0.57_0.58_1.17_0.75_0.45_0.07_0.11_2.40_0.49_0.09_1.59_0.10'\n",
      " '0.16_0.45_0.24_0.28_0.37_0.59_0.24_0.04_0.48_0.45_0.60_0.43_0.37_0.49_0.32_0.24_0.51_0.30_0.25_0.25'\n",
      " '0.15_0.40_0.80_0.12_0.42_0.09_0.33_0.44_0.37_0.05_0.26_0.45_0.92_0.95_0.17_0.27_0.76_0.38_0.73_0.17'\n",
      " '0.05_0.51_0.42_0.43_0.18_0.05_0.72_0.09_3.18_0.20_0.18_0.39_0.11_0.42_0.91_0.37_0.17_0.62_116.40_0.24'\n",
      " '0.07_0.43_0.28_0.05_0.10_0.46_0.16_0.08_1.59_0.59_0.11_0.26_0.47_1.31_0.28_0.31_0.57_1.56_0.25_0.14'\n",
      " '0.10_1.15_0.38_0.84_73.76_0.04_0.78_0.22_1.41_0.07_0.15_0.09_0.04_0.12_0.35_0.28_0.17_0.11_0.60_0.13'\n",
      " '0.14_0.06_0.06_0.22_0.13_0.05_0.13_0.06_0.05_0.12_0.46_0.06_0.26_0.08_0.14_0.09_0.12_0.10_0.14_0.15'\n",
      " '0.12_0.08_0.08_0.29_0.13_0.12_0.13_0.05_0.03_0.21_0.10_0.17_0.14_0.26_0.19_0.04_0.09_0.10_0.08_0.13'\n",
      " '0.12_0.05_0.11_0.09_0.13_0.13_0.10_0.03_0.05_0.06_0.11_0.09_0.09_0.05_0.12_0.05_0.14_0.32_0.08_0.14'\n",
      " '0.05_0.14_0.43_0.14_0.13_0.10_0.15_0.06_0.09_0.13_0.14_0.18_0.09_0.10_0.28_0.10_0.04_0.10_0.27_0.12'\n",
      " '0.05_0.09_0.23_0.08_0.04_0.04_0.12_0.19_0.09_0.08_0.12_0.19_0.10_0.13_0.04_0.63_0.10_0.27_0.23_0.10'\n",
      " '0.07_0.15_0.10_0.06_0.08_0.10_0.15_0.06_0.11_0.06_0.10_0.08_0.06_0.08_0.17_0.10_0.27_0.12_0.12_0.17'\n",
      " '0.13_0.10_0.13_0.09_0.08_0.06_0.09_0.34_0.10_0.08_0.05_0.09_0.09_0.06_0.12_0.05_0.10_0.10_0.27_0.39'\n",
      " '0.07_0.14_0.11_0.06_0.10_0.06_0.09_0.10_46.99_0.06_0.10_0.15_0.09_0.10_3.09_0.06_0.18_0.13_161.21_0.15'\n",
      " '0.05_0.10_0.10_0.08_0.09_0.11_0.13_0.10_0.82_0.18_0.05_0.06_0.10_0.58_0.18_0.09_0.10_0.13_0.06_0.14'\n",
      " '0.07_0.74_0.11_0.13_114.64_0.06_0.15_0.11_0.18_0.05_0.09_0.06_0.62_0.09_0.06_0.06_0.06_0.05_0.08_0.13'\n",
      " '0.06_0.05_0.06_0.19_0.13_0.05_0.13_0.06_0.05_0.12_0.58_0.06_0.26_0.08_0.18_0.09_0.12_0.10_0.14_0.16'\n",
      " '0.12_0.08_0.08_0.29_0.13_0.11_0.13_0.05_0.03_0.25_0.10_0.17_0.14_0.23_0.18_0.04_0.09_0.10_0.08_0.13'\n",
      " '0.13_0.05_0.12_0.09_0.13_0.13_0.10_0.03_0.05_0.06_0.11_0.09_0.09_0.05_0.12_0.05_0.13_0.30_0.08_0.14'\n",
      " '0.05_0.14_0.46_0.15_0.13_0.10_0.17_0.06_0.09_0.13_0.14_0.18_0.09_0.10_0.27_0.10_0.04_0.10_0.26_0.12'\n",
      " '0.05_0.09_0.23_0.08_0.04_0.04_0.12_0.19_0.09_0.08_0.12_0.23_0.10_0.13_0.04_0.93_0.10_0.31_0.24_0.10'\n",
      " '0.08_0.15_0.10_0.06_0.08_0.10_0.15_0.06_0.11_0.06_0.10_0.08_0.06_0.08_0.17_0.10_0.27_0.12_0.12_0.17'\n",
      " '0.13_0.10_0.13_0.09_0.06_0.06_0.09_0.32_0.10_0.08_0.05_0.09_0.09_0.06_0.12_0.05_0.10_0.09_0.29_0.40'\n",
      " '0.08_0.14_0.11_0.06_0.10_0.06_0.09_0.10_59.00_0.06_0.10_0.15_0.09_0.10_3.56_0.06_0.19_0.13_197.37_0.16'\n",
      " '0.05_0.10_0.10_0.08_0.09_0.11_0.13_0.10_0.80_0.18_0.05_0.06_0.10_0.47_0.17_0.09_0.10_0.13_0.06_0.14'\n",
      " '0.08_1.53_0.12_0.13_126.29_0.06_0.15_0.11_0.16_0.05_0.09_0.06_0.56_0.09_0.06_0.06_0.06_0.05_0.08_0.13'\n",
      " '0.07_0.19_0.06_0.05_0.06_0.04_0.11_0.05_0.05_0.23_0.29_0.05_0.39_0.08_0.23_0.06_0.13_0.09_0.23_0.23'\n",
      " '0.09_0.07_0.06_0.41_0.06_0.13_0.06_0.10_0.03_0.15_0.10_0.20_0.08_0.16_0.23_0.03_0.12_0.05_0.08_0.11'\n",
      " '0.09_0.04_0.21_0.16_0.09_0.15_0.13_0.02_0.06_0.04_0.14_0.10_0.06_0.04_0.07_0.06_0.14_0.53_0.09_0.12'\n",
      " '0.04_0.10_0.57_0.24_0.31_0.09_0.09_0.04_0.07_0.15_0.21_0.28_0.05_0.10_0.30_0.28_0.04_0.13_0.35_0.18'\n",
      " '0.04_0.14_0.23_0.05_0.04_0.11_0.13_0.23_0.06_0.06_0.10_0.17_0.09_0.18_0.05_0.55_0.16_0.58_0.17_0.21'\n",
      " '0.11_0.10_0.11_0.05_0.09_0.09_0.12_0.08_0.09_0.05_0.09_0.11_0.06_0.06_0.10_0.06_0.33_0.11_0.08_0.16'\n",
      " '0.09_0.08_0.19_0.08_0.08_0.06_0.09_0.15_0.08_0.14_0.05_0.06_0.09_0.06_0.10_0.08_0.10_0.09_0.39_0.27'\n",
      " '0.05_0.31_0.09_0.13_0.15_0.09_0.09_0.11_45.77_0.09_0.09_0.23_0.09_0.08_0.24_0.06_0.10_0.16_161.27_0.14'\n",
      " '0.04_0.11_0.10_0.14_0.05_0.14_0.17_0.15_0.65_0.20_0.05_0.07_0.06_1.11_0.14_0.10_0.10_0.14_0.06_0.13'\n",
      " '0.09_0.79_0.11_0.07_113.35_0.06_0.23_0.06_0.26_0.09_0.09_0.08_0.66_0.06_0.06_0.05_0.13_0.20_0.05_0.15']\n"
     ]
    }
   ],
   "source": [
    "print_unique_columns(df12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6a452fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for layer 4, learning rate 0.01, optimizer Adam:\n",
      "{'mean_timesteps_0': np.float64(50.3), 'std_timesteps_0': np.float64(0.0), 'mean_timesteps_1': np.float64(86.5), 'std_timesteps_1': np.float64(42.1), 'mean_timesteps_2': np.float64(98.03333333333333), 'std_timesteps_2': np.float64(47.56022793113632), 'mean_timesteps_3': np.float64(96.375), 'std_timesteps_3': np.float64(51.310030647698355), 'mean_timesteps_4': np.float64(93.20000000000002), 'std_timesteps_4': np.float64(50.242646780429155), 'mean_timesteps_5': np.float64(98.11666666666666), 'std_timesteps_5': np.float64(55.95389228848838), 'mean_timesteps_6': np.float64(99.38571428571429), 'std_timesteps_6': np.float64(54.68701868437718), 'mean_timesteps_7': np.float64(95.925), 'std_timesteps_7': np.float64(54.6872776104113), 'mean_timesteps_8': np.float64(96.9), 'std_timesteps_8': np.float64(58.66389698011629), 'mean_timesteps_9': np.float64(95.58000000000001), 'std_timesteps_9': np.float64(57.964850175505546), 'mean_timesteps_10': np.float64(94.30000000000001), 'std_timesteps_10': np.float64(56.66996122811107), 'mean_timesteps_11': np.float64(95.20000000000002), 'std_timesteps_11': np.float64(55.50860786819866), 'mean_timesteps_12': np.float64(96.97692307692307), 'std_timesteps_12': np.float64(57.109549639663655), 'mean_timesteps_13': np.float64(99.67142857142856), 'std_timesteps_13': np.float64(58.421844359704686), 'mean_timesteps_14': np.float64(102.63333333333333), 'std_timesteps_14': np.float64(60.380710211618364), 'mean_timesteps_15': np.float64(102.34375), 'std_timesteps_15': np.float64(62.22542608973489), 'mean_timesteps_16': np.float64(101.55882352941177), 'std_timesteps_16': np.float64(60.923801126142244), 'mean_timesteps_17': np.float64(103.20000000000002), 'std_timesteps_17': np.float64(62.750430366790326), 'mean_timesteps_18': np.float64(167.85789473684207), 'std_timesteps_18': np.float64(328.63981211043875), 'mean_timesteps_19': np.float64(162.085), 'std_timesteps_19': np.float64(321.80530728184743)}\n",
      "Results for layer 4, learning rate 0.01, optimizer AdamW:\n",
      "{'mean_timesteps_0': np.float64(50.4), 'std_timesteps_0': np.float64(0.0), 'mean_timesteps_1': np.float64(87.0), 'std_timesteps_1': np.float64(42.5), 'mean_timesteps_2': np.float64(98.1), 'std_timesteps_2': np.float64(47.58543547698646), 'mean_timesteps_3': np.float64(96.35), 'std_timesteps_3': np.float64(51.20980720293996), 'mean_timesteps_4': np.float64(93.16), 'std_timesteps_4': np.float64(50.16957565645386), 'mean_timesteps_5': np.float64(98.04999999999998), 'std_timesteps_5': np.float64(55.97748377780145), 'mean_timesteps_6': np.float64(99.2142857142857), 'std_timesteps_6': np.float64(54.58192336807099), 'mean_timesteps_7': np.float64(95.7375), 'std_timesteps_7': np.float64(54.57732300653898), 'mean_timesteps_8': np.float64(96.75555555555556), 'std_timesteps_8': np.float64(58.586912222365996), 'mean_timesteps_9': np.float64(95.42), 'std_timesteps_9': np.float64(57.86647502105727), 'mean_timesteps_10': np.float64(94.15454545454546), 'std_timesteps_10': np.float64(56.573402037353006), 'mean_timesteps_11': np.float64(95.10000000000001), 'std_timesteps_11': np.float64(55.43932660676194), 'mean_timesteps_12': np.float64(96.9), 'std_timesteps_12': np.float64(57.16577967949231), 'mean_timesteps_13': np.float64(99.53571428571429), 'std_timesteps_13': np.float64(58.388642938130225), 'mean_timesteps_14': np.float64(102.52000000000001), 'std_timesteps_14': np.float64(60.45730330401833), 'mean_timesteps_15': np.float64(102.24375), 'std_timesteps_15': np.float64(62.338887715251474), 'mean_timesteps_16': np.float64(101.49411764705881), 'std_timesteps_16': np.float64(61.032265945756784), 'mean_timesteps_17': np.float64(103.15), 'std_timesteps_17': np.float64(62.83204584905851), 'mean_timesteps_18': np.float64(165.49473684210525), 'std_timesteps_18': np.float64(318.62863740549267), 'mean_timesteps_19': np.float64(159.85), 'std_timesteps_19': np.float64(312.0234636848817)}\n",
      "Results for layer 4, learning rate 0.01, optimizer RMSprop:\n",
      "{'mean_timesteps_0': np.float64(13.7), 'std_timesteps_0': np.float64(0.0), 'mean_timesteps_1': np.float64(38.7), 'std_timesteps_1': np.float64(25.9), 'mean_timesteps_2': np.float64(44.06666666666667), 'std_timesteps_2': np.float64(29.263271162226438), 'mean_timesteps_3': np.float64(42.225), 'std_timesteps_3': np.float64(30.173481812426058), 'mean_timesteps_4': np.float64(198.46), 'std_timesteps_4': np.float64(339.22317141733123), 'mean_timesteps_5': np.float64(176.06666666666666), 'std_timesteps_5': np.float64(325.5708868976986), 'mean_timesteps_6': np.float64(156.8857142857143), 'std_timesteps_6': np.float64(306.4013283996222), 'mean_timesteps_7': np.float64(141.1875), 'std_timesteps_7': np.float64(292.4721749065391), 'mean_timesteps_8': np.float64(135.1888888888889), 'std_timesteps_8': np.float64(288.86068248180413), 'mean_timesteps_9': np.float64(125.25999999999999), 'std_timesteps_9': np.float64(276.5100304193605), 'mean_timesteps_10': np.float64(117.6), 'std_timesteps_10': np.float64(266.1903322638083), 'mean_timesteps_11': np.float64(111.4), 'std_timesteps_11': np.float64(256.19974760227234), 'mean_timesteps_12': np.float64(107.59230769230768), 'std_timesteps_12': np.float64(251.61421034104197), 'mean_timesteps_13': np.float64(104.17142857142858), 'std_timesteps_13': np.float64(244.74877496195558), 'mean_timesteps_14': np.float64(102.25333333333333), 'std_timesteps_14': np.float64(239.04683146535854), 'mean_timesteps_15': np.float64(98.925), 'std_timesteps_15': np.float64(234.81726051358092), 'mean_timesteps_16': np.float64(95.29411764705883), 'std_timesteps_16': np.float64(228.6787785079686), 'mean_timesteps_17': np.float64(93.84444444444445), 'std_timesteps_17': np.float64(226.06967675922152), 'mean_timesteps_18': np.float64(155.62631578947367), 'std_timesteps_18': np.float64(483.8807587832662), 'mean_timesteps_19': np.float64(148.775), 'std_timesteps_19': np.float64(472.7535011553847)}\n"
     ]
    }
   ],
   "source": [
    "layer = 4\n",
    "learning_rate = 0.01\n",
    "optimizer_list = ['Adam', 'AdamW', 'RMSprop']\n",
    "type = \"timesteps\"\n",
    "results_4_001 = {}\n",
    "\n",
    "for optimizer in optimizer_list:\n",
    "    df_filtered = df12[(df12[\"layer\"] == layer) & (df12[\"learning_rate\"] == learning_rate) & (df12[\"optimizer\"] == optimizer)]\n",
    "    result = compute_time_steps(df_filtered, type=type, last_timestep=list(range(20)))\n",
    "    print(f\"Results for layer {layer}, learning rate {learning_rate}, optimizer {optimizer}:\")\n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029cd2c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "itallm2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
