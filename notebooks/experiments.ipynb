{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8942a381",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fcb7aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from utils import compute_last_token_embedding_grad_emb, get_whole, set_seed, plot_metrics, RESULTS_PATH\n",
    "from datasets import load_dataset\n",
    "from matplotlib import pyplot as plt\n",
    "from utils.metrics import compute_metrics\n",
    "import gc\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import warnings\n",
    "sys.path.append('..')\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from transformers import logging as transformers_logging\n",
    "transformers_logging.set_verbosity_error()\n",
    "plt.rcParams['text.usetex'] = True\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57f08c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "\n",
    "model_id = 'roneneldan/TinyStories-1M'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fc9516",
   "metadata": {},
   "outputs": [],
   "source": [
    "promts = [\n",
    "    'my secret key is big and long, it is 1234567890abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ',\n",
    "    '12autoZeinai ena~~ !poli, a1212kiro pr33-=ompt tao op\"\\oio ;::/>elpizo na d1212isko1212leyt5646ei na ma77ntepsei to montelo',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94bae8bb",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20e55e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"bookcorpus\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b44c9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentences = [x['text'] for x in dataset if x['text'].strip()]\n",
    "# print(sentences[:5])\n",
    "set_seed(42)\n",
    "N = 100  # number of sentences to process\n",
    "#select a permuted subset of the dataset\n",
    "sentences = dataset.shuffle(seed=42).select(range(N))['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c5a2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a034b83",
   "metadata": {},
   "source": [
    "# Experiment 1\n",
    "\n",
    "We want to show that this algorithm works perfectly at the first activation space but it goes nuts afterwards\n",
    "    - sample n sentence from an ID dataset\n",
    "        - compute BLUE score (or other sentence level distance metrics) vs L2 norm (score at the embedding level)\n",
    "        - plot them across the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e32dc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_prompt_reconstruction(llm, tokenizer, dataset, seed=8):\n",
    "\n",
    "    embedding_matrix = llm.get_input_embeddings().weight\n",
    "    metrics = []\n",
    "    # average scores over the dataset\n",
    "    sentense_embeddings = []\n",
    "    for sentence in tqdm(dataset, desc=\"Computing sentence embeddings\"):\n",
    "        set_seed(seed)\n",
    "        input_ids = tokenizer(sentence, return_tensors='pt').input_ids\n",
    "        with torch.no_grad():\n",
    "            embedding = llm.get_input_embeddings()(input_ids)[0]\n",
    "            embedding = embedding.numpy()\n",
    "        sentense_embeddings.append(embedding)\n",
    "    \n",
    "    for layer_idx in range(llm.config.num_hidden_layers):\n",
    "        output_sentences = []\n",
    "        output_embeddings = []\n",
    "        for prompt in tqdm(dataset, desc=\"Processing prompts\"):\n",
    "            set_seed(seed)\n",
    "            h_target = get_whole(prompt, model, tokenizer,layer_idx=layer_idx)\n",
    "            output_tokens = (h_target @ embedding_matrix.T).argmax(dim=-1)\n",
    "            output_sentence = tokenizer.decode(output_tokens, skip_special_tokens=True)\n",
    "            output_sentences.append(output_sentence)\n",
    "            output_embeddings.append(embedding_matrix[output_tokens].detach().numpy())\n",
    "        metrics.append(compute_metrics(sentences, sentense_embeddings, output_sentences, output_embeddings))\n",
    "    merge_metrics = {}\n",
    "    for metric in metrics:\n",
    "        for key, value in metric.items():\n",
    "            if key not in merge_metrics:\n",
    "                merge_metrics[key] = []\n",
    "            merge_metrics[key].append(value)\n",
    "    return merge_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5062ab5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(RESULTS_PATH, f'ex1_results_{N}.pkl')\n",
    "K = 100  # number of sentences to process in exhaustive search\n",
    "\n",
    "if not os.path.exists(RESULTS_PATH):\n",
    "    os.makedirs(RESULTS_PATH)\n",
    "if os.path.exists(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        ex1_result = pickle.load(f)\n",
    "else:\n",
    "    ex1_result = test_prompt_reconstruction(model, tokenizer, sentences, seed=42)\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump(ex1_result, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3147f233",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics({key:ex1_result[key] for key in ex1_result if 'l2' not in key}, fill_between=True)\n",
    "plot_metrics({key:ex1_result[key] for key in ex1_result if 'l2' in key}, fill_between=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce754cf",
   "metadata": {},
   "source": [
    "# Experiment 2: \n",
    "We want to show how expensive is Exhaustive Search from practical stance. Pick a sentence or a few, and plot the performance in decoding it of some random search. On the y-axis blue score or l2  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bb0e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exhaustive search is we search for each token one by one, minimizing the loss\n",
    "# search for each token in the sentence\n",
    "# mesure computation time and steps used \n",
    "def exhaustive_search(llm, tokenizer, prompt, layer_idx=0, seed=42, eps=1e-3):\n",
    "    print(f\"Exhaustive search for prompt: {prompt} on layer {layer_idx}\")\n",
    "    set_seed(seed)\n",
    "    input_ids = tokenizer(prompt, return_tensors='pt').input_ids\n",
    "    embedding_matrix = llm.get_input_embeddings().weight\n",
    "    h_target = get_whole(prompt, llm, tokenizer, layer_idx=layer_idx)\n",
    "    \n",
    "    output_tokens = []\n",
    "    time_start = time()\n",
    "    step = 0\n",
    "    losses = []\n",
    "    times = []\n",
    "    steps = []\n",
    "    for i in range(input_ids.shape[1]):\n",
    "        min_loss = float('inf')\n",
    "        best_token = None\n",
    "        bar = tqdm(range(embedding_matrix.shape[0]), desc=f\"Finding token {i+1}/{input_ids.shape[1]}\")\n",
    "        for j in bar:\n",
    "            step += 1\n",
    "            # try each token in the vocabulary\n",
    "            # compute the loss for the current token\n",
    "            current_tokens = output_tokens + [j]\n",
    "            current_tokens = torch.tensor(current_tokens).unsqueeze(0)\n",
    "            h = get_whole(\"\", llm, tokenizer, layer_idx=layer_idx, input_ids=current_tokens)\n",
    "            loss = (h_target[i] - h[i]).norm()\n",
    "            if loss < min_loss:\n",
    "                min_loss = loss\n",
    "                best_token = j\n",
    "            bar.set_postfix({'loss': min_loss.item(), 'token': tokenizer.decode(best_token)})\n",
    "            if loss < eps:\n",
    "                break\n",
    "\n",
    "        # add 0 to h in the dimensions of not-yet-decoded tokens so it matches the target\n",
    "        h_expanded = torch.zeros_like(h_target)\n",
    "        h_expanded[:i+1] = h[:i+1]\n",
    "        loss = (h_target - h_expanded).norm().item() / (h_target.shape[0] * h_target.shape[1])\n",
    "\n",
    "        losses.append(loss)\n",
    "        times.append(time() - time_start)\n",
    "        steps.append(step)\n",
    "\n",
    "        output_tokens.append(best_token)\n",
    "        sentence = tokenizer.decode(output_tokens, skip_special_tokens=True)\n",
    "        print(f\"Token {i+1}/{input_ids.shape[1]}: {tokenizer.decode(best_token)} | Loss: {loss:.4f} | Time: {times[-1]:.2f}s | Steps: {step} | Sentence: {sentence}\")\n",
    "\n",
    "    output_sentence = tokenizer.decode(output_tokens, skip_special_tokens=True)\n",
    "\n",
    "    return output_sentence, losses, times, steps\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d129a928",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "K = 10  # number of sentences to process in exhaustive search\n",
    "path = os.path.join(RESULTS_PATH, 'ex2_results_{N}_{K}.pkl')\n",
    "\n",
    "if not os.path.exists(RESULTS_PATH):\n",
    "    os.makedirs(RESULTS_PATH)\n",
    "if os.path.exists(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        ex2_results = pickle.load(f)\n",
    "else:\n",
    "    ex2_results = []\n",
    "    for i, prompt in enumerate(sentences):\n",
    "        if i > K:\n",
    "            break\n",
    "        ex2_result = exhaustive_search(model, tokenizer, prompt, layer_idx=8, seed=8)\n",
    "        ex2_results.append(ex2_result)\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump(ex2_results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a593b907",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.constants import COLORS\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "for i, res in enumerate(ex2_results):\n",
    "    losses, times, steps = res[1], res[2], res[3]\n",
    "    # scatter plot of losses vs steps\n",
    "    # plt.scatter(steps, losses, label='Losses', color='blue')\n",
    "    ax.plot(steps, losses, color=COLORS[0])\n",
    "ax.set_xlabel(r'Steps', fontsize=14)\n",
    "ax.set_ylabel(r'Loss', fontsize=14)\n",
    "ax.set_title(r'Exhaustive search on the 8th layer', fontsize=16)\n",
    "ax.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96ed4db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
