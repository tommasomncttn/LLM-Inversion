{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8942a381",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fcb7aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from utils import (\n",
    "    load_sentences,\n",
    "    load_or_run,\n",
    "    plot_metrics, \n",
    "    plot_metrics_over_time,\n",
    "    RESULTS_PATH\n",
    ")\n",
    "from matplotlib import pyplot as plt\n",
    "import gc\n",
    "import sys\n",
    "import warnings\n",
    "sys.path.append('..')\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from transformers import logging as transformers_logging\n",
    "transformers_logging.set_verbosity_error()\n",
    "plt.rcParams['text.usetex'] = True\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "TOKENIZERS_PARALLELISM=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d38456",
   "metadata": {},
   "outputs": [],
   "source": [
    "#constants\n",
    "N = 100\n",
    "SEED = 42  \n",
    "FORCE_RECOMPUTE=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57f08c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "\n",
    "model_id = 'roneneldan/TinyStories-1M'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "sentences = load_sentences()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a034b83",
   "metadata": {},
   "source": [
    "# Experiment 1\n",
    "\n",
    "We want to show that this algorithm works perfectly at the first activation space but it goes nuts afterwards\n",
    "    - sample n sentence from an ID dataset\n",
    "        - compute BLUE score (or other sentence level distance metrics) vs L2 norm (score at the embedding level)\n",
    "        - plot them across the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5062ab5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import test_prompt_reconstruction\n",
    "path = os.path.join(RESULTS_PATH, f'ex1_results_{N}.pkl')\n",
    "\n",
    "ex1_result = load_or_run(\n",
    "    path, \n",
    "    test_prompt_reconstruction,\n",
    "    model,\n",
    "    tokenizer,\n",
    "    sentences,\n",
    "    seed=SEED,\n",
    "    force=FORCE_RECOMPUTE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2582fab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rename_dict = {\n",
    "    'bleu' : 'BLEU',\n",
    "    'rouge-l_f' : 'ROUGE-L-F',\n",
    "    'bertscore_f1' : 'BERTScore-F1',\n",
    "}\n",
    "\n",
    "plot_metrics({key:ex1_result[key] for key in ex1_result if 'l2' not in key}, \n",
    "             fill_between=True, rename=rename_dict, xlabel='Layer', ylabel='Score', title='Prompt Reconstruction From Hidden States')\n",
    "plot_metrics({key:ex1_result[key] for key in ex1_result if 'l2' in key}, \n",
    "             fill_between=True, rename=rename_dict, xlabel='Layer', ylabel='L2 Distance', title='L2 Distance From Hidden States')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce754cf",
   "metadata": {},
   "source": [
    "# Experiment 2: \n",
    "We want to show how expensive is Exhaustive Search from practical stance. Pick a sentence or a few, and plot the performance in decoding it of some random search. On the y-axis blue score or l2  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d129a928",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import run_experiment\n",
    "from utils import exhaustive_search\n",
    "from utils.general import consolidate_logs\n",
    "\n",
    "\n",
    "K = 10  # number of sentences to process in exhaustive search\n",
    "path = os.path.join(RESULTS_PATH, f'ex2_results_{N}_{K}.pkl')\n",
    "\n",
    "ex2_results = load_or_run(\n",
    "    path, \n",
    "    run_experiment,\n",
    "    exhaustive_search,\n",
    "    sentences[1:K+1],\n",
    "    K,\n",
    "    model,\n",
    "    tokenizer,\n",
    "    layer_idx=8,\n",
    "    seed=8,\n",
    "    force=FORCE_RECOMPUTE,\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a593b907",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.constants import COLORS\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "for i, (_, res) in enumerate(ex2_results):\n",
    "    l2, times, steps = res['l2_distance'], res['time'], res['step']\n",
    "    if len(l2) != len(times) or len(l2) != len(steps):\n",
    "        continue\n",
    "    # scatter plot of losses vs steps\n",
    "    # plt.scatter(steps, losses, label='Losses', color='blue')\n",
    "    ax.plot(times, l2, color=COLORS[0])\n",
    "ax.set_xlabel(r'Steps', fontsize=14)\n",
    "ax.set_ylabel(r'Loss', fontsize=14)\n",
    "ax.set_title(r'Exhaustive search on the 8th layer', fontsize=16)\n",
    "ax.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce08be8e",
   "metadata": {},
   "source": [
    "aggregated results of the second experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4621832",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_collected = {\n",
    "    key: [x[1][key] for x in ex2_results] for key in ex2_results[0][1].keys()\n",
    "}\n",
    "# set losses to be arrays of the same length (max length of losses)\n",
    "max_length = max(len(x) for x in results_collected['l2_distance'])\n",
    "for key in results_collected:\n",
    "    results_collected[key] = [x + [np.nan] * (max_length - len(x)) for x in results_collected[key]]\n",
    "    results_collected[key] = np.array(results_collected[key])\n",
    "\n",
    "for metric_name in ['bertscore_f1', 'l2_distance', 'rouge-l_f']:\n",
    "    plot_metrics_over_time({\n",
    "        metric_name: results_collected[metric_name],\n",
    "        f'{metric_name}_time': results_collected['time'],\n",
    "    }, xlabel='Time (s)', ylabel=f'{metric_name} score',\n",
    "    title=f'Invert Whole Prompt: {metric_name} vs Time', \n",
    "    rename={metric_name: 'Whole Prompt GD'}, \n",
    "    fill_between=True,\n",
    "    window_size=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35efc1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens_over_time(results):\n",
    "    \"\"\"Calculate the number of tokens found over time.\"\"\"\n",
    "    results_collected = {\n",
    "        'losses': [x[1]['l2_distance'] for x in ex2_results],\n",
    "        'times': [x[1]['time'] for x in ex2_results],\n",
    "        'steps': [x[1]['step'] for x in ex2_results],\n",
    "    }\n",
    "    # set losses to be arrays of the same length (max length of losses)\n",
    "    max_length = max(len(x) for x in results_collected['losses'])\n",
    "    for key in results_collected:\n",
    "        results_collected[key] = [x + [np.nan] * (max_length - len(x)) for x in results_collected[key]]\n",
    "        results_collected[key] = np.array(results_collected[key])\n",
    "        results_collected[key] = results_collected[key].T\n",
    "\n",
    "    plot_metrics({\n",
    "        # 'losses': results_collected['losses'],\n",
    "        'times': results_collected['times'],\n",
    "        # 'steps': results_collected['steps']\n",
    "    }, xlabel='Tokens found', ylabel='Time (s)', title='Exhaustive Search Time per Step (layer 8)', rename={'times': 'Exhaustive search'}, fill_between=True)\n",
    "\n",
    "tokens_over_time(ex2_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2eeb97",
   "metadata": {},
   "source": [
    "## Experiment 3:\n",
    "\n",
    "Now we want to show how bad is Hardprompt. Ideally, we would like to take a sentence and to show how much time it takes in decoding it. Same plot as E2\n",
    "    - Suggest that there is no possibility of escaping local minima\n",
    "    - Suggest that the tightest converge we can prove is O(V^|n|) (`put it nicely and not formally`)\n",
    "        - ***NB*** Do this experiment at each layer of the 8\n",
    "        - ***NB*** Do this experiment with each algorithm\n",
    "        -     - **E3.1 where you simply use our algorithm on the dataset of the other three and we want to show that it converges to perfect BLUE and L2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96ed4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use invert_whole_prompt on layer 8 \n",
    "from utils import gd_all_tokens\n",
    "\n",
    "K = 10  # number of sentences to process in exhaustive search\n",
    "max_iter = 10000  # maximum number of iterations for the optimization\n",
    "lr = 1e-3  # learning rate for the optimization\n",
    "path = os.path.join(RESULTS_PATH, f'ex3_results_{N}_{K}_{max_iter}_{lr}.pkl')\n",
    "\n",
    "ex3_results = load_or_run(\n",
    "    path, \n",
    "    run_experiment,\n",
    "    gd_all_tokens,\n",
    "    sentences[1:K+1], \n",
    "    K,\n",
    "    model,\n",
    "    tokenizer,\n",
    "    layer_idx=8,\n",
    "    lr=lr,\n",
    "    log_freq=100,\n",
    "    n_iterations=max_iter,\n",
    "    force=FORCE_RECOMPUTE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89305e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric_name in ['bertscore_f1', 'l2_distance', 'rouge-l_f']:\n",
    "    # plot each sentence result\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    for i, (sent, res) in enumerate(ex3_results):\n",
    "        losses, times, steps = res[metric_name], res['time'], res['step']\n",
    "        # scatter plot of losses vs steps\n",
    "        # plt.scatter(steps, losses, label='Losses', color='blue')\n",
    "        ax.plot(times, losses, color=COLORS[3])\n",
    "    ax.set_xlabel(r'Time (s)', fontsize=14)\n",
    "    ax.set_ylabel(f'{metric_name} score', fontsize=14)\n",
    "    ax.set_title(r'Invert Whole Prompt on the 8th Layer', fontsize=16)\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26f1c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_collected = {\n",
    "    key: [x[1][key] for x in ex3_results] for key in ex3_results[0][1].keys()\n",
    "}\n",
    "# set losses to be arrays of the same length (max length of losses)\n",
    "max_length = max(len(x) for x in results_collected['l2_distance'])\n",
    "for key in results_collected:\n",
    "    results_collected[key] = [x + [np.nan] * (max_length - len(x)) for x in results_collected[key]]\n",
    "    results_collected[key] = np.array(results_collected[key])\n",
    "    results_collected[key] = results_collected[key].T\n",
    "\n",
    "for metric_name in ['bertscore_f1', 'l2_distance', 'rouge-l_f']:\n",
    "    plot_metrics_over_time({\n",
    "        metric_name: results_collected[metric_name],\n",
    "        f'{metric_name}_time': results_collected['time'],\n",
    "    }, xlabel='Time (s)', ylabel=f'{metric_name} score',\n",
    "    title=f'Invert Whole Prompt: {metric_name} vs Time', \n",
    "    rename={metric_name: 'Whole Prompt GD'}, \n",
    "    fill_between=True,\n",
    "    window_size=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab80a6a",
   "metadata": {},
   "source": [
    "## Experiment 3.1 - comparison with exhaustive search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d783c8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex3_results_collected = {\n",
    "    key: [x[1][key] for x in ex3_results] for key in ex3_results[0][1].keys()\n",
    "}\n",
    "# set losses to be arrays of the same length (max length of losses)\n",
    "max_length = max(len(x) for x in ex3_results_collected['l2_distance'])\n",
    "for key in ex3_results_collected:\n",
    "    ex3_results_collected[key] = [x + [np.nan] * (max_length - len(x)) for x in ex3_results_collected[key]]\n",
    "    ex3_results_collected[key] = np.array(ex3_results_collected[key])\n",
    "    ex3_results_collected[key] = ex3_results_collected[key].T\n",
    "\n",
    "ex2_results_collected = {\n",
    "    key: [x[1][key] for x in ex2_results] for key in ex2_results[0][1].keys()\n",
    "}\n",
    "# set losses to be arrays of the same length (max length of losses)\n",
    "max_length = max(len(x) for x in ex2_results_collected['l2_distance'])\n",
    "for key in ex2_results_collected:\n",
    "    ex2_results_collected[key] = [x + [np.nan] * (max_length - len(x)) for x in ex2_results_collected[key]]\n",
    "    ex2_results_collected[key] = np.array(ex2_results_collected[key])\n",
    "    ex2_results_collected[key] = ex2_results_collected[key].T\n",
    "\n",
    "for metric_name in ['bertscore_f1', 'l2_distance', 'rouge-l_f']:\n",
    "    plot_metrics_over_time({\n",
    "        f'ex2_{metric_name}': ex2_results_collected[metric_name],\n",
    "        f'ex3_{metric_name}': ex3_results_collected[metric_name],\n",
    "        f'ex2_{metric_name}_time': ex2_results_collected['time'],\n",
    "        f'ex3_{metric_name}_time': ex3_results_collected['time'],\n",
    "    }, xlabel='Time (s)', ylabel=f'{metric_name} score',\n",
    "    title=f'Exhaustive Search vs GD: {metric_name} vs Time',\n",
    "    rename={\n",
    "        f'ex2_{metric_name}': 'Exhaustive Search',\n",
    "        f'ex3_{metric_name}': 'GD - Full Prompt',\n",
    "    },\n",
    "    fill_between=True,\n",
    "    window_size=None,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
