{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8942a381",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fcb7aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from utils import (\n",
    "    extract_hidden_states_prompt, \n",
    "    extract_hidden_states_ids,\n",
    "    set_seed, plot_metrics, \n",
    "    RESULTS_PATH\n",
    ")\n",
    "from datasets import load_dataset\n",
    "from matplotlib import pyplot as plt\n",
    "from utils.metrics import compute_metrics\n",
    "import gc\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import warnings\n",
    "sys.path.append('..')\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from transformers import logging as transformers_logging\n",
    "transformers_logging.set_verbosity_error()\n",
    "plt.rcParams['text.usetex'] = True\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "TOKENIZERS_PARALLELISM=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57f08c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "\n",
    "model_id = 'roneneldan/TinyStories-1M'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fc9516",
   "metadata": {},
   "outputs": [],
   "source": [
    "promts = [\n",
    "    'my secret key is big and long, it is 1234567890abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ',\n",
    "    '12autoZeinai ena~~ !poli, a1212kiro pr33-=ompt tao op\"\\oio ;::/>elpizo na d1212isko1212leyt5646ei na ma77ntepsei to montelo',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94bae8bb",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20e55e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"bookcorpus\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b44c9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentences = [x['text'] for x in dataset if x['text'].strip()]\n",
    "# print(sentences[:5])\n",
    "set_seed(42)\n",
    "N = 100  # number of sentences to process\n",
    "#select a permuted subset of the dataset\n",
    "sentences = dataset.shuffle(seed=42).select(range(N))['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c5a2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a034b83",
   "metadata": {},
   "source": [
    "# Experiment 1\n",
    "\n",
    "We want to show that this algorithm works perfectly at the first activation space but it goes nuts afterwards\n",
    "    - sample n sentence from an ID dataset\n",
    "        - compute BLUE score (or other sentence level distance metrics) vs L2 norm (score at the embedding level)\n",
    "        - plot them across the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e32dc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_prompt_reconstruction(llm, tokenizer, dataset, seed=8):\n",
    "\n",
    "    embedding_matrix = llm.get_input_embeddings().weight\n",
    "    metrics = []\n",
    "    # average scores over the dataset\n",
    "    sentense_embeddings = []\n",
    "    for sentence in tqdm(dataset, desc=\"Computing sentence embeddings\"):\n",
    "        set_seed(seed)\n",
    "        input_ids = tokenizer(sentence, return_tensors='pt').input_ids\n",
    "        with torch.no_grad():\n",
    "            embedding = llm.get_input_embeddings()(input_ids)[0]\n",
    "            embedding = embedding.numpy()\n",
    "        sentense_embeddings.append(embedding)\n",
    "    \n",
    "    for layer_idx in range(llm.config.num_hidden_layers):\n",
    "        output_sentences = []\n",
    "        output_embeddings = []\n",
    "        for prompt in tqdm(dataset, desc=\"Processing prompts\"):\n",
    "            set_seed(seed)\n",
    "            h_target = extract_hidden_states_prompt(prompt, model, tokenizer,layer_idx=layer_idx)\n",
    "            output_tokens = (h_target @ embedding_matrix.T).argmax(dim=-1)\n",
    "            output_sentence = tokenizer.decode(output_tokens, skip_special_tokens=True)\n",
    "            output_sentences.append(output_sentence)\n",
    "            output_embeddings.append(embedding_matrix[output_tokens].detach().numpy())\n",
    "        metrics.append(compute_metrics(sentences, sentense_embeddings, output_sentences, output_embeddings))\n",
    "    merge_metrics = {}\n",
    "    for metric in metrics:\n",
    "        for key, value in metric.items():\n",
    "            if key not in merge_metrics:\n",
    "                merge_metrics[key] = []\n",
    "            merge_metrics[key].append(value)\n",
    "    return merge_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5062ab5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(RESULTS_PATH, f'ex1_results_{N}.pkl')\n",
    "\n",
    "if not os.path.exists(RESULTS_PATH):\n",
    "    os.makedirs(RESULTS_PATH)\n",
    "if os.path.exists(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        ex1_result = pickle.load(f)\n",
    "else:\n",
    "    ex1_result = test_prompt_reconstruction(model, tokenizer, sentences, seed=42)\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump(ex1_result, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3147f233",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics({key:ex1_result[key] for key in ex1_result if 'l2' not in key}, fill_between=True)\n",
    "plot_metrics({key:ex1_result[key] for key in ex1_result if 'l2' in key}, fill_between=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce754cf",
   "metadata": {},
   "source": [
    "# Experiment 2: \n",
    "We want to show how expensive is Exhaustive Search from practical stance. Pick a sentence or a few, and plot the performance in decoding it of some random search. On the y-axis blue score or l2  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bb0e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exhaustive search is we search for each token one by one, minimizin the loss\n",
    "# search for each token in the sentence\n",
    "# mesure computation time and steps used \n",
    "\n",
    "\n",
    "def exhaustive_search(llm, tokenizer, prompt, layer_idx=0, seed=42, eps=1e-3):\n",
    "    print(f\"Exhaustive search for prompt: {prompt} on layer {layer_idx}\")\n",
    "    set_seed(seed)\n",
    "    input_ids = tokenizer(prompt, return_tensors='pt').input_ids\n",
    "    embedding_matrix = llm.get_input_embeddings().weight\n",
    "    h_target = extract_hidden_states_prompt(prompt, llm, tokenizer, layer_idx=layer_idx)\n",
    "    \n",
    "    output_tokens = []\n",
    "    time_start = time()\n",
    "    step = 0\n",
    "    l2s = []\n",
    "    times = []\n",
    "    steps = []\n",
    "    for i in range(input_ids.shape[1]):\n",
    "        min_loss = float('inf')\n",
    "        best_token = None\n",
    "        bar = tqdm(range(embedding_matrix.shape[0]), desc=f\"Finding token {i+1}/{input_ids.shape[1]}\")\n",
    "        for j in bar:\n",
    "            step += 1\n",
    "            # try each token in the vocabulary\n",
    "            # compute the loss for the current token\n",
    "            current_tokens = output_tokens + [j]\n",
    "            current_tokens = torch.tensor(current_tokens).unsqueeze(0)\n",
    "            h = extract_hidden_states_ids(current_tokens, llm, layer_idx)\n",
    "            loss = (h_target[i] - h[i]).norm()\n",
    "            if loss < min_loss:\n",
    "                min_loss = loss\n",
    "                best_token = j\n",
    "            bar.set_postfix({'loss': min_loss.item(), 'token': tokenizer.decode(best_token)})\n",
    "            if loss < eps:\n",
    "                break\n",
    "\n",
    "        # add 0 to h in the dimensions of not-yet-decoded tokens so it matches the target\n",
    "        h_expanded = torch.zeros_like(h_target)\n",
    "        h_expanded[:i+1] = h[:i+1]\n",
    "        l2 = (h_target - h_expanded).norm(p=2, dim=1).mean().item()\n",
    "\n",
    "        l2s.append(l2)\n",
    "        times.append(time() - time_start)\n",
    "        steps.append(step)\n",
    "\n",
    "        output_tokens.append(best_token)\n",
    "        sentence = tokenizer.decode(output_tokens, skip_special_tokens=True)\n",
    "        print(f\"Token {i+1}/{input_ids.shape[1]}: {tokenizer.decode(best_token)} | L2: {l2:.4f} | Time: {times[-1]:.2f}s | Steps: {step} | Sentence: {sentence}\")\n",
    "\n",
    "    output_sentence = tokenizer.decode(output_tokens, skip_special_tokens=True)\n",
    "\n",
    "    return output_sentence, l2s, times, steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d129a928",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 10  # number of sentences to process in exhaustive search\n",
    "path = os.path.join(RESULTS_PATH, f'ex2_results_{N}_{K}.pkl')\n",
    "\n",
    "if not os.path.exists(RESULTS_PATH):\n",
    "    os.makedirs(RESULTS_PATH)\n",
    "if os.path.exists(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        ex2_results = pickle.load(f)\n",
    "else:\n",
    "    ex2_results = []\n",
    "    for i, prompt in enumerate(sentences):\n",
    "        if i > K:\n",
    "            break\n",
    "        ex2_result = exhaustive_search(model, tokenizer, prompt, layer_idx=8, seed=8)\n",
    "        ex2_results.append(ex2_result)\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump(ex2_results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a593b907",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.constants import COLORS\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "for i, res in enumerate(ex2_results):\n",
    "    losses, times, steps = res[1], res[2], res[3]\n",
    "    # scatter plot of losses vs steps\n",
    "    # plt.scatter(steps, losses, label='Losses', color='blue')\n",
    "    ax.plot(steps, losses, color=COLORS[0])\n",
    "ax.set_xlabel(r'Steps', fontsize=14)\n",
    "ax.set_ylabel(r'Loss', fontsize=14)\n",
    "ax.set_title(r'Exhaustive search on the 8th layer', fontsize=16)\n",
    "ax.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce08be8e",
   "metadata": {},
   "source": [
    "aggregated results of the second experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35efc1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_collected = {\n",
    "    'losses': [x[1] for x in ex2_results],\n",
    "    'times': [x[2] for x in ex2_results],\n",
    "    'steps': [x[3] for x in ex2_results],\n",
    "}\n",
    "# set losses to be arrays of the same length (max length of losses)\n",
    "max_length = max(len(x) for x in results_collected['losses'])\n",
    "for key in results_collected:\n",
    "    results_collected[key] = [x + [np.nan] * (max_length - len(x)) for x in results_collected[key]]\n",
    "    results_collected[key] = np.array(results_collected[key])\n",
    "    results_collected[key] = results_collected[key].T\n",
    "\n",
    "\n",
    "plot_metrics({\n",
    "    # 'losses': results_collected['losses'],\n",
    "    'times': results_collected['times'],\n",
    "    # 'steps': results_collected['steps']\n",
    "}, xlabel='Tokens found', ylabel='Time (s)', title='Exhaustive Search Time per Step (layer 8)', rename={'times': 'Exhaustive search'}, fill_between=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb5e53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot time on the x-axis and number of tokens found on the y-axis\n",
    "# results have to be pivoted, currently one entrly per token found\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "results_pivoted = {\n",
    "    'times': [],\n",
    "    'steps': [],\n",
    "    'tokens_found': [],\n",
    "    'losses': [],\n",
    "}\n",
    "for i, res in enumerate(ex2_results):\n",
    "    times, steps, losses = res[2], res[3], res[1]\n",
    "    results_pivoted['times'].append(times)\n",
    "    results_pivoted['steps'].append(steps)\n",
    "    results_pivoted['tokens_found'].append(np.arange(1, len(losses) + 1)/len(losses))\n",
    "    results_pivoted['losses'].append(losses)\n",
    "\n",
    "results_pivoted = pd.DataFrame(results_pivoted)\n",
    "results_pivoted = results_pivoted.explode(['times', 'steps', 'tokens_found', 'losses'])\n",
    "\n",
    "# scatter plot of time (x-axis) vs loss (y-axis)\n",
    "plt.figure(figsize=(10, 6))\n",
    "# color by the fraction of tokens found\n",
    "# plt.scatter(results_pivoted['times'], results_pivoted['losses'], \n",
    "#             c=results_pivoted['tokens_found'], cmap='viridis', alpha=0.7, edgecolors='w', s=50)\n",
    "# plt.colorbar(label='Fraction of Tokens Found')\n",
    "plt.plot(results_pivoted['times'], results_pivoted['losses'], \n",
    "            color=COLORS[0], marker=\"o\"\n",
    ")\n",
    "plt.xlabel('Time (s)', fontsize=14)\n",
    "plt.ylabel('Loss', fontsize=14)\n",
    "plt.title('Exhaustive Search: Time vs Loss', fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2eeb97",
   "metadata": {},
   "source": [
    "## Experiment 3:\n",
    "\n",
    "Now we want to show how bad is Hardprompt. Ideally, we would like to take a sentence and to show how much time it takes in decoding it. Same plot as E2\n",
    "    - Suggest that there is no possibility of escaping local minima\n",
    "    - Suggest that the tightest converge we can prove is O(V^|n|) (`put it nicely and not formally`)\n",
    "        - ***NB*** Do this experiment at each layer of the 8\n",
    "        - ***NB*** Do this experiment with each algorithm\n",
    "        -     - **E3.1 where you simply use our algorithm on the dataset of the other three and we want to show that it converges to perfect BLUE and L2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96ed4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use invert_whole_prompt on layer 8 \n",
    "from utils import invert_whole_prompt\n",
    "\n",
    "K = 10  # number of sentences to process in exhaustive search\n",
    "max_iter = 3000  # maximum number of iterations for the optimization\n",
    "lr = 1e-3  # learning rate for the optimization\n",
    "path = os.path.join(RESULTS_PATH, f'ex3_results_{N}_{K}_{max_iter}_{lr}_tmp.pkl')\n",
    "\n",
    "if not os.path.exists(RESULTS_PATH):\n",
    "    os.makedirs(RESULTS_PATH)\n",
    "if os.path.exists(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        ex3_results = pickle.load(f)\n",
    "else:\n",
    "    ex3_results = []\n",
    "    for i, prompt in enumerate(sentences):\n",
    "        if i >= K:\n",
    "            break\n",
    "        ex3_result = invert_whole_prompt(prompt, model, tokenizer, layer_idx=8, n_iterations=max_iter, lr=lr, log_freq=10)\n",
    "        ex3_results.append(ex3_result)\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump(ex3_results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827f24ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex3_results[0][1].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89305e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric_name in ['bertscore_f1', 'l2_distance', 'rouge-l_f']:\n",
    "    # plot each sentence result\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    for i, (sent, res) in enumerate(ex3_results):\n",
    "        losses, times, steps = res[metric_name], res['time'], res['step']\n",
    "        # scatter plot of losses vs steps\n",
    "        # plt.scatter(steps, losses, label='Losses', color='blue')\n",
    "        ax.plot(times, losses, color=COLORS[3])\n",
    "    ax.set_xlabel(r'Time (s)', fontsize=14)\n",
    "    ax.set_ylabel(f'{metric_name} score', fontsize=14)\n",
    "    ax.set_title(r'Invert Whole Prompt on the 8th Layer', fontsize=16)\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26f1c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_collected = {\n",
    "    key: [x[1][key] for x in ex3_results] for key in ex3_results[0][1].keys()\n",
    "    # 'l2': [x[1]['l2_distance'] for x in ex3_results],\n",
    "    # 'times': [x[1]['time'] for x in ex3_results],\n",
    "    # 'steps': [x[1]['step'] for x in ex3_results],\n",
    "}\n",
    "# set losses to be arrays of the same length (max length of losses)\n",
    "max_length = max(len(x) for x in results_collected['l2_distance'])\n",
    "for key in results_collected:\n",
    "    results_collected[key] = [x + [np.nan] * (max_length - len(x)) for x in results_collected[key]]\n",
    "    results_collected[key] = np.array(results_collected[key])\n",
    "    results_collected[key] = results_collected[key].T\n",
    "\n",
    "for metric_name in ['bertscore_f1', 'l2_distance', 'rouge-l_f']:\n",
    "    plot_metrics({\n",
    "        'times': results_collected[metric_name],\n",
    "    }, xlabel='Steps x100', ylabel='Time (s)', title=f'Wholoe prompt descent (layer 8) - {metric_name}', rename={'times': 'Exhaustive search'}, fill_between=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
