{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8942a381",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fcb7aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from utils.general import compute_last_token_embedding_grad_emb, get_whole, set_seed\n",
    "from datasets import load_dataset\n",
    "from matplotlib import pyplot as plt\n",
    "from utils.metrics import compute_metrics\n",
    "import gc\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import warnings\n",
    "sys.path.append('..')\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from transformers import logging as transformers_logging\n",
    "transformers_logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57f08c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "\n",
    "model_id = 'roneneldan/TinyStories-1M'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fc9516",
   "metadata": {},
   "outputs": [],
   "source": [
    "promts = [\n",
    "    'my secret key is big and long, it is 1234567890abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ',\n",
    "    '12autoZeinai ena~~ !poli, a1212kiro pr33-=ompt tao op\"\\oio ;::/>elpizo na d1212isko1212leyt5646ei na ma77ntepsei to montelo',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94bae8bb",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20e55e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"bookcorpus\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b44c9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentences = [x['text'] for x in dataset if x['text'].strip()]\n",
    "# print(sentences[:5])\n",
    "set_seed(42)\n",
    "N = 100  # number of sentences to process\n",
    "#select a permuted subset of the dataset\n",
    "sentences = dataset.shuffle(seed=42).select(range(N))['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c5a2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a034b83",
   "metadata": {},
   "source": [
    "# Experiment 1\n",
    "\n",
    "We want to show that this algorithm works perfectly at the first activation space but it goes nuts afterwards\n",
    "    - sample n sentence from an ID dataset\n",
    "        - compute BLUE score (or other sentence level distance metrics) vs L2 norm (score at the embedding level)\n",
    "        - plot them across the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e32dc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_prompt_reconstruction(llm, tokenizer, dataset, seed=8):\n",
    "\n",
    "    embedding_matrix = llm.get_input_embeddings().weight\n",
    "    metrics = []\n",
    "    # average scores over the dataset\n",
    "    for layer_idx in range(llm.config.num_hidden_layers):\n",
    "        output_sentences = []\n",
    "        output_embeddings = []\n",
    "        for prompt in tqdm(dataset, desc=\"Processing prompts\"):\n",
    "            set_seed(seed)\n",
    "            h_target = get_whole(prompt, model, tokenizer,layer_idx=layer_idx)\n",
    "            embedding_matrix.shape\n",
    "            output_tokens = (h_target @ embedding_matrix.T).argmax(dim=-1)\n",
    "            output_sentence = tokenizer.decode(output_tokens, skip_special_tokens=True)\n",
    "            output_sentences.append(output_sentence)\n",
    "            output_embeddings.append(embedding_matrix[output_tokens])\n",
    "        metrics.append(compute_metrics(sentences, output_sentences, output_embeddings))\n",
    "    merge_metrics = {}\n",
    "    for metric in metrics:\n",
    "        for key, value in metric.items():\n",
    "            if key not in merge_metrics:\n",
    "                merge_metrics[key] = []\n",
    "            merge_metrics[key].append(value)\n",
    "    print(merge_metrics)\n",
    "    return merge_metrics\n",
    "ex1_result = test_prompt_reconstruction(model, tokenizer, sentences, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3147f233",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.constants import COLORS\n",
    "\n",
    "def plot_metrics(metrics):\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    for i, (key, values) in enumerate(metrics.items()):\n",
    "        ax.plot(range(len(values)), values, label=key, color=COLORS[i % len(COLORS)], linewidth=2, marker='o')\n",
    "    ax.set_xlabel(r'Layer Index', fontsize=14)\n",
    "    ax.set_ylabel(r'Metric Value', fontsize=14)\n",
    "    ax.set_title(r'Metrics per Layer', fontsize=16)\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "plot_metrics(ex1_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
