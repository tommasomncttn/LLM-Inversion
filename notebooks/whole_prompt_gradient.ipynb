{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d810884",
   "metadata": {},
   "source": [
    "# whole prompt descent\n",
    "\n",
    "Can we descent on the whole prompt at once?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5e0aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# no need to restart kernel after code (in the imported files) changes\n",
    "%load_ext autoreload\n",
    "%autoreload 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3d7896",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from utils import compute_last_token_embedding_grad, get_whole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5f3d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"roneneldan/TinyStories-1M\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"roneneldan/TinyStories-1M\", output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255851be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.general import compute_all_token_embeddings_grad\n",
    "\n",
    "\n",
    "def invert_whole_prompt(prompt, model, tokenizer, layer_idx, n_iterations=1000, gamma=1e-1):\n",
    "    \"\"\"\n",
    "    Invert the entire prompt at once by optimizing embeddings for all tokens simultaneously.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): The input prompt to invert.\n",
    "        model: The language model.\n",
    "        tokenizer: The tokenizer for the model.\n",
    "        layer_idx (int): The layer index to target for inversion.\n",
    "        n_iterations (int): Number of optimization iterations.\n",
    "        gamma (float): Step size for gradient descent.\n",
    "\n",
    "    Returns:\n",
    "        str: The reconstructed prompt.\n",
    "    \"\"\"\n",
    "    # Tokenize the prompt and get target hidden states\n",
    "    tokenized = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    input_ids = tokenized[\"input_ids\"].squeeze(0)\n",
    "    h_target = get_whole(prompt, model, tokenizer, layer_idx, grad=False)\n",
    "\n",
    "    # Initialize random embeddings for the entire sequence\n",
    "    embedding_matrix = model.get_input_embeddings().weight\n",
    "    vocab_size, hidden_size = embedding_matrix.shape\n",
    "    random_ids = torch.randint(0, vocab_size, (input_ids.size(0),))\n",
    "    x_i_plus_1 = embedding_matrix[random_ids]\n",
    "    losses = []\n",
    "    distances = []\n",
    "\n",
    "    with tqdm(total=n_iterations, desc=\"Inverting prompt\") as pbar:\n",
    "        for iteration in range(n_iterations):\n",
    "            # Compute gradients for the entire sequence\n",
    "            grad_oracle, loss = compute_all_token_embeddings_grad(\n",
    "                y=random_ids,\n",
    "                llm=model,\n",
    "                layer_idx=layer_idx,\n",
    "                h_target=h_target,\n",
    "                tokenizer=tokenizer,\n",
    "            ) \n",
    "            losses.append(loss)\n",
    "            x_i_plus_1 = x_i_plus_1 - gamma * grad_oracle\n",
    "\n",
    "            dist = torch.cdist(x_i_plus_1, embedding_matrix)\n",
    "            random_ids = torch.argmin(dist, dim=1)\n",
    "\n",
    "            dist_from_prompt = torch.norm(\n",
    "                embedding_matrix[random_ids] - embedding_matrix[input_ids]\n",
    "            )\n",
    "            average_distance = dist_from_prompt.mean().item()\n",
    "            distances.append(average_distance)\n",
    "\n",
    "            pbar.set_postfix({\"Loss\": loss, \"Distance\": average_distance})\n",
    "            pbar.update(1)\n",
    "\n",
    "            if average_distance < 1e-3:\n",
    "                break\n",
    "\n",
    "    reconstructed_prompt = tokenizer.decode(random_ids.tolist(), skip_special_tokens=True)\n",
    "    return reconstructed_prompt, losses, distances\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b0a902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "from utils.plotting import plot_loss\n",
    "\n",
    "\n",
    "def test_inversion_on_layers(prompt, model, tokenizer, n_iterations=1000, gamma=1e-3):\n",
    "    \"\"\"\n",
    "    Test inversion on all layers of the model.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): The input prompt to invert.\n",
    "        model: The language model.\n",
    "        tokenizer: The tokenizer for the model.\n",
    "    \"\"\"\n",
    "    print(f\"Inverting prompt: {prompt}\")\n",
    "    n_layers = len(model.transformer.h)\n",
    "    losses_list = []\n",
    "    distances_list = []\n",
    "    for i in range(n_layers):\n",
    "        layer_idx = i\n",
    "        reconstructed_prompt, losses, distances = invert_whole_prompt(\n",
    "            prompt, model, tokenizer, layer_idx, n_iterations=n_iterations, gamma=gamma\n",
    "        )\n",
    "        losses_list.append(losses)\n",
    "        distances_list.append(distances)\n",
    "        print(f\"Layer {layer_idx}: Reconstructed Prompt: {reconstructed_prompt}\")\n",
    "    plot_loss(losses_list, title=\"Losses during inversion\", xlabel=\"Iteration\", ylabel=\"Loss\", log_scale=True)\n",
    "    plot_loss(distances_list, title=\"Distances of the prompt embeddings\", xlabel=\"Iteration\", ylabel=\"Distance\", log_scale=True)\n",
    "    return losses_list, distances_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22083cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"My name is george and I live in Greece.\"\n",
    "losses_list, distances_list = test_inversion_on_layers(prompt, model, tokenizer, n_iterations=1000, gamma=1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f4b1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = \"my name is george and I live in Greece.\"\n",
    "prompt = \"\"\"\n",
    "According to all known laws of aviation,\n",
    "there is no way a bee should be able to fly.\n",
    "\n",
    "Its wings are too small to get its fat little body off the ground.\n",
    "The bee, of course, flies anyway\n",
    "because bees don’t care what humans think is impossible.\n",
    "\n",
    "Yellow, black. Yellow, black.\n",
    "Yellow, black. Yellow, black.\n",
    "Ooh, black and yellow!\n",
    "Let’s shake it up a little.\n",
    "\n",
    "Barry! Breakfast is ready!\n",
    "\n",
    "Coming!\n",
    "\n",
    "Hang on a second.\n",
    "Hello?\n",
    "\n",
    "Barry?\n",
    "Adam?\n",
    "\n",
    "Can you believe this is happening?\n",
    "I can’t. I’ll pick you up.\n",
    "\n",
    "Looking sharp.\n",
    "\"\"\"\n",
    "losses_list, distances_list = test_inversion_on_layers(prompt, model, tokenizer, n_iterations=1000, gamma=1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b982853",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Here is my secrete key: b4e3cfe16a409f237a91c778e5f82b1493d546bc3adbd268cb346f8e2f55e72c. Do not share it with anyone!!!\"\n",
    "\n",
    "losses_list, distances_list = test_inversion_on_layers(prompt, model, tokenizer, n_iterations=1000, gamma=1e-1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
